% This chapter is the conclusion

\section{Contributions of this thesis}

Computational models that encode semantic knowledge in distributed representations are demonstrating better and better performance on tasks that mirror human cognitive capacities, particularly when trained on large amounts of data (so that useful concepts can naturally emerge in distributed memory). However, unlike the models trained to achieve these impressive solutions to specific AI problems, most human learning takes place without a particular skill or application in mind. Rather, general concepts and facts that emerge from experience via constant observation of the environment are later applied quickly and effectively to multiple tasks and goals. The goal of this thesis has been to explore ways of effecting this human-like unsupervised learning in the case of neural language models.

Thanks largely to the power of the distributional hypothesis of natural language, various established methods already existed for acquiring general-purpose word representations from linguistic input. For phrase or sentence semantics, however, this was not the case. Most general approaches to interpreting phrases or sentences involved mapping between text and symbolic or localist representations such as logical forms (see e.g.~\citealt{poon2009unsupervised}). This thesis demonstrates that the symbolic approach is not the only way to represent phrases and sentences in a generally-applicable way. The potential impact of extending the distributional approach from words to phrases and sentences is tremendous, given the desirable computational and modelling properties of distributed representations and the fact that that most information transfer between humans involves these larger units rather than words in isolation. Nonetheless, many challenges remain, many of which are caused by the fact that while a given word typically occurs multiple times in a given dataset, the repetition of a phrase or a sentence is highly unlikely. Put another way, the path from frequency to meaning, comparatively well understood for the lexicon~\citep{turney2010frequency}, remains less clear for phrases and sentences.    

The principal contributions of this thesis are as follows:

\paragraph{A new resource for the evaluation of distributed word representations} Without robust ways to evaluate the quality of word representations, it would be difficult to compare various approaches and detect improvements. Existing methods suffered from a range of limitations, such as low word coverage, poorly defined scores or low inter-rater agreement. In chapter~\ref{CH2} I described SimLex-999, a resource designed to mitigate these limitations. SimLex covers a more representative set of word concepts than many alternative evaluation resources. It measures semantic similarity, a relation about which native English speakers seem to have clearer, more consistent intuitions. Since its development, SimLex-999 has been used to evaluate numerous new algorithms and approaches for word representation learning. It has also been translated into German, Italian and Russian. 

\paragraph{A novel method for acquiring distributed word representations from bilingual texts} In Chapter~\ref{CH3}, I showed how the objective of translating between sentences in bilingual corpora, via neural machine translation models, yields word representation spaces that are more naturally orientated according to semantic similarity than monolingual neural language models. Indeed, such a model produced what was at the time the best reported performance of a distributional model on the SimLex-999 benchmark of similarity modelling.  

\paragraph{Learning phrase representations by training NLMs on dictionaries or encyclopedias} in Chapter~\ref{CH4}, I showed how NLMs could be effectively trained on the textual definitions or descriptions in dictionaries and encyclopedias. In these models, dictionaries provide a bridge between lexical meaning and phrase meaning, allowing the model's interpretation of phrases to be `supervised' by the corresponding lexical representation (which can be  easily acquired by models described in Chapters~\ref{CH2} and~\ref{CH3}). The combination of the representational power of neural language models and the principled semantic information in dictionaries proves to be very powerful. The trained models generalise well beyond the training data. They are capable of beating established dictionary-indexing software at retrieving concepts not defined in the training data, an effect that is magnified when the linguistic style of description of definition differs from that of the training set, and can even answer general-knowledge crossword questions. Moreover, the phrase or sentence representations encoded by models trained in this way perform more consistently than alternative NLM architectures across the suite of supervised and unsupervised evaluations applied to all models in Chapter~\ref{CH5}. The dictionary-based training objective seems to be a particularly simple and effective means to learn general-purpose representations of phrases, and to encode the corresponding general knowledge in distributed semantic memory. 

\paragraph{Two novel models for learning distributed sentence representations from text} In addition to a systematic comparison of methods for acquiring phrase and sentence representations from unlabelled text-based data, in Chapter~\ref{CH6}, I developed two new algorithms, each with certain specific advantages over existing approaches. The first, the sequential denoising autoencoder, is a modification of the SkipThought model that can be trained on any collection of unordered sentences, and learns representations that are particularly applicable to paraphrasing applications. The second, FastSent, is a modification CBOW, a well-known log-linear model for lexical representation learning, in which word embeddings are optimised to form useful sentence representations under the addition operation. Like other shallow neural language models, FastSent performs best in unsupervised applications involving a linear decoding of its representation space. It outperforms alternatives at direct prediction of sentence relatedness, and qualitative analysis (e.g. via the web demo) suggests a more semantically plausible space of sentence representations than alternatives.       

\paragraph{Representing naturally-occurring language in memory networks} Memory networks had previously been applied to toy tasks involving artificial language, such as question answering. In Chapter~\ref{CH6}, I described one of the first studies in which memory networks are trained to effectively represent naturally-occurring languages (passages of multiple sentences). I also showed how contextual neural language models such as memory networks provide a more extrinsic way to compare representational forms for text, particularly phrases and sentences. I showed that models that effectively focus on small sub-sentential windows convey more useful information (at least with respect to a missing-word completion task) than those whose focus is both broader (entire sentences) or narrower (ordered sequences of words). In addition, I produced and released the Children's Book Test, a benchmark designed to evaluate how well models represent and select information from extra sentential contexts. Together, these contributions can be understood in a general tendency of language processing research away from analysing individual sentences in isolation, towards models that can effectively interpret utterances in particular contexts of documents or dialogues.  

\section{Future work} The approaches to knowledge representation and language learning described in this thesis involve training models to make predictions from a language corpora or structured text-based resources. The extension of the training environment beyond unstructured running text was intended to mitigate the discrepancy between the information available to human language learners and the signal from text alone. Nevertheless, there is another important point of difference between these approaches and human language learning that may prove just as important to address if models are to exhibit truly human-like linguistic behaviours. In the all of the models considered in this thesis, the learning environment is \emph{passive}, in that the training information source is pre-determined before learning begins, and does not alter at any stage as a consequence of the output of the model. As such, they mimic the part of language learning in which children observe adult language by listening to the conversations of others or reading. This is undoubtedly the way in which babies learn their first words, and even after they can talk, this passive observation may well be a critical part of language learning. Nevertheless, an important part of human language learning is also~\emph{interactive}. After a period of observing language passively, children develop the ability to influence the nature of their future linguistic experience (e.g by moving conversations in a particular direction). In this interactive setting, learners infer linguistic meaning not simply by observing correct language, or even determining what in the world that language refers to, but also by noticing the reaction it provokes in others and realising the communicative acts that it facilitates. Further, to satisfy complex communicative acts, this learning must be robust to sequences of multiple training examples with little or no knowledge of whether their interpretations of these examples are `correct' with respect to the goal in question. These aspects of learning, currently lacking from the approaches studied in this thesis, may be critical for efficiently training models to replicate human linguistic behaviour in a robust way.

The next stage of this research programme is therefore to place the language models described in this thesis into more dynamic, interactive and goal-driven learning environments. In recent work, interactive learning frameworks such as reinforcement learning have proved very effective tools for training agents to resolve computer games, particularly when deep neural networks are used to represent the situations faced by the agent and thus effectively reduce the search space among state-action pairs~\citep{mnih2015human}. Indeed, the same strategy has also been applied to language games, in which states are described by textual descriptions as part and the model agent must choose between three possible actions at each stage~\citep{narasimhan2015language}. Despite these promising results, however, there are many significant challenges in applying such a strategy to language learning in general. Linguistic behaviour cannot easily be reduced to a (small) finite number of possible actions, which makes learning very challenging. Moreover, it is not trivial to model the vast and dynamic array of behavioural goals that are characteristic of human activity and which combine to make general language understanding viable. 

FINAL PARA