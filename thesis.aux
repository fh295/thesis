\relax 
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\@newglossary{main}{glg}{gls}{glo}
\@istfilename{thesis.ist}
\@glsorder{word}
\@writefile{toc}{\thispagestyle {empty}}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{15}{chapter.1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\citation{dist}
\citation{landauer1997solution}
\citation{blei2003latent}
\citation{griffiths2007topics}
\citation{bengio2003neural}
\citation{collobert2008unified}
\citation{mikolov2013efficient}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Understanding and evaluating unsupervised models of word representation}{17}{chapter.2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\citation{finkelstein2001placing}
\citation{bruni2014multimodal}
\citation{gentner1978relational}
\citation{hill2013quantitative}
\citation{tversky1977features}
\@writefile{toc}{\contentsline {paragraph}{The Challenge of Evaluation}{18}{section*.5}}
\citation{collobert2008unified,turian2010word}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Reproducing human semantic knowledge}{19}{section.2.1}}
\citation{plaut1995semantic,mcrae2012semantic}
\citation{mcrae2012semantic}
\citation{grigg2009lacan}
\citation{crutch2009different}
\citation{lucas2000semantic}
\citation{fellbaum1999wordnet}
\citation{wu1994verbs}
\citation{wu1994verbs}
\citation{nelson2004university}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Similarity and Association}{20}{subsection.2.1.1}}
\citation{hill2013quantitative}
\citation{he2008indirect,Haghighi2008Learning,marton2009improved,beltagysemantic}
\citation{navigli2009word}
\citation{phan2008learning}
\citation{rose2002reuters}
\@writefile{toc}{\contentsline {paragraph}{Association and similarity in NLP}{21}{section*.7}}
\citation{huang2012improving,reisinger2010multi,luong2013better}
\citation{budanitsky2006evaluating}
\citation{turney2012domain}
\citation{agirre2009study}
\citation{agirre2009study}
\@writefile{lot}{\contentsline {table}{\numberline {2.1}{\ignorespaces  Top: Concept pairs with the lowest WupSim scores in the USF dataset overall. Bottom: Pairs with the largest discrepancy in rank between association strength (high) and WupSim (low).\relax }}{22}{table.caption.6}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{tab1}{{2.1}{22}{Top: Concept pairs with the lowest WupSim scores in the USF dataset overall. Bottom: Pairs with the largest discrepancy in rank between association strength (high) and WupSim (low).\relax \relax }{table.caption.6}{}}
\citation{gentner2006verbs}
\citation{markman1997similar}
\citation{hill2014multi}
\citation{paivio1991dual,hill2013quantitative}
\citation{hill2013concreteness}
\citation{kielaimproving}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Motivation for SimLex-999}{23}{section.2.2}}
\newlabel{motivation}{{2.2}{23}{Motivation for SimLex-999\relax }{section.2.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Concepts, part-of-speech and concreteness}{23}{subsection.2.2.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Existing gold standards and evaluation resources}{23}{subsection.2.2.2}}
\newlabel{existing}{{2.2.2}{23}{Existing gold standards and evaluation resources\relax }{subsection.2.2.2}{}}
\citation{finkelstein2001placing}
\citation{huang2012improving,bansal2014tailoring}
\@writefile{toc}{\contentsline {paragraph}{Representative}{24}{section*.8}}
\@writefile{toc}{\contentsline {paragraph}{Clearly-defined}{24}{section*.9}}
\@writefile{toc}{\contentsline {paragraph}{Consistent and reliable}{24}{section*.10}}
\@writefile{toc}{\contentsline {paragraph}{}{24}{section*.11}}
\@writefile{toc}{\contentsline {paragraph}{\bf  WordSim-353}{24}{section*.12}}
\citation{collobert2008unified}
\citation{huang2012improving}
\citation{huang2012improving}
\citation{pado2007flexible,reisinger2010mixture,silberer2014learning}
\citation{yong1999case,cunningham2005information,resnik201011}
\citation{alfonseca2002extending}
\citation{agirre2009study}
\citation{agirre2009study}
\@writefile{toc}{\contentsline {paragraph}{\bf  WS-Sim}{25}{section*.13}}
\citation{agirre2009study}
\citation{rubenstein1965contextual}
\citation{hassan2011semantic}
\citation{rubenstein1965contextual}
\citation{bruni2014multimodal}
\citation{bruni2012distributional2,bernardi2013relatedness}
\@writefile{toc}{\contentsline {paragraph}{\bf  Rubenstein \& Goodenough}{26}{section*.14}}
\@writefile{toc}{\contentsline {paragraph}{\bf  The MEN Test Collection}{26}{section*.15}}
\citation{bruni2012distributional}
\citation{landauer1997solution}
\citation{griffiths2007topics}
\citation{turney2010frequency}
\@writefile{toc}{\contentsline {paragraph}{\bf  Synonym detection sets}{27}{section*.16}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}The SimLex-999 Dataset}{27}{section.2.3}}
\newlabel{simlex}{{2.3}{27}{The SimLex-999 Dataset\relax }{section.2.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Choice of Concepts}{27}{subsection.2.3.1}}
\@writefile{toc}{\contentsline {paragraph}{Separating similarity from association}{27}{section*.17}}
\citation{leech1994claws4}
\citation{fellbaum1999wordnet}
\citation{hill2014multi,kielaimproving}
\@writefile{toc}{\contentsline {paragraph}{POS category}{28}{section*.18}}
\@writefile{toc}{\contentsline {paragraph}{Concreteness}{28}{section*.19}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces  Boxplots showing the interaction between concreteness and POS for concepts in USF. The white boxes range from the first to third quartiles and the central vertical line indicates the median.\relax }}{29}{figure.caption.20}}
\newlabel{fig1}{{2.1}{29}{Boxplots showing the interaction between concreteness and POS for concepts in USF. The white boxes range from the first to third quartiles and the central vertical line indicates the median.\relax \relax }{figure.caption.20}{}}
\@writefile{toc}{\contentsline {paragraph}{Final sampling}{30}{section*.21}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Question Design}{30}{subsection.2.3.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces  Instructions for SimLex-999 annotators.\relax }}{30}{figure.caption.22}}
\newlabel{fig2}{{2.2}{30}{Instructions for SimLex-999 annotators.\relax \relax }{figure.caption.22}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces  A group of noun pairs to be rated by moving the sliders. The rating slider was initially at position 0, and it was possible to attribute a rating of 0, although it was necessary to have actively moved the slider to that position to proceed to the next page.\relax }}{31}{figure.caption.23}}
\newlabel{fig3}{{2.3}{31}{A group of noun pairs to be rated by moving the sliders. The rating slider was initially at position 0, and it was possible to attribute a rating of 0, although it was necessary to have actively moved the slider to that position to proceed to the next page.\relax \relax }{figure.caption.23}{}}
\citation{huang2012improving}
\citation{resnik1995using,pedersen2004wordnet}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.3}Context-free rating}{32}{subsection.2.3.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.4}Questionnaire structure}{32}{subsection.2.3.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.5}Participants}{33}{subsection.2.3.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.6}Post-processing}{33}{subsection.2.3.6}}
\citation{pado2007flexible,reisinger2010mixture,silberer2014learning}
\citation{bruni2012distributional2}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Analysis of Dataset}{34}{section.2.4}}
\newlabel{analysis}{{2.4}{34}{Analysis of Dataset\relax }{section.2.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.1}Inter-annotator agreement}{34}{subsection.2.4.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces  {\bf  Left:} Inter-annotator agreement, measured by average pairwise Spearman \relax $\rho \relax \GenericError  {               }{LaTeX Error: Bad math environment delimiter}{See the LaTeX manual or LaTeX Companion for explanation.}{Your command was ignored.\MessageBreak Type  I <command> <return>  to replace it with another command,\MessageBreak or  <return>  to continue without it.} correlation, for ratings of concept types in SimLex-999. {\bf  Right:} Response consistency, reflecting the standard deviation of annotator ratings for each pair, averaged over all pairs in the concept category.\relax }}{34}{figure.caption.24}}
\newlabel{fig4}{{2.4}{34}{{\bf Left:} Inter-annotator agreement, measured by average pairwise Spearman \(\rho \) correlation, for ratings of concept types in SimLex-999. {\bf Right:} Response consistency, reflecting the standard deviation of annotator ratings for each pair, averaged over all pairs in the concept category.\relax \relax }{figure.caption.24}{}}
\citation{paivio1991dual}
\citation{williams2009predicting}
\citation{wiebe2000learning}
\@writefile{lot}{\contentsline {table}{\numberline {2.2}{\ignorespaces  {\bf  Top: Similarity aligns with association} Pairs with a small difference in rank between USF (association) and SimLex-999 (similarity) scores for each POS category. {\bf  Bottom: Similarity contrasts with association} Pairs with a high difference in rank for each POS category. *Note that the distribution of USF association scores on the interval [0,10] is highly skewed towards the lower bound in both SimLex-999 and the USF dataset as a whole.\relax }}{36}{table.caption.25}}
\newlabel{tab2}{{2.2}{36}{{\bf Top: Similarity aligns with association} Pairs with a small difference in rank between USF (association) and SimLex-999 (similarity) scores for each POS category. {\bf Bottom: Similarity contrasts with association} Pairs with a high difference in rank for each POS category. *Note that the distribution of USF association scores on the interval [0,10] is highly skewed towards the lower bound in both SimLex-999 and the USF dataset as a whole.\relax \relax }{table.caption.25}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.2}Response validity: Similarity not association}{36}{subsection.2.4.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces  {\bf  (a)} Pairs rated by WS-353 annotators (blue points, ranked by rating) and the corresponding rating of annotators following the SimLex-999 instructions (red points). {\bf  (b-c)} The same analysis, restricted to pairs in the WS-Sim or WS-Rel subsets of WS-353.\relax }}{36}{figure.caption.26}}
\newlabel{fig5}{{2.5}{36}{{\bf (a)} Pairs rated by WS-353 annotators (blue points, ranked by rating) and the corresponding rating of annotators following the SimLex-999 instructions (red points). {\bf (b-c)} The same analysis, restricted to pairs in the WS-Sim or WS-Rel subsets of WS-353.\relax \relax }{figure.caption.26}{}}
\citation{agirre2009study}
\citation{cruse1986lexical}
\citation{levy2015supervised}
\citation{rosch1976structural}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.3}Finer-grained Semantic Relations}{37}{subsection.2.4.3}}
\citation{baroni2014don}
\citation{Bengio2003lm}
\citation{collobert2008unified}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces  Average SimLex and USF free association scores across pairs representing different fine-grained semantic relations. All relations were extracted from WordNet. \relax $n\_hypernym\relax \GenericError  {               }{LaTeX Error: Bad math environment delimiter}{See the LaTeX manual or LaTeX Companion for explanation.}{Your command was ignored.\MessageBreak Type  I <command> <return>  to replace it with another command,\MessageBreak or  <return>  to continue without it.} refers to a direct hypernymy path of length \relax $n\relax \GenericError  {               }{LaTeX Error: Bad math environment delimiter}{See the LaTeX manual or LaTeX Companion for explanation.}{Your command was ignored.\MessageBreak Type  I <command> <return>  to replace it with another command,\MessageBreak or  <return>  to continue without it.}. Note that the average SimLex rating across all 999 word pairs (dashed red line) is much higher than the average USF rating (dashed golden line) because of differences in the rating procedure. The more interesting differences concern the relative strength of similarity vs. association across the different relation types.\relax }}{39}{figure.caption.27}}
\newlabel{fig6}{{2.6}{39}{Average SimLex and USF free association scores across pairs representing different fine-grained semantic relations. All relations were extracted from WordNet. \(n\_hypernym\) refers to a direct hypernymy path of length \(n\). Note that the average SimLex rating across all 999 word pairs (dashed red line) is much higher than the average USF rating (dashed golden line) because of differences in the rating procedure. The more interesting differences concern the relative strength of similarity vs. association across the different relation types.\relax \relax }{figure.caption.27}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Evaluating Models with SimLex-999}{39}{section.2.5}}
\newlabel{evaluation}{{2.5}{39}{Evaluating Models with SimLex-999\relax }{section.2.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.1}Neural language models for word representation}{39}{subsection.2.5.1}}
\newlabel{prev}{{2.5.1}{39}{Neural language models for word representation\relax }{subsection.2.5.1}{}}
\@writefile{toc}{\contentsline {paragraph}{\bf  Collobert \& Weston}{39}{section*.28}}
\citation{turian2010word}
\citation{collobert2008unified}
\citation{lewis2004rcv1}
\citation{huang2012improving}
\citation{collobert2008unified}
\citation{collobert2008unified}
\@writefile{toc}{\contentsline {paragraph}{\bf  Huang et al.}{40}{section*.29}}
\citation{mikolov2013efficient}
\citation{mikolov2013efficient,mikolov2013distributed}
\citation{mikolov2013efficient}
\citation{baroni2014don}
\@writefile{toc}{\contentsline {paragraph}{\bf  Log-linear models}{41}{section*.30}}
\citation{kiela2014systematic}
\citation{bird2006nltk}
\citation{recchia2009more}
\citation{landauer1997solution}
\citation{golub1970singular}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.2}\bf  Vector space (counting) models}{42}{subsection.2.5.2}}
\@writefile{toc}{\contentsline {paragraph}{\bf  LSA}{42}{section*.31}}
\citation{mikolov2013efficient}
\citation{huang2012improving}
\citation{collobert2008unified}
\citation{mikolov2013efficient}
\citation{huang2012improving}
\citation{collobert2008unified}
\citation{lewis2004rcv1}
\citation{lewis2004rcv1}
\citation{huang2012improving}
\citation{huang2012improving}
\citation{huang2012improving}
\citation{huang2012improving}
\citation{collobert2008unified}
\citation{mikolov2013efficient}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.3}Results}{43}{subsection.2.5.3}}
\newlabel{assoc}{{2.5.3}{43}{\bf Overall performance on SimLex-999\relax }{section*.32}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces Performance of NLMs on WS-353, MEN and SimLex-999. All models are trained on Wikipedia; note that as Wikipedia is constantly growing, the \cite {mikolov2013efficient} model exploited slightly more training data ($\approx $1000m tokens) than the \cite {huang2012improving} model ($\approx $990m), which in turn exploited more than the \cite {collobert2008unified} model ($\approx $852m). Dashed horizontal lines indicate the level of inter-annotator agreement for the three datasets.\relax }}{43}{figure.caption.33}}
\newlabel{fig7}{{2.7}{43}{Performance of NLMs on WS-353, MEN and SimLex-999. All models are trained on Wikipedia; note that as Wikipedia is constantly growing, the \protect \cite {mikolov2013efficient} model exploited slightly more training data ($\approx $1000m tokens) than the \protect \cite {huang2012improving} model ($\approx $990m), which in turn exploited more than the \protect \cite {collobert2008unified} model ($\approx $852m). Dashed horizontal lines indicate the level of inter-annotator agreement for the three datasets.\relax \relax }{figure.caption.33}{}}
\@writefile{toc}{\contentsline {paragraph}{\bf  Overall performance on SimLex-999}{43}{figure.caption.34}}
\citation{mikolov2013efficient}
\citation{lewis2004rcv1}
\citation{baroni2014don}
\citation{huang2012improving}
\citation{collobert2008unified}
\@writefile{lof}{\contentsline {figure}{\numberline {2.8}{\ignorespaces Comparison between the leading NLM, \emph  {Mikolov et al.}, the vector space model, \emph  {VSM}, and the \emph  {LSA} model. All models were trained on the $\approx $150m word RCV1 Corpus \citep {lewis2004rcv1}.\relax }}{44}{figure.caption.34}}
\newlabel{fig8}{{2.8}{44}{Comparison between the leading NLM, \emph {Mikolov et al.}, the vector space model, \emph {VSM}, and the \emph {LSA} model. All models were trained on the $\approx $150m word RCV1 Corpus \protect \citep {lewis2004rcv1}.\relax \relax }{figure.caption.34}{}}
\@writefile{toc}{\contentsline {paragraph}{\bf  Modeling similarity vs. association}{44}{section*.35}}
\citation{collobert2008unified}
\citation{huang2012improving}
\citation{mikolov2013efficient}
\citation{mikolov2013efficient}
\citation{turney2012domain}
\@writefile{lof}{\contentsline {figure}{\numberline {2.9}{\ignorespaces  The ability of NLMs to model the similarity of highly-associated concepts versus concepts in general. The two models on the right hand side also demonstrate the effect of training an NLM (the \cite {mikolov2013efficient} model) on running-text (\emph  {Mikolov et al.}) vs. on dependency-based input (\emph  {Levy \& Goldberg}).\relax }}{45}{figure.caption.36}}
\newlabel{fig9}{{2.9}{45}{The ability of NLMs to model the similarity of highly-associated concepts versus concepts in general. The two models on the right hand side also demonstrate the effect of training an NLM (the \protect \cite {mikolov2013efficient} model) on running-text (\emph {Mikolov et al.}) vs. on dependency-based input (\emph {Levy \& Goldberg}).\relax \relax }{figure.caption.36}{}}
\citation{levy2014dependency}
\citation{mikolov2013efficient}
\citation{mikolov2013efficient}
\citation{mikolov2013efficient}
\@writefile{lof}{\contentsline {figure}{\numberline {2.10}{\ignorespaces  The effect of different window sizes (indicated in square brackets [ ]) on NLM and LSA models. \relax }}{46}{figure.caption.37}}
\newlabel{fig10}{{2.10}{46}{The effect of different window sizes (indicated in square brackets [ ]) on NLM and LSA models. \relax \relax }{figure.caption.37}{}}
\newlabel{begin}{{2.5.3}{46}{\bf Learning concepts of different POS\relax }{section*.40}{}}
\@writefile{toc}{\contentsline {paragraph}{\bf  Learning concepts of different POS}{46}{section*.40}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.11}{\ignorespaces \nobreakspace  {} Performance of models on POS-based subsets of SimLex-999. The window size for each model is indicated in parentheses. Inter-annotator agreement for each POS is indicated by the dashed horizontal line.\relax }}{47}{figure.caption.38}}
\newlabel{fig11}{{2.11}{47}{~ Performance of models on POS-based subsets of SimLex-999. The window size for each model is indicated in parentheses. Inter-annotator agreement for each POS is indicated by the dashed horizontal line.\relax \relax }{figure.caption.38}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.12}{\ignorespaces  The importance of dependency-focussed contexts (in the Levy \& Goldberg model) for capturing concepts of different POS, when compared to a standard Skipgram (BOW) model trained on the same Wikipedia corpus.\relax }}{47}{figure.caption.39}}
\newlabel{fig12}{{2.12}{47}{The importance of dependency-focussed contexts (in the Levy \& Goldberg model) for capturing concepts of different POS, when compared to a standard Skipgram (BOW) model trained on the same Wikipedia corpus.\relax \relax }{figure.caption.39}{}}
\citation{levy2014dependency}
\citation{mikolov2013efficient}
\citation{markman1997similar}
\citation{sun2008verb}
\newlabel{end}{{2.5.3}{48}{\bf Learning concrete and abstract concepts\relax }{section*.41}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.13}{\ignorespaces  Performance of models on concreteness-based subsets of SimLex-999. Window size is indicated in parentheses. Horizontal dashed lines indicate inter-annotator agreement between SimLex-999 annotators on the two subsets.\relax }}{48}{figure.caption.42}}
\newlabel{fig13}{{2.13}{48}{Performance of models on concreteness-based subsets of SimLex-999. Window size is indicated in parentheses. Horizontal dashed lines indicate inter-annotator agreement between SimLex-999 annotators on the two subsets.\relax \relax }{figure.caption.42}{}}
\citation{hill2013concreteness}
\@writefile{toc}{\contentsline {paragraph}{\bf  Learning concrete and abstract concepts}{49}{figure.caption.42}}
\@writefile{toc}{\contentsline {section}{\numberline {2.6}Conclusion}{49}{section.2.6}}
\citation{gershmanmetaphor}
\citation{barsalou2003grounding}
\citation{dist}
\citation{andrews2009integrating}
\citation{barsalou2005situating}
\citation{feng2010visual,bruni2012distributional}
\citation{silberer2012grounded,rollermultimodal}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Improved distributed word representations by learning from more than text}{53}{chapter.3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Learning Distributed Word Representations from Text}{53}{section.3.1}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Modelling Word Acquisition with Multi-Modal Data and Neural Language Models}{53}{section.3.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Introduction}{53}{subsection.3.2.1}}
\citation{bruni2014multimodal}
\citation{silberer2012grounded}
\citation{rollermultimodal}
\citation{silberer2014learning}
\citation{paivio1991dual,hill2013quantitative}
\citation{leech1994claws4}
\citation{nelson2004university}
\citation{crutch2005abstract}
\citation{collobert2008unified,mesnil2012unsupervised}
\citation{mikolov2013efficient}
\citation{von2004labeling}
\citation{devereux2013centre}
\citation{nelson2004university}
\citation{mikolov2013efficient}
\citation{turian2010word}
\citation{mikolov2013efficient}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Model Design}{55}{subsection.3.2.2}}
\@writefile{toc}{\contentsline {paragraph}{Language-only Model}{55}{section*.43}}
\citation{mikolov2013efficient}
\citation{mikolov2013efficient}
\citation{duchi2011adaptive}
\citation{morin2005hierarchical}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Our multi-modal model architecture. Light boxes are elements of the original \cite  {mikolov2013efficient} model. For target words \relax $w_n\relax \GenericError  {               }{LaTeX Error: Bad math environment delimiter}{See the LaTeX manual or LaTeX Companion for explanation.}{Your command was ignored.\MessageBreak Type  I <command> <return>  to replace it with another command,\MessageBreak or  <return>  to continue without it.} in the domain of \relax $\mathbf  {P}\relax \GenericError  {               }{LaTeX Error: Bad math environment delimiter}{See the LaTeX manual or LaTeX Companion for explanation.}{Your command was ignored.\MessageBreak Type  I <command> <return>  to replace it with another command,\MessageBreak or  <return>  to continue without it.}, the model updates based on corpus context words \relax $ w_{n+i} \relax \GenericError  {               }{LaTeX Error: Bad math environment delimiter}{See the LaTeX manual or LaTeX Companion for explanation.}{Your command was ignored.\MessageBreak Type  I <command> <return>  to replace it with another command,\MessageBreak or  <return>  to continue without it.} then on words \relax $p^w_{n+i}\relax \GenericError  {               }{LaTeX Error: Bad math environment delimiter}{See the LaTeX manual or LaTeX Companion for explanation.}{Your command was ignored.\MessageBreak Type  I <command> <return>  to replace it with another command,\MessageBreak or  <return>  to continue without it.} in perceptual psuedo-sentences. Otherwise, updates are based solely on the \relax $ w_{n+i}. \relax \GenericError  {               }{LaTeX Error: Bad math environment delimiter}{See the LaTeX manual or LaTeX Companion for explanation.}{Your command was ignored.\MessageBreak Type  I <command> <return>  to replace it with another command,\MessageBreak or  <return>  to continue without it.}\relax }}{56}{figure.caption.44}}
\citation{mikolov2013efficient}
\citation{bybee2001frequency,chater2006probabilistic}
\@writefile{toc}{\contentsline {paragraph}{Multi-modal Extension}{57}{section*.45}}
\citation{mikolov2013efficient}
\citation{von2004labeling}
\citation{devereux2013centre}
\citation{mcrae2005semantic}
\citation{silberer2012grounded,rollermultimodal}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces  Example pseudo-sentences generated by our model.\relax }}{58}{figure.caption.46}}
\newlabel{examples}{{3.2}{58}{Example pseudo-sentences generated by our model.\relax \relax }{figure.caption.46}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.3}Information Sources}{58}{subsection.3.2.3}}
\newlabel{percep_sources}{{3.2.3}{58}{Information Sources\relax }{subsection.3.2.3}{}}
\@writefile{toc}{\contentsline {paragraph}{ESPGame Dataset}{58}{section*.47}}
\citation{fodor1983modularity}
\citation{nelson2004university}
\citation{andrews2009integrating,feng2010visual,silberer2012grounded,rollermultimodal}
\citation{huang2012improving,silberer2012grounded}
\@writefile{toc}{\contentsline {paragraph}{CSLB Property Norms}{59}{section*.48}}
\@writefile{toc}{\contentsline {paragraph}{Linguistic Input}{59}{section*.49}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.4}Evaluation}{59}{subsection.3.2.4}}
\citation{leech1994claws4}
\@writefile{lot}{\contentsline {table}{\numberline {3.1}{\ignorespaces  Concepts identified in images in the ESP Game (left) and features produced for concepts by human annotators in the CSLB dataset (with feature strength, max=30).\relax }}{60}{table.caption.50}}
\newlabel{font-table}{{3.1}{60}{Concepts identified in images in the ESP Game (left) and features produced for concepts by human annotators in the CSLB dataset (with feature strength, max=30).\relax \relax }{table.caption.50}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.2}{\ignorespaces  Example concept pairs (with mean concreteness rating) and free-association scores from the USF dataset.\relax }}{60}{table.caption.51}}
\newlabel{font-table}{{3.2}{60}{Example concept pairs (with mean concreteness rating) and free-association scores from the USF dataset.\relax \relax }{table.caption.51}{}}
\citation{mikolov2013efficient}
\citation{silberer2012grounded}
\citation{hardoon2004canonical}
\citation{bruni2014multimodal}
\citation{bruni2014multimodal}
\@writefile{lot}{\contentsline {table}{\numberline {3.3}{\ignorespaces  Details the subsets of USF data used in our evaluations, downloadable from our website.\relax }}{61}{table.caption.52}}
\newlabel{font-table}{{3.3}{61}{Details the subsets of USF data used in our evaluations, downloadable from our website.\relax \relax }{table.caption.52}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.5}Results and Discussion}{61}{subsection.3.2.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.6}Combining information sources}{61}{subsection.3.2.6}}
\citation{johns2012perceptual}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.7}Propagating input to abstract concepts}{62}{subsection.3.2.7}}
\@writefile{toc}{\contentsline {paragraph}{Johns and Jones}{62}{section*.53}}
\citation{johns2012perceptual}
\citation{myers1990classical}
\citation{mikolov2013efficient}
\citation{feng2010visual,silberer2012grounded,silberer2013models}
\@writefile{toc}{\contentsline {paragraph}{Ridge Regression}{63}{section*.54}}
\@writefile{toc}{\contentsline {paragraph}{Comparisons}{63}{section*.55}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces  The proposed approach compared with other methods of information combination (left) and propagation. Dashed lines indicate language-only model baseline.\relax }}{64}{figure.caption.56}}
\newlabel{main_results}{{3.3}{64}{The proposed approach compared with other methods of information combination (left) and propagation. Dashed lines indicate language-only model baseline.\relax \relax }{figure.caption.56}{}}
\citation{katja2005content}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces  Top: Comparing the strategy of directly representing abstract concepts from perceptual information where available (yellow bars) vs. propagating via concrete concepts. Bottom: The effect of increasing \relax $\alpha \relax \GenericError  {               }{LaTeX Error: Bad math environment delimiter}{See the LaTeX manual or LaTeX Companion for explanation.}{Your command was ignored.\MessageBreak Type  I <command> <return>  to replace it with another command,\MessageBreak or  <return>  to continue without it.} on correlation with USF pairs (Spearman \relax $\rho \relax \GenericError  {               }{LaTeX Error: Bad math environment delimiter}{See the LaTeX manual or LaTeX Companion for explanation.}{Your command was ignored.\MessageBreak Type  I <command> <return>  to replace it with another command,\MessageBreak or  <return>  to continue without it.}) for each concept type. Horizontal dashed lines indicate language-only model baseline.\relax }}{65}{figure.caption.57}}
\newlabel{repprop}{{3.4}{65}{Top: Comparing the strategy of directly representing abstract concepts from perceptual information where available (yellow bars) vs. propagating via concrete concepts. Bottom: The effect of increasing \(\alpha \) on correlation with USF pairs (Spearman \(\rho \)) for each concept type. Horizontal dashed lines indicate language-only model baseline.\relax \relax }{figure.caption.57}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.8}Direct representation vs. propagation}{65}{subsection.3.2.8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.9}Source and quantity of perceptual input}{66}{subsection.3.2.9}}
\citation{srivastava2012multimodal,wu2013online}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.10}Conclusions}{67}{subsection.3.2.10}}
\@writefile{toc}{\contentsline {paragraph}{Type I}{67}{section*.58}}
\@writefile{toc}{\contentsline {paragraph}{Type II}{67}{section*.59}}
\citation{grafton2009embodied,barsalou2010grounded}
\citation{landauer1997solution,turney2010frequency}
\citation{Bengio2003lm,mnih2009scalable,collobert2008unified}
\citation{mikolov2013distributed,Pennington2014}
\citation{baroni2014don}
\@writefile{toc}{\contentsline {paragraph}{Type III}{68}{section*.60}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Multi-modal fusion based on image dispersion}{68}{section.3.3}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Sequence-to-Sequence Learning of Word Representations From Bilingual Data}{68}{section.3.4}}
\citation{kalchbrenner13emnlp,devlin2014fast,Sutskever2014sequence}
\@writefile{toc}{\contentsline {section}{\numberline {3.5}Learning Embeddings with Neural Language Models}{69}{section.3.5}}
\citation{Bengio2003lm}
\citation{collobert2008unified}
\citation{mikolov2013distributed,Pennington2014}
\citation{mikolov2013distributed}
\citation{Hill2014EMNLP,levy2014dependency}
\citation{Haghighi2008Learning,vulic2011identifying,mikolov2013exploiting,Hermann:2014:ICLR,lauly2014autoencoder}
\citation{Haghighi2008Learning,vulic2011identifying,mikolov2013exploiting}
\citation{Klementiev,Hermann:2014:ICLR,lauly2014autoencoder,Kocisky:2014}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.1}Monolingual Models}{70}{subsection.3.5.1}}
\citation{Hermann:2014:ICLR}
\citation{Klementiev}
\citation{Hermann:2014:ICLR}
\citation{lauly2014autoencoder}
\citation{Hermann:2014:ICLR}
\citation{faruqui2014improving}
\citation{faruqui2014improving}
\citation{faruqui2014improving}
\citation{kalchbrenner13emnlp,Sutskever2014sequence}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.2}Bilingual Representation-learning Models}{71}{subsection.3.5.2}}
\citation{bahdanau2014neural}
\citation{kalchbrenner13emnlp,Cho2014,bahdanau2014neural}
\citation{Cho2014}
\citation{bahdanau2014neural}
\citation{kalchbrenner13emnlp}
\citation{devlin2014fast}
\citation{Sutskever2014sequence}
\citation{Cho2014}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.3}Neural Machine Translation Models}{72}{subsection.3.5.3}}
\@writefile{toc}{\contentsline {section}{\numberline {3.6}Experiments}{72}{section.3.6}}
\citation{Hermann:2014:ICLR}
\citation{faruqui2014improving}
\citation{mikolov2013distributed}
\citation{Pennington2014}
\citation{collobert2008unified}
\citation{Agirre2009,bruni2014multimodal,baroni2014don}
\citation{Agirre2009}
\citation{bruni2014multimodal}
\citation{hill2014simlex}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6.1}Similarity and relatedness modelling}{73}{subsection.3.6.1}}
\citation{hill2014simlex}
\citation{faruqui2014improving}
\citation{hill2014simlex}
\citation{nelson2004university}
\@writefile{lot}{\contentsline {table}{\numberline {3.4}{\ignorespaces  NMT embeddings (RNNenc and RNNsearch) clearly outperform alternative embedding-learning architectures on tasks that require modelling similarity (below the dashed line), but not on tasks that reflect relatedness. Bilingual embedding spaces learned without the translation objective are somewhere between these two extremes.\relax }}{74}{table.caption.61}}
\newlabel{table:perf}{{3.4}{74}{NMT embeddings (RNNenc and RNNsearch) clearly outperform alternative embedding-learning architectures on tasks that require modelling similarity (below the dashed line), but not on tasks that reflect relatedness. Bilingual embedding spaces learned without the translation objective are somewhere between these two extremes.\relax \relax }{table.caption.61}{}}
\citation{landauer1997solution}
\@writefile{lot}{\contentsline {table}{\numberline {3.5}{\ignorespaces Nearest neighbours (excluding plurals) in the embedding spaces of different models. All models were trained for 6 epochs on the translation corpus except CW and FD (as noted previously). NMT embedding spaces are oriented according to similarity, whereas embeddings learned by monolingual models are organized according to relatedness. The other bilingual model BiCVM also exhibits a notable focus on similarity.\relax }}{75}{table.caption.62}}
\newlabel{table:neigh}{{3.5}{75}{Nearest neighbours (excluding plurals) in the embedding spaces of different models. All models were trained for 6 epochs on the translation corpus except CW and FD (as noted previously). NMT embedding spaces are oriented according to similarity, whereas embeddings learned by monolingual models are organized according to relatedness. The other bilingual model BiCVM also exhibits a notable focus on similarity.\relax \relax }{table.caption.62}{}}
\citation{mikolov2013distributed}
\citation{mikolov2013distributed}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6.2}Importance of training data quantity}{76}{subsection.3.6.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6.3}Analogy Resolution}{76}{subsection.3.6.3}}
\citation{mikolov2013distributed}
\citation{faruqui2014improving}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces The effect of increasing the amount of training data on the quality of monolingual embeddings, based on similarity-based evaluations (SimLex-999) and two relatedness-based evaluations (MEN and WordSim-353). \emph  {ET} in the legend indicates models trained on the English half of the translation corpus. \emph  {Wiki} indicates models trained on Wikipedia.\relax }}{77}{figure.caption.63}}
\newlabel{fig:size}{{3.5}{77}{The effect of increasing the amount of training data on the quality of monolingual embeddings, based on similarity-based evaluations (SimLex-999) and two relatedness-based evaluations (MEN and WordSim-353). \emph {ET} in the legend indicates models trained on the English half of the translation corpus. \emph {Wiki} indicates models trained on Wikipedia.\relax \relax }{figure.caption.63}{}}
\citation{levy2014dependency}
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces Translation-based embeddings perform best on syntactic analogies (\emph  {run,ran: hide, hid}). Monolingual skipgram/Glove models are better at semantic analogies (\emph  {father, man; mother, woman})\relax }}{78}{figure.caption.64}}
\newlabel{fig:analogy}{{3.6}{78}{Translation-based embeddings perform best on syntactic analogies (\emph {run,ran: hide, hid}). Monolingual skipgram/Glove models are better at semantic analogies (\emph {father, man; mother, woman})\relax \relax }{figure.caption.64}{}}
\citation{morin2005hierarchical}
\citation{mikolov2013distributed,Hermann:2014:ICLR}
\@writefile{toc}{\contentsline {section}{\numberline {3.7}Effect of Target Language}{79}{section.3.7}}
\newlabel{lang_effects}{{3.7}{79}{Effect of Target Language\relax }{section.3.7}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.6}{\ignorespaces Comparison of embeddings learned by RNN Search models translating between English-French (EN-FR) and English-German (EN-DE) on all semantic evaluations (left) and nearest neighbours of selected cue words (right). Bold italics indicate target-language-specific effects. Evaluation items and vocabulary searches were restricted to words common to both models. \relax }}{79}{table.caption.65}}
\newlabel{table:de}{{3.6}{79}{Comparison of embeddings learned by RNN Search models translating between English-French (EN-FR) and English-German (EN-DE) on all semantic evaluations (left) and nearest neighbours of selected cue words (right). Bold italics indicate target-language-specific effects. Evaluation items and vocabulary searches were restricted to words common to both models. \relax \relax }{table.caption.65}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.8}Overcoming the Vocabulary Size Problem}{79}{section.3.8}}
\citation{Jean}
\citation{Bengio+Senecal-2003-small}
\citation{luong2014addressing}
\@writefile{lot}{\contentsline {table}{\numberline {3.7}{\ignorespaces Comparison of embeddings learned by the original (RNN Search - 30k French words, 50k German words) and extended-vocabulary (RNN Search-LV -500k words) models translating from English to French (EN-FR) and from English to German (EN-DE). For fair comparisons, all evaluations were restricted to the intersection of all model vocabularies.\relax }}{81}{table.caption.66}}
\newlabel{table:ex}{{3.7}{81}{Comparison of embeddings learned by the original (RNN Search - 30k French words, 50k German words) and extended-vocabulary (RNN Search-LV -500k words) models translating from English to French (EN-FR) and from English to German (EN-DE). For fair comparisons, all evaluations were restricted to the intersection of all model vocabularies.\relax \relax }{table.caption.66}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.9}How Similarity Emerges}{81}{section.3.9}}
\newlabel{section:exp}{{3.9}{81}{How Similarity Emerges\relax }{section.3.9}{}}
\citation{faruqui2014improving}
\citation{hill2014simlex}
\citation{faruqui2014improving}
\@writefile{toc}{\contentsline {section}{\numberline {3.10}Conclusion}{82}{section.3.10}}
\citation{Hermann:2014:ICLR}
\citation{faruqui2014improving}
\citation{Kocisky:2014}
\citation{dist}
\citation{mikolov2013distributed}
\citation{zock2004word}
\citation{307754,Klementiev,hermann2013multilingual,gouws2014bilbowa}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Representing phrases with neural language models}{85}{chapter.4}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Introduction}{85}{section.4.1}}
\citation{mikolov2010recurrent}
\citation{kiros2014unifying}
\citation{bahdanau2014neural}
\citation{mikolov2013distributed}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Neural Language Model Architectures}{86}{section.4.2}}
\citation{bengio1994learning}
\citation{hochreiter1997long}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}Long Short Term Memory}{87}{subsection.4.2.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}Bag-of-Words NLMs}{88}{subsection.4.2.2}}
\citation{huang2012improving}
\citation{bordes2015large}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.3}Pre-trained Input Representations}{89}{subsection.4.2.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.4}Training Objective}{89}{subsection.4.2.4}}
\citation{faruqui2014retrofitting}
\citation{cho2014learning}
\citation{bergstra+al:2010-scipy}
\citation{zeiler2012adadelta}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.5}Implementation Details}{90}{subsection.4.2.5}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Reverse Dictionaries}{90}{section.4.3}}
\citation{bilac2003improving,bilac2004dictionary,zock2004word}
\citation{shaw2013building}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.1}Data Collection and Training}{91}{subsection.4.3.1}}
\citation{mitchell2010composition}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.2}Comparisons}{92}{subsection.4.3.2}}
\citation{shaw2013building}
\@writefile{lot}{\contentsline {table}{\numberline {4.1}{\ignorespaces Performance of different reverse dictionary models in different evaluation settings. *Low variance in \emph  {mult} models is due to consistently poor scores, so not highlighted.\relax }}{93}{table.caption.67}}
\newlabel{results}{{4.1}{93}{Performance of different reverse dictionary models in different evaluation settings. *Low variance in \emph {mult} models is due to consistently poor scores, so not highlighted.\relax \relax }{table.caption.67}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.3}Reverse Dictionary Evaluation}{93}{subsection.4.3.3}}
\citation{leech1994claws4}
\@writefile{lot}{\contentsline {table}{\numberline {4.2}{\ignorespaces Style difference between \emph  {dictionary definitions} and \emph  {concept descriptions} in the evaluation.\relax }}{94}{table.caption.68}}
\newlabel{tb:tablename}{{4.2}{94}{Style difference between \emph {dictionary definitions} and \emph {concept descriptions} in the evaluation.\relax \relax }{table.caption.68}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.4}Results}{94}{subsection.4.3.4}}
\citation{iyyer2015deep}
\@writefile{lot}{\contentsline {table}{\numberline {4.3}{\ignorespaces The top-five candidates for example queries (invented by the authors) from different reverse dictionary models. Both the RNN and BOW models are without Word2Vec input and use the cosine loss.\relax }}{96}{table.caption.69}}
\newlabel{qual}{{4.3}{96}{The top-five candidates for example queries (invented by the authors) from different reverse dictionary models. Both the RNN and BOW models are without Word2Vec input and use the cosine loss.\relax \relax }{table.caption.69}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.5}Qualitative Analysis}{96}{subsection.4.3.5}}
\citation{hermann2013multilingual,lauly2014autoencoder,gouws2014bilbowa}
\citation{gouws2014bilbowa}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.6}Cross-Lingual Reverse Dictionaries}{97}{subsection.4.3.6}}
\@writefile{lot}{\contentsline {table}{\numberline {4.4}{\ignorespaces Responses from cross-lingual reverse dictionary models to selected queries. Underlined responses are `correct' or potentially useful for a native French speaker.\relax }}{97}{table.caption.70}}
\newlabel{cross}{{4.4}{97}{Responses from cross-lingual reverse dictionary models to selected queries. Underlined responses are `correct' or potentially useful for a native French speaker.\relax \relax }{table.caption.70}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.7}Discussion}{98}{subsection.4.3.7}}
\citation{ferrucci2010building}
\citation{molla2007question}
\citation{Iyyer:Boyd-Graber:Claudino:Socher:Daume-2014,weston2015towards}
\citation{berant14paraphrasing,bordes2014question}
\citation{littman2002probabilistic}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}General Knowledge (crossword) Question Answering}{99}{section.4.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.1}Evaluation}{99}{subsection.4.4.1}}
\citation{littman2002probabilistic}
\citation{ginsberg2011dr}
\@writefile{lot}{\contentsline {table}{\numberline {4.5}{\ignorespaces Examples of the different question types in the crossword question evaluation dataset.\relax }}{100}{table.caption.71}}
\newlabel{tb:tablename}{{4.5}{100}{Examples of the different question types in the crossword question evaluation dataset.\relax \relax }{table.caption.71}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.2}Benchmarks and Comparisons}{100}{subsection.4.4.2}}
\citation{littman2002probabilistic,ginsberg2011dr}
\@writefile{lot}{\contentsline {table}{\numberline {4.6}{\ignorespaces Performance of different models on crossword questions of different length. The two commercial systems are evaluated via their web interface so only accuracy@10 can be reported in those cases. \relax }}{101}{table.caption.72}}
\newlabel{results2}{{4.6}{101}{Performance of different models on crossword questions of different length. The two commercial systems are evaluated via their web interface so only accuracy@10 can be reported in those cases. \relax \relax }{table.caption.72}{}}
\newlabel{tb:tablename}{{4.6}{101}{Performance of different models on crossword questions of different length. The two commercial systems are evaluated via their web interface so only accuracy@10 can be reported in those cases. \relax \relax }{table.caption.72}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.3}Results}{101}{subsection.4.4.3}}
\citation{mikolov2013distributed}
\@writefile{lot}{\contentsline {table}{\numberline {4.7}{\ignorespaces Responses from different models to example crossword clues. In each case the model output is filtered to exclude any candidates that are not of the same length as the correct answer. BOW and RNN models are trained without Word2Vec input embeddings and cosine loss.\relax }}{102}{table.caption.73}}
\newlabel{egs}{{4.7}{102}{Responses from different models to example crossword clues. In each case the model output is filtered to exclude any candidates that are not of the same length as the correct answer. BOW and RNN models are trained without Word2Vec input embeddings and cosine loss.\relax \relax }{table.caption.73}{}}
\citation{bordes2014question,graves2014neural,weston2015towards}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.4}Qualitative Analysis}{103}{subsection.4.4.4}}
\citation{graves2014neural,weston2015towards}
\@writefile{toc}{\contentsline {section}{\numberline {4.5}Conclusion}{104}{section.4.5}}
\citation{baroni2014don}
\citation{levy2014neural}
\citation{hill2014simlex}
\citation{collobert2011natural}
\citation{mitchell2008vector,clark2007combining,baroni2014frege,milajevs2014evaluating}
\citation{Sutskever2014sequence}
\citation{mao2014deep}
\citation{serban2015building}
\citation{cho2014learning}
\citation{norman1972memory}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Representing sentences with neural language models}{105}{chapter.5}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.0.1}Introduction}{105}{subsection.5.0.1}}
\citation{kiros2015skip}
\citation{hill2015learning}
\citation{socher2011semi,kalchbrenner2014convolutional}
\citation{milajevs2014evaluating}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.0.2}Distributed Sentence Representations}{106}{subsection.5.0.2}}
\citation{kiros2015skip}
\citation{cho2014learning}
\citation{le2014distributed}
\citation{mikolov2013distributed}
\citation{mitchell2010composition}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.0.3}Existing Models Trained on Text}{107}{subsection.5.0.3}}
\citation{marcobaronijointly}
\citation{hill2015learning}
\citation{chen2015microsoft}
\citation{szegedy2014going}
\citation{russakovsky2014imagenet}
\citation{cho2014learning}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.0.4}Models Trained on Structured Resources}{108}{subsection.5.0.4}}
\citation{vincent2008extracting}
\citation{vincent2010stacked}
\citation{dai2015semi}
\citation{iyyer2015deep}
\citation{sutskever2011generating}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.0.5}Novel Text-Based Models}{109}{subsection.5.0.5}}
\citation{harris1954distributional,polajnar2015exploration}
\newlabel{eqn1}{{5.1}{110}{Novel Text-Based Models\relax }{equation.5.0.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.0.6}Training and Model Selection}{110}{subsection.5.0.6}}
\citation{collobert2011natural,mikolov2013efficient,kiros2015skip}
\citation{hill2015learning,baroni2014don,levy2015improving}
\@writefile{lot}{\contentsline {table}{\numberline {5.1}{\ignorespaces  {\bf  Properties of models compared in this study} \hspace  {6mm} {\bf  OS:} requires training corpus of sentences in order. {\bf  R:} requires structured resource for training. {\bf  WO:} encoder sensitive to word order. {\bf  SD:} dimension of sentence representation. {\bf  WD:} dimension of word representation. {\bf  TR:} approximate training time (hours) on the dataset in this paper. * indicates trained on GPU. {\bf  TE:} approximate time (s) taken to encode 0.5m sentences.\relax }}{111}{table.caption.74}}
\newlabel{modelprops}{{5.1}{111}{{\bf Properties of models compared in this study} \hspace {6mm} {\bf OS:} requires training corpus of sentences in order. {\bf R:} requires structured resource for training. {\bf WO:} encoder sensitive to word order. {\bf SD:} dimension of sentence representation. {\bf WD:} dimension of word representation. {\bf TR:} approximate training time (hours) on the dataset in this paper. * indicates trained on GPU. {\bf TE:} approximate time (s) taken to encode 0.5m sentences.\relax \relax }{table.caption.74}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.2}{\ignorespaces  Example sentence pairs and `similarity' ratings from the unsupervised evaluations used in this study.\relax }}{111}{table.caption.75}}
\newlabel{unsex}{{5.2}{111}{Example sentence pairs and `similarity' ratings from the unsupervised evaluations used in this study.\relax \relax }{table.caption.75}{}}
\citation{dolan2004unsupervised}
\citation{pang2005seeing}
\citation{hu2004mining}
\citation{pang2004sentimental}
\citation{wiebe2005annotating}
\citation{voorhees2002overview}
\citation{kiros2015skip}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.0.7}Evaluating Sentence Representations}{112}{subsection.5.0.7}}
\@writefile{lot}{\contentsline {table}{\numberline {5.3}{\ignorespaces  Performance of sentence representation models on {\bf  supervised} evaluations (Section\nobreakspace  {}\ref  {supersec}). Bold numbers indicate best performance in class. Underlined indicates best overall. \relax }}{112}{table.caption.76}}
\newlabel{supervised}{{5.3}{112}{Performance of sentence representation models on {\bf supervised} evaluations (Section~\ref {supersec}). Bold numbers indicate best performance in class. Underlined indicates best overall. \relax \relax }{table.caption.76}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.0.8}Supervised Evaluations}{112}{subsection.5.0.8}}
\newlabel{supersec}{{5.0.8}{112}{Supervised Evaluations\relax }{subsection.5.0.8}{}}
\citation{marelli2014sick}
\citation{agirre2014semeval}
\@writefile{lot}{\contentsline {table}{\numberline {5.4}{\ignorespaces  Performance of sentence representation models (Spearman/Pearson correlations) on {\bf  unsupervised} (relatedness) evaluations (Section\nobreakspace  {}\ref  {unseval}). Models are grouped according to training data as indicated in Table\nobreakspace  {}\ref  {supervised}.\relax }}{113}{table.caption.77}}
\newlabel{unsupervised}{{5.4}{113}{Performance of sentence representation models (Spearman/Pearson correlations) on {\bf unsupervised} (relatedness) evaluations (Section~\ref {unseval}). Models are grouped according to training data as indicated in Table~\ref {supervised}.\relax \relax }{table.caption.77}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.0.9}Unsupervised Evaluations}{113}{subsection.5.0.9}}
\newlabel{unseval}{{5.0.9}{113}{Unsupervised Evaluations\relax }{subsection.5.0.9}{}}
\citation{ji2013discriminative}
\citation{vincent2008extracting}
\citation{marcobaronijointly}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.0.10}Results}{114}{subsection.5.0.10}}
\citation{voorhees2002overview}
\citation{bruni2014multimodal}
\citation{almahairi2015learning}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.0.11}Discussion}{115}{subsection.5.0.11}}
\@writefile{lot}{\contentsline {table}{\numberline {5.5}{\ignorespaces Sample nearest neighbour queries selected from a randomly sampled 0.5m sentences of the Toronto Books Corpus.\relax }}{116}{table.caption.78}}
\newlabel{neighbours}{{5.5}{116}{Sample nearest neighbour queries selected from a randomly sampled 0.5m sentences of the Toronto Books Corpus.\relax \relax }{table.caption.78}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.6}{\ignorespaces  Internal consistency (Chronbach's \relax $\alpha \relax \GenericError  {               }{LaTeX Error: Bad math environment delimiter}{See the LaTeX manual or LaTeX Companion for explanation.}{Your command was ignored.\MessageBreak Type  I <command> <return>  to replace it with another command,\MessageBreak or  <return>  to continue without it.}) among evaluations when individual benchmarks are left out of the (supervised or unsupervised) cohorts. Consistency rank within cohort is in parentheses (1 = most consistent with other evaluations).\relax }}{116}{table.caption.79}}
\newlabel{consistency}{{5.6}{116}{Internal consistency (Chronbach's \(\alpha \)) among evaluations when individual benchmarks are left out of the (supervised or unsupervised) cohorts. Consistency rank within cohort is in parentheses (1 = most consistent with other evaluations).\relax \relax }{table.caption.79}{}}
\citation{wiebe2005annotating}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.0.12}Conclusion}{117}{subsection.5.0.12}}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Representing semantics in memory networks}{119}{chapter.6}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {chapter}{\numberline {7}Conclusion}{121}{chapter.7}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\bibstyle{plainnat}
\bibdata{thesis}
\bibcite{Agirre2009}{{1}{2009{a}}{{Agirre et~al.}}{{Agirre, Alfonseca, Hall, Kravalova, Pasca, and Soroa}}}
\bibcite{agirre2009study}{{2}{2009{b}}{{Agirre et~al.}}{{Agirre, Alfonseca, Hall, Kravalova, Pa{\c {s}}ca, and Soroa}}}
\bibcite{agirre2014semeval}{{3}{2014}{{Agirre et~al.}}{{Agirre, Banea, Cardie, Cer, Diab, Gonzalez-Agirre, Guo, Mihalcea, Rigau, and Wiebe}}}
\bibcite{alfonseca2002extending}{{4}{2002}{{Alfonseca and Manandhar}}{{}}}
\bibcite{almahairi2015learning}{{5}{2015}{{Almahairi et~al.}}{{Almahairi, Kastner, Cho, and Courville}}}
\bibcite{andrews2009integrating}{{6}{2009}{{Andrews et~al.}}{{Andrews, Vigliocco, and Vinson}}}
\bibcite{bahdanau2014neural}{{7}{2015}{{Bahdanau et~al.}}{{Bahdanau, Cho, and Bengio}}}
\bibcite{bansal2014tailoring}{{8}{2014}{{Bansal et~al.}}{{Bansal, Gimpel, and Livescu}}}
\@writefile{toc}{\contentsline {chapter}{Bibliography}{123}{section*.80}}
\bibcite{baroni2014frege}{{9}{2014{a}}{{Baroni et~al.}}{{Baroni, Bernardi, and Zamparelli}}}
\bibcite{baroni2014don}{{10}{2014{b}}{{Baroni et~al.}}{{Baroni, Dinu, and Kruszewski}}}
\bibcite{barsalou2010grounded}{{11}{2010}{{Barsalou}}{{}}}
\bibcite{barsalou2005situating}{{12}{2005}{{Barsalou and Wiemer-Hastings}}{{}}}
\bibcite{barsalou2003grounding}{{13}{2003}{{Barsalou et~al.}}{{Barsalou, Kyle~Simmons, Barbey, and Wilson}}}
\bibcite{beltagysemantic}{{14}{2014}{{Beltagy et~al.}}{{Beltagy, Erk, and Mooney}}}
\bibcite{Bengio+Senecal-2003-small}{{15}{2003}{{Bengio and S\'en\'ecal}}{{}}}
\bibcite{bengio1994learning}{{16}{1994}{{Bengio et~al.}}{{Bengio, Simard, and Frasconi}}}
\bibcite{Bengio2003lm}{{17}{2003{a}}{{Bengio et~al.}}{{Bengio, Ducharme, Vincent, and Janvin}}}
\bibcite{bengio2003neural}{{18}{2003{b}}{{Bengio et~al.}}{{Bengio, Ducharme, Vincent, and Janvin}}}
\bibcite{berant14paraphrasing}{{19}{2014}{{Berant and Liang}}{{}}}
\bibcite{bergstra+al:2010-scipy}{{20}{2010}{{Bergstra et~al.}}{{Bergstra, Breuleux, Bastien, Lamblin, Pascanu, Desjardins, Turian, Warde-Farley, and Bengio}}}
\bibcite{bernardi2013relatedness}{{21}{2013}{{Bernardi et~al.}}{{Bernardi, Dinu, Marelli, and Baroni}}}
\bibcite{bilac2003improving}{{22}{2003}{{Bilac et~al.}}{{Bilac, Baldwin, and Tanaka}}}
\bibcite{bilac2004dictionary}{{23}{2004}{{Bilac et~al.}}{{Bilac, Watanabe, Hashimoto, Tokunaga, and Tanaka}}}
\bibcite{bird2006nltk}{{24}{2006}{{Bird}}{{}}}
\bibcite{blei2003latent}{{25}{2003}{{Blei et~al.}}{{Blei, Ng, and Jordan}}}
\bibcite{bordes2014question}{{26}{2014}{{Bordes et~al.}}{{Bordes, Chopra, and Weston}}}
\bibcite{bordes2015large}{{27}{2015}{{Bordes et~al.}}{{Bordes, Usunier, Chopra, and Weston}}}
\bibcite{bruni2012distributional}{{28}{2012{a}}{{Bruni et~al.}}{{Bruni, Boleda, Baroni, and Tran}}}
\bibcite{bruni2012distributional2}{{29}{2012{b}}{{Bruni et~al.}}{{Bruni, Uijlings, Baroni, and Sebe}}}
\bibcite{bruni2014multimodal}{{30}{2014}{{Bruni et~al.}}{{Bruni, Tran, and Baroni}}}
\bibcite{budanitsky2006evaluating}{{31}{2006}{{Budanitsky and Hirst}}{{}}}
\bibcite{bybee2001frequency}{{32}{2001}{{Bybee and Hopper}}{{}}}
\bibcite{lauly2014autoencoder}{{33}{2014}{{Chandar et~al.}}{{Chandar, Lauly, Larochelle, Khapra, Ravindran, Raykar, and Saha}}}
\bibcite{chater2006probabilistic}{{34}{2006}{{Chater and Manning}}{{}}}
\bibcite{chen2015microsoft}{{35}{2015}{{Chen et~al.}}{{Chen, Fang, Lin, Vedantam, Gupta, Dollar, and Zitnick}}}
\bibcite{cho2014learning}{{36}{2014{a}}{{Cho et~al.}}{{Cho, Van~Merri{\"e}nboer, Gulcehre, Bahdanau, Bougares, Schwenk, and Bengio}}}
\bibcite{Cho2014}{{37}{2014{b}}{{Cho et~al.}}{{Cho, van Merrienboer, Gulcehre, Bougares, Schwenk, and Bengio}}}
\bibcite{clark2007combining}{{38}{2007}{{Clark and Pulman}}{{}}}
\bibcite{collobert2008unified}{{39}{2008}{{Collobert and Weston}}{{}}}
\bibcite{collobert2011natural}{{40}{2011}{{Collobert et~al.}}{{Collobert, Weston, Bottou, Karlen, Kavukcuoglu, and Kuksa}}}
\bibcite{cruse1986lexical}{{41}{1986}{{Cruse}}{{}}}
\bibcite{crutch2005abstract}{{42}{2005}{{Crutch and Warrington}}{{}}}
\bibcite{crutch2009different}{{43}{2009}{{Crutch et~al.}}{{Crutch, Connell, and Warrington}}}
\bibcite{cunningham2005information}{{44}{2005}{{Cunningham}}{{}}}
\bibcite{dai2015semi}{{45}{2015}{{Dai and Le}}{{}}}
\bibcite{devereux2013centre}{{46}{2013}{{Devereux et~al.}}{{Devereux, Tyler, Geertzen, and Randall}}}
\bibcite{devlin2014fast}{{47}{2014}{{Devlin et~al.}}{{Devlin, Zbib, Huang, Lamar, Schwartz, and Makhoul}}}
\bibcite{dolan2004unsupervised}{{48}{2004}{{Dolan et~al.}}{{Dolan, Quirk, and Brockett}}}
\bibcite{duchi2011adaptive}{{49}{2011}{{Duchi et~al.}}{{Duchi, Hazan, and Singer}}}
\bibcite{faruqui2014improving}{{50}{2014}{{Faruqui and Dyer}}{{}}}
\bibcite{faruqui2014retrofitting}{{51}{2014}{{Faruqui et~al.}}{{Faruqui, Dodge, Jauhar, Dyer, Hovy, and Smith}}}
\bibcite{fellbaum1999wordnet}{{52}{1999}{{Fellbaum}}{{}}}
\bibcite{feng2010visual}{{53}{2010}{{Feng and Lapata}}{{}}}
\bibcite{ferrucci2010building}{{54}{2010}{{Ferrucci et~al.}}{{Ferrucci, Brown, Chu-Carroll, Fan, Gondek, Kalyanpur, Lally, Murdock, Nyberg, Prager, Schlaefer, and Welty}}}
\bibcite{finkelstein2001placing}{{55}{2001}{{Finkelstein et~al.}}{{Finkelstein, Gabrilovich, Matias, Rivlin, Solan, Wolfman, and Ruppin}}}
\bibcite{dist}{{56}{1957}{{Firth}}{{}}}
\bibcite{fodor1983modularity}{{57}{1983}{{Fodor}}{{}}}
\bibcite{gentner1978relational}{{58}{1978}{{Gentner}}{{}}}
\bibcite{gentner2006verbs}{{59}{2006}{{Gentner}}{{}}}
\bibcite{gershmanmetaphor}{{60}{2014}{{Gershman and Dyer}}{{}}}
\bibcite{ginsberg2011dr}{{61}{2011}{{Ginsberg}}{{}}}
\bibcite{golub1970singular}{{62}{1970}{{Golub and Reinsch}}{{}}}
\bibcite{gouws2014bilbowa}{{63}{2014}{{Gouws et~al.}}{{Gouws, Bengio, and Corrado}}}
\bibcite{grafton2009embodied}{{64}{2009}{{Grafton}}{{}}}
\bibcite{graves2014neural}{{65}{2014}{{Graves et~al.}}{{Graves, Wayne, and Danihelka}}}
\bibcite{griffiths2007topics}{{66}{2007}{{Griffiths et~al.}}{{Griffiths, Steyvers, and Tenenbaum}}}
\bibcite{grigg2009lacan}{{67}{2009}{{Grigg}}{{}}}
\bibcite{Haghighi2008Learning}{{68}{2008}{{Haghighi et~al.}}{{Haghighi, Liang, Berg-Kirkpatrick, and Klein}}}
\bibcite{hardoon2004canonical}{{69}{2004}{{Hardoon et~al.}}{{Hardoon, Szedmak, and Shawe-Taylor}}}
\bibcite{harris1954distributional}{{70}{1954}{{Harris}}{{}}}
\bibcite{hassan2011semantic}{{71}{2011}{{Hassan and Mihalcea}}{{}}}
\bibcite{he2008indirect}{{72}{2008}{{He et~al.}}{{He, Yang, Gao, Nguyen, and Moore}}}
\bibcite{hermann2013multilingual}{{73}{2013}{{Hermann and Blunsom}}{{}}}
\bibcite{Hermann:2014:ICLR}{{74}{2014}{{Hermann and Blunsom}}{{}}}
\bibcite{Hill2014EMNLP}{{75}{2014}{{Hill and Korhonen}}{{}}}
\bibcite{hill2013concreteness}{{76}{2013{a}}{{Hill et~al.}}{{Hill, Kiela, and Korhonen}}}
\bibcite{hill2013quantitative}{{77}{2013{b}}{{Hill et~al.}}{{Hill, Korhonen, and Bentz}}}
\bibcite{hill2014multi}{{78}{2014}{{Hill et~al.}}{{Hill, Reichart, and Korhonen}}}
\bibcite{hill2015learning}{{79}{2015{a}}{{Hill et~al.}}{{Hill, Cho, Korhonen, and Bengio}}}
\bibcite{hill2014simlex}{{80}{2015{b}}{{Hill et~al.}}{{Hill, Reichart, and Korhonen}}}
\bibcite{hochreiter1997long}{{81}{1997}{{Hochreiter and Schmidhuber}}{{}}}
\bibcite{hu2004mining}{{82}{2004}{{Hu and Liu}}{{}}}
\bibcite{huang2012improving}{{83}{2012}{{Huang et~al.}}{{Huang, Socher, Manning, and Ng}}}
\bibcite{Iyyer:Boyd-Graber:Claudino:Socher:Daume-2014}{{84}{2014}{{Iyyer et~al.}}{{Iyyer, Boyd-Graber, Claudino, Socher, and {Daum\'e III}}}}
\bibcite{iyyer2015deep}{{85}{2015}{{Iyyer et~al.}}{{Iyyer, Manjunatha, Boyd-Graber, and III}}}
\bibcite{Jean}{{86}{2015}{{Jean et~al.}}{{Jean, Cho, Memisevic, and Bengio}}}
\bibcite{ji2013discriminative}{{87}{2013}{{Ji and Eisenstein}}{{}}}
\bibcite{johns2012perceptual}{{88}{2012}{{Johns and Jones}}{{}}}
\bibcite{kalchbrenner13emnlp}{{89}{2013}{{Kalchbrenner and Blunsom}}{{}}}
\bibcite{kalchbrenner2014convolutional}{{90}{2014}{{Kalchbrenner et~al.}}{{Kalchbrenner, Grefenstette, and Blunsom}}}
\bibcite{kiela2014systematic}{{91}{2014}{{Kiela and Clark}}{{}}}
\bibcite{kielaimproving}{{92}{2014}{{Kiela et~al.}}{{Kiela, Hill, Korhonen, and Clark}}}
\bibcite{kiros2014unifying}{{93}{2015{a}}{{Kiros et~al.}}{{Kiros, Salakhutdinov, and Zemel}}}
\bibcite{kiros2015skip}{{94}{2015{b}}{{Kiros et~al.}}{{Kiros, Zhu, Salakhutdinov, Zemel, Urtasun, Torralba, and Fidler}}}
\bibcite{Klementiev}{{95}{2012}{{Klementiev et~al.}}{{Klementiev, Titov, and Bhattarai}}}
\bibcite{Kocisky:2014}{{96}{2014}{{Ko\v {c}isk\'{y} et~al.}}{{Ko\v {c}isk\'{y}, Hermann, and Blunsom}}}
\bibcite{landauer1997solution}{{97}{1997}{{Landauer and Dumais}}{{}}}
\bibcite{le2014distributed}{{98}{2014}{{Le and Mikolov}}{{}}}
\bibcite{leech1994claws4}{{99}{1994}{{Leech et~al.}}{{Leech, Garside, and Bryant}}}
\bibcite{levy2014dependency}{{100}{2014{a}}{{Levy and Goldberg}}{{}}}
\bibcite{levy2014neural}{{101}{2014{b}}{{Levy and Goldberg}}{{}}}
\bibcite{levy2015improving}{{102}{2015{a}}{{Levy et~al.}}{{Levy, Goldberg, and Dagan}}}
\bibcite{levy2015supervised}{{103}{2015{b}}{{Levy et~al.}}{{Levy, Remus, Biemann, and Dagan}}}
\bibcite{lewis2004rcv1}{{104}{2004}{{Lewis et~al.}}{{Lewis, Yang, Rose, and Li}}}
\bibcite{littman2002probabilistic}{{105}{2002}{{Littman et~al.}}{{Littman, Keim, and Shazeer}}}
\bibcite{lucas2000semantic}{{106}{2000}{{Lucas}}{{}}}
\bibcite{luong2013better}{{107}{2013}{{Luong et~al.}}{{Luong, Socher, and Manning}}}
\bibcite{luong2014addressing}{{108}{2015}{{Luong et~al.}}{{Luong, Sutskever, Le, Vinyals, and Zaremba}}}
\bibcite{mao2014deep}{{109}{2015}{{Mao et~al.}}{{Mao, Xu, Yang, Wang, and Yulle}}}
\bibcite{marelli2014sick}{{110}{2014}{{Marelli et~al.}}{{Marelli, Menini, Baroni, Bentivogli, Bernardi, and Zamparelli}}}
\bibcite{markman1997similar}{{111}{1997}{{Markman and Wisniewski}}{{}}}
\bibcite{marton2009improved}{{112}{2009}{{Marton et~al.}}{{Marton, Callison-Burch, and Resnik}}}
\bibcite{mcrae2005semantic}{{113}{2005}{{McRae et~al.}}{{McRae, Cree, Seidenberg, and McNorgan}}}
\bibcite{mcrae2012semantic}{{114}{2012}{{McRae et~al.}}{{McRae, Khalkhali, and Hare}}}
\bibcite{mesnil2012unsupervised}{{115}{2012}{{Mesnil et~al.}}{{Mesnil, Dauphin, Glorot, Rifai, Bengio, Goodfellow, Lavoie, Muller, Desjardins, Warde-Farley, et~al.}}}
\bibcite{mikolov2010recurrent}{{116}{2010}{{Mikolov et~al.}}{{Mikolov, Karafi{\'a}t, Burget, Cernock{\`y}, and Khudanpur}}}
\bibcite{mikolov2013efficient}{{117}{2013{a}}{{Mikolov et~al.}}{{Mikolov, Chen, Corrado, and Dean}}}
\bibcite{mikolov2013exploiting}{{118}{2013{b}}{{Mikolov et~al.}}{{Mikolov, Le, and Sutskever}}}
\bibcite{mikolov2013distributed}{{119}{2013{c}}{{Mikolov et~al.}}{{Mikolov, Sutskever, Chen, Corrado, and Dean}}}
\bibcite{milajevs2014evaluating}{{120}{2014}{{Milajevs et~al.}}{{Milajevs, Kartsaklis, Sadrzadeh, and Purver}}}
\bibcite{mitchell2008vector}{{121}{2008}{{Mitchell and Lapata}}{{}}}
\bibcite{mitchell2010composition}{{122}{2010}{{Mitchell and Lapata}}{{}}}
\bibcite{mnih2009scalable}{{123}{2009}{{Mnih and Hinton}}{{}}}
\bibcite{molla2007question}{{124}{2007}{{Moll{\'a} and Vicedo}}{{}}}
\bibcite{morin2005hierarchical}{{125}{2005}{{Morin and Bengio}}{{}}}
\bibcite{myers1990classical}{{126}{1990}{{Myers}}{{}}}
\bibcite{navigli2009word}{{127}{2009}{{Navigli}}{{}}}
\bibcite{nelson2004university}{{128}{2004}{{Nelson et~al.}}{{Nelson, McEvoy, and Schreiber}}}
\bibcite{norman1972memory}{{129}{1972}{{Norman}}{{}}}
\bibcite{pado2007flexible}{{130}{2007}{{Pad\'o et~al.}}{{Pad\'o, Pad\'o, and Erk}}}
\bibcite{paivio1991dual}{{131}{1991}{{Paivio}}{{}}}
\bibcite{pang2004sentimental}{{132}{2004}{{Pang and Lee}}{{}}}
\bibcite{pang2005seeing}{{133}{2005}{{Pang and Lee}}{{}}}
\bibcite{pedersen2004wordnet}{{134}{2004}{{Pedersen et~al.}}{{Pedersen, Patwardhan, and Michelizzi}}}
\bibcite{Pennington2014}{{135}{2014}{{Pennington et~al.}}{{Pennington, Socher, and Manning}}}
\bibcite{marcobaronijointly}{{136}{2015}{{Pham et~al.}}{{Pham, Kruszewski, Lazaridou, and Baroni}}}
\bibcite{phan2008learning}{{137}{2008}{{Phan et~al.}}{{Phan, Nguyen, and Horiguchi}}}
\bibcite{plaut1995semantic}{{138}{1995}{{Plaut}}{{}}}
\bibcite{polajnar2015exploration}{{139}{2015}{{Polajnar et~al.}}{{Polajnar, Rimell, and Clark}}}
\bibcite{recchia2009more}{{140}{2009}{{Recchia and Jones}}{{}}}
\bibcite{reisinger2010mixture}{{141}{2010{a}}{{Reisinger and Mooney}}{{}}}
\bibcite{reisinger2010multi}{{142}{2010{b}}{{Reisinger and Mooney}}{{}}}
\bibcite{resnik1995using}{{143}{1995}{{Resnik}}{{}}}
\bibcite{resnik201011}{{144}{2010}{{Resnik and Lin}}{{}}}
\bibcite{rollermultimodal}{{145}{2013}{{Roller and Schulte~im Walde}}{{}}}
\bibcite{rosch1976structural}{{146}{1976}{{Rosch et~al.}}{{Rosch, Simpson, and Miller}}}
\bibcite{rose2002reuters}{{147}{2002}{{Rose et~al.}}{{Rose, Stevenson, and Whitehead}}}
\bibcite{rubenstein1965contextual}{{148}{1965}{{Rubenstein and Goodenough}}{{}}}
\bibcite{russakovsky2014imagenet}{{149}{2014}{{Russakovsky et~al.}}{{Russakovsky, Deng, Su, Krause, Satheesh, Ma, Huang, Karpathy, Khosla, Bernstein, et~al.}}}
\bibcite{serban2015building}{{150}{2015}{{Serban et~al.}}{{Serban, Sordoni, Bengio, Courville, and Pineau}}}
\bibcite{shaw2013building}{{151}{2013}{{Shaw et~al.}}{{Shaw, Datta, VanderMeer, and Dutta}}}
\bibcite{silberer2014learning}{{152}{2014}{{Silberer and Lapata}}{{}}}
\bibcite{silberer2012grounded}{{153}{2012}{{Silberer and Lapata}}{{}}}
\bibcite{silberer2013models}{{154}{2013}{{Silberer et~al.}}{{Silberer, Ferrari, and Lapata}}}
\bibcite{socher2011semi}{{155}{2011}{{Socher et~al.}}{{Socher, Pennington, Huang, Ng, and Manning}}}
\bibcite{srivastava2012multimodal}{{156}{2012}{{Srivastava and Salakhutdinov}}{{}}}
\bibcite{sun2008verb}{{157}{2008}{{Sun et~al.}}{{Sun, Korhonen, and Krymolowski}}}
\bibcite{sutskever2011generating}{{158}{2011}{{Sutskever et~al.}}{{Sutskever, Martens, and Hinton}}}
\bibcite{Sutskever2014sequence}{{159}{2014}{{Sutskever et~al.}}{{Sutskever, Vinyals, and Le}}}
\bibcite{szegedy2014going}{{160}{2014}{{Szegedy et~al.}}{{Szegedy, Liu, Jia, Sermanet, Reed, Anguelov, Erhan, Vanhoucke, and Rabinovich}}}
\bibcite{turian2010word}{{161}{2010}{{Turian et~al.}}{{Turian, Ratinov, and Bengio}}}
\bibcite{turney2012domain}{{162}{2012}{{Turney}}{{}}}
\bibcite{turney2010frequency}{{163}{2010}{{Turney and Pantel}}{{}}}
\bibcite{tversky1977features}{{164}{1977}{{Tversky}}{{}}}
\bibcite{vincent2008extracting}{{165}{2008}{{Vincent et~al.}}{{Vincent, Larochelle, Bengio, and Manzagol}}}
\bibcite{vincent2010stacked}{{166}{2010}{{Vincent et~al.}}{{Vincent, Larochelle, Lajoie, Bengio, and Manzagol}}}
\bibcite{von2004labeling}{{167}{2004}{{Von~Ahn and Dabbish}}{{}}}
\bibcite{voorhees2002overview}{{168}{2002}{{Voorhees}}{{}}}
\bibcite{307754}{{169}{2011}{{Vulic et~al.}}{{Vulic, De~Smet, and Moens}}}
\bibcite{vulic2011identifying}{{170}{2011}{{Vuli{\'c} et~al.}}{{Vuli{\'c}, De~Smet, and Moens}}}
\bibcite{weston2015towards}{{171}{2015}{{Weston et~al.}}{{Weston, Bordes, Chopra, and Mikolov}}}
\bibcite{wiebe2000learning}{{172}{2000}{{Wiebe}}{{}}}
\bibcite{wiebe2005annotating}{{173}{2005}{{Wiebe et~al.}}{{Wiebe, Wilson, and Cardie}}}
\bibcite{katja2005content}{{174}{2005}{{Wiemer-Hastings and Xu}}{{}}}
\bibcite{williams2009predicting}{{175}{2009}{{Williams and Anand}}{{}}}
\bibcite{wu2013online}{{176}{2013}{{Wu et~al.}}{{Wu, Hoi, Xia, Zhao, Wang, and Miao}}}
\bibcite{wu1994verbs}{{177}{1994}{{Wu and Palmer}}{{}}}
\bibcite{yong1999case}{{178}{1999}{{Yong and Foo}}{{}}}
\bibcite{zeiler2012adadelta}{{179}{2012}{{Zeiler}}{{}}}
\bibcite{zock2004word}{{180}{2004}{{Zock and Bilac}}{{}}}
\@writefile{toc}{\contentsline {chapter}{\numberline {A}Extra Information}{141}{appendix.A}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
