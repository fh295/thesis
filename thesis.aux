\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\providecommand\@newglossary[4]{}
\@newglossary{main}{glg}{gls}{glo}
\providecommand\@glsorder[1]{}
\providecommand\@istfilename[1]{}
\@istfilename{thesis.ist}
\@glsorder{word}
\@writefile{toc}{\thispagestyle {empty}}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{15}{chapter.1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\citation{dist}
\citation{cordier1965,harper1965}
\citation{landauer1997solution}
\citation{blei2003latent}
\citation{griffiths2007topics}
\citation{bengio2003neural}
\citation{collobert2008unified}
\citation{mikolov2013efficient}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Understanding and evaluating distributed word representations}{17}{chapter.2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{CH2}{{2}{17}{Understanding and evaluating distributed word representations}{chapter.2}{}}
\citation{finkelstein2001placing}
\citation{bruni2014multimodal}
\citation{gentner1978relational}
\citation{hill2013quantitative}
\citation{tversky1977features}
\@writefile{toc}{\contentsline {paragraph}{The Challenge of Evaluation}{18}{section*.5}}
\citation{collobert2008unified,turian2010word}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Reproducing human semantic knowledge}{19}{section.2.1}}
\citation{plaut1995semantic,mcrae2012semantic}
\citation{mcrae2012semantic}
\citation{grigg2009lacan}
\citation{crutch2009different}
\citation{lucas2000semantic}
\citation{fellbaum1999wordnet}
\citation{wu1994verbs}
\citation{wu1994verbs}
\citation{nelson2004university}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Similarity and Association}{20}{subsection.2.1.1}}
\citation{hill2013quantitative}
\citation{he2008indirect,Haghighi2008Learning,marton2009improved,beltagysemantic}
\citation{navigli2009word}
\citation{phan2008learning}
\citation{rose2002reuters}
\citation{huang2012improving,reisinger2010multi,luong2013better}
\citation{budanitsky2006evaluating}
\citation{turney2012domain}
\@writefile{toc}{\contentsline {paragraph}{Association and similarity in NLP}{21}{section*.7}}
\citation{agirre2009study}
\citation{agirre2009study}
\citation{agirre2009study}
\@writefile{lot}{\contentsline {table}{\numberline {2.1}{\ignorespaces  Top: Concept pairs with the lowest WupSim scores in the USF dataset overall. Bottom: Pairs with the largest discrepancy in rank between association strength (high) and WupSim (low).\relax }}{22}{table.caption.6}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{tab1}{{2.1}{22}{Top: Concept pairs with the lowest WupSim scores in the USF dataset overall. Bottom: Pairs with the largest discrepancy in rank between association strength (high) and WupSim (low).\relax }{table.caption.6}{}}
\citation{gentner2006verbs}
\citation{markman1997similar}
\citation{hill2014multi}
\citation{paivio1991dual,hill2013quantitative}
\citation{hill2013concreteness}
\citation{kielaimproving}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Motivation for SimLex-999}{23}{section.2.2}}
\newlabel{motivation}{{2.2}{23}{Motivation for SimLex-999}{section.2.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Concepts, part-of-speech and concreteness}{23}{subsection.2.2.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Existing gold standards and evaluation resources}{23}{subsection.2.2.2}}
\newlabel{existing}{{2.2.2}{23}{Existing gold standards and evaluation resources}{subsection.2.2.2}{}}
\citation{finkelstein2001placing}
\citation{huang2012improving,bansal2014tailoring}
\@writefile{toc}{\contentsline {paragraph}{Representative}{24}{section*.8}}
\@writefile{toc}{\contentsline {paragraph}{Clearly-defined}{24}{section*.9}}
\@writefile{toc}{\contentsline {paragraph}{Consistent and reliable}{24}{section*.10}}
\@writefile{toc}{\contentsline {paragraph}{}{24}{section*.11}}
\@writefile{toc}{\contentsline {paragraph}{\bf  WordSim-353}{24}{section*.12}}
\citation{collobert2008unified}
\citation{huang2012improving}
\citation{huang2012improving}
\citation{pado2007flexible,reisinger2010mixture,silberer2014learning}
\citation{yong1999case,cunningham2005information,resnik201011}
\citation{alfonseca2002extending}
\citation{agirre2009study}
\citation{agirre2009study}
\@writefile{toc}{\contentsline {paragraph}{\bf  WS-Sim}{25}{section*.13}}
\citation{agirre2009study}
\citation{rubenstein1965contextual}
\citation{hassan2011semantic}
\citation{rubenstein1965contextual}
\citation{bruni2014multimodal}
\citation{bruni2012distributional2,bernardi2013relatedness}
\@writefile{toc}{\contentsline {paragraph}{\bf  Rubenstein \& Goodenough}{26}{section*.14}}
\@writefile{toc}{\contentsline {paragraph}{\bf  The MEN Test Collection}{26}{section*.15}}
\citation{bruni2012distributional}
\citation{landauer1997solution}
\citation{griffiths2007topics}
\citation{turney2010frequency}
\@writefile{toc}{\contentsline {paragraph}{\bf  Synonym detection sets}{27}{section*.16}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}The SimLex-999 Dataset}{27}{section.2.3}}
\newlabel{simlex}{{2.3}{27}{The SimLex-999 Dataset}{section.2.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Choice of Concepts}{27}{subsection.2.3.1}}
\@writefile{toc}{\contentsline {paragraph}{Separating similarity from association}{27}{section*.17}}
\citation{leech1994claws4}
\citation{fellbaum1999wordnet}
\citation{hill2014multi,kielaimproving}
\@writefile{toc}{\contentsline {paragraph}{POS category}{28}{section*.18}}
\@writefile{toc}{\contentsline {paragraph}{Concreteness}{28}{section*.19}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces  Boxplots showing the interaction between concreteness and POS for concepts in USF. The white boxes range from the first to third quartiles and the central vertical line indicates the median.\relax }}{29}{figure.caption.20}}
\newlabel{fig1}{{2.1}{29}{Boxplots showing the interaction between concreteness and POS for concepts in USF. The white boxes range from the first to third quartiles and the central vertical line indicates the median.\relax }{figure.caption.20}{}}
\@writefile{toc}{\contentsline {paragraph}{Final sampling}{30}{section*.21}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Question Design}{30}{subsection.2.3.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces  Instructions for SimLex-999 annotators.\relax }}{30}{figure.caption.22}}
\newlabel{fig2}{{2.2}{30}{Instructions for SimLex-999 annotators.\relax }{figure.caption.22}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces  A group of noun pairs to be rated by moving the sliders. The rating slider was initially at position 0, and it was possible to attribute a rating of 0, although it was necessary to have clicked on the slider in the zero position to assign that rating and proceed to the next page.\relax }}{31}{figure.caption.23}}
\newlabel{fig3}{{2.3}{31}{A group of noun pairs to be rated by moving the sliders. The rating slider was initially at position 0, and it was possible to attribute a rating of 0, although it was necessary to have clicked on the slider in the zero position to assign that rating and proceed to the next page.\relax }{figure.caption.23}{}}
\citation{huang2012improving}
\citation{resnik1995using,pedersen2004wordnet}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.3}Context-free rating}{32}{subsection.2.3.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.4}Questionnaire structure}{32}{subsection.2.3.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.5}Participants}{33}{subsection.2.3.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.6}Post-processing}{33}{subsection.2.3.6}}
\citation{pado2007flexible,reisinger2010mixture,silberer2014learning}
\citation{bruni2012distributional2}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Analysis of Dataset}{34}{section.2.4}}
\newlabel{analysis}{{2.4}{34}{Analysis of Dataset}{section.2.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.1}Inter-annotator agreement}{34}{subsection.2.4.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces  {\bf  Left:} Inter-annotator agreement, measured by average pairwise Spearman \(\rho \) correlation, for ratings of concept types in SimLex-999. {\bf  Right:} Response consistency, reflecting the standard deviation of annotator ratings for each pair, averaged over all pairs in the concept category.\relax }}{34}{figure.caption.24}}
\newlabel{fig4}{{2.4}{34}{{\bf Left:} Inter-annotator agreement, measured by average pairwise Spearman \(\rho \) correlation, for ratings of concept types in SimLex-999. {\bf Right:} Response consistency, reflecting the standard deviation of annotator ratings for each pair, averaged over all pairs in the concept category.\relax }{figure.caption.24}{}}
\citation{paivio1991dual}
\citation{williams2009predicting}
\citation{wiebe2000learning}
\@writefile{lot}{\contentsline {table}{\numberline {2.2}{\ignorespaces  {\bf  Top: Similarity aligns with association} Pairs with a small difference in rank between USF (association) and SimLex-999 (similarity) scores for each POS category. {\bf  Bottom: Similarity contrasts with association} Pairs with a high difference in rank for each POS category. *Note that the distribution of USF association scores on the interval [0,10] is highly skewed towards the lower bound in both SimLex-999 and the USF dataset as a whole.\relax }}{36}{table.caption.25}}
\newlabel{tab2}{{2.2}{36}{{\bf Top: Similarity aligns with association} Pairs with a small difference in rank between USF (association) and SimLex-999 (similarity) scores for each POS category. {\bf Bottom: Similarity contrasts with association} Pairs with a high difference in rank for each POS category. *Note that the distribution of USF association scores on the interval [0,10] is highly skewed towards the lower bound in both SimLex-999 and the USF dataset as a whole.\relax }{table.caption.25}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.2}Response validity: Similarity not association}{36}{subsection.2.4.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces  {\bf  (a)} Pairs rated by WS-353 annotators (blue points, ranked by rating) and the corresponding rating of annotators following the SimLex-999 instructions (red points). {\bf  (b-c)} The same analysis, restricted to pairs in the WS-Sim or WS-Rel subsets of WS-353.\relax }}{36}{figure.caption.26}}
\newlabel{fig5}{{2.5}{36}{{\bf (a)} Pairs rated by WS-353 annotators (blue points, ranked by rating) and the corresponding rating of annotators following the SimLex-999 instructions (red points). {\bf (b-c)} The same analysis, restricted to pairs in the WS-Sim or WS-Rel subsets of WS-353.\relax }{figure.caption.26}{}}
\citation{agirre2009study}
\citation{cruse1986lexical}
\citation{levy2015supervised}
\citation{rosch1976structural}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.3}Finer-grained Semantic Relations}{37}{subsection.2.4.3}}
\citation{baroni2014don}
\citation{Bengio2003lm}
\citation{collobert2008unified}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces  Average SimLex and USF free association scores across pairs representing different fine-grained semantic relations. All relations were extracted from WordNet. \(n\_hypernym\) refers to a direct hypernymy path of length \(n\). Note that the average SimLex rating across all 999 word pairs (dashed red line) is much higher than the average USF rating (dashed golden line) because of differences in the rating procedure. The more interesting differences concern the relative strength of similarity vs. association across the different relation types.\relax }}{39}{figure.caption.27}}
\newlabel{fig6}{{2.6}{39}{Average SimLex and USF free association scores across pairs representing different fine-grained semantic relations. All relations were extracted from WordNet. \(n\_hypernym\) refers to a direct hypernymy path of length \(n\). Note that the average SimLex rating across all 999 word pairs (dashed red line) is much higher than the average USF rating (dashed golden line) because of differences in the rating procedure. The more interesting differences concern the relative strength of similarity vs. association across the different relation types.\relax }{figure.caption.27}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Evaluating Models with SimLex-999}{39}{section.2.5}}
\newlabel{evaluation}{{2.5}{39}{Evaluating Models with SimLex-999}{section.2.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.1}Neural language models for word representation}{39}{subsection.2.5.1}}
\newlabel{prev}{{2.5.1}{39}{Neural language models for word representation}{subsection.2.5.1}{}}
\@writefile{toc}{\contentsline {paragraph}{\bf  Collobert \& Weston}{39}{section*.28}}
\citation{turian2010word}
\citation{collobert2008unified}
\citation{lewis2004rcv1}
\citation{huang2012improving}
\citation{collobert2008unified}
\citation{collobert2008unified}
\citation{mikolov2013efficient}
\@writefile{toc}{\contentsline {paragraph}{\bf  Huang et al.}{40}{section*.29}}
\citation{mikolov2013efficient,mikolov2013distributed}
\citation{mikolov2013efficient}
\citation{baroni2014don}
\newlabel{shallow}{{2.5.1}{41}{\bf Log-linear models}{section*.30}{}}
\@writefile{toc}{\contentsline {paragraph}{\bf  Log-linear models}{41}{section*.30}}
\citation{kiela2014systematic}
\citation{bird2006nltk}
\citation{recchia2009more}
\citation{landauer1997solution}
\citation{golub1970singular}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.2}\bf  Vector space (counting) models}{42}{subsection.2.5.2}}
\@writefile{toc}{\contentsline {paragraph}{\bf  LSA}{42}{section*.31}}
\citation{mikolov2013efficient}
\citation{huang2012improving}
\citation{collobert2008unified}
\citation{mikolov2013efficient}
\citation{huang2012improving}
\citation{collobert2008unified}
\citation{lewis2004rcv1}
\citation{lewis2004rcv1}
\citation{huang2012improving}
\citation{huang2012improving}
\citation{huang2012improving}
\citation{huang2012improving}
\citation{collobert2008unified}
\citation{mikolov2013efficient}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.3}Results}{43}{subsection.2.5.3}}
\newlabel{assoc}{{2.5.3}{43}{\bf Overall performance on SimLex-999}{section*.32}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces Performance of NLMs on WS-353, MEN and SimLex-999. All models are trained on Wikipedia; note that as Wikipedia is constantly growing, the \cite {mikolov2013efficient} model exploited slightly more training data ($\approx $1000m tokens) than the \cite {huang2012improving} model ($\approx $990m), which in turn exploited more than the \cite {collobert2008unified} model ($\approx $852m). Dashed horizontal lines indicate the level of inter-annotator agreement for the three datasets.\relax }}{43}{figure.caption.33}}
\newlabel{fig7}{{2.7}{43}{Performance of NLMs on WS-353, MEN and SimLex-999. All models are trained on Wikipedia; note that as Wikipedia is constantly growing, the \protect \cite {mikolov2013efficient} model exploited slightly more training data ($\approx $1000m tokens) than the \protect \cite {huang2012improving} model ($\approx $990m), which in turn exploited more than the \protect \cite {collobert2008unified} model ($\approx $852m). Dashed horizontal lines indicate the level of inter-annotator agreement for the three datasets.\relax }{figure.caption.33}{}}
\@writefile{toc}{\contentsline {paragraph}{\bf  Overall performance on SimLex-999}{43}{figure.caption.34}}
\citation{mikolov2013efficient}
\citation{lewis2004rcv1}
\citation{baroni2014don}
\citation{huang2012improving}
\citation{collobert2008unified}
\@writefile{lof}{\contentsline {figure}{\numberline {2.8}{\ignorespaces Comparison between the leading NLM, \emph  {Mikolov et al.}, the vector space model, \emph  {VSM}, and the \emph  {LSA} model. All models were trained on the $\approx $150m word RCV1 Corpus \citep {lewis2004rcv1}.\relax }}{44}{figure.caption.34}}
\newlabel{fig8}{{2.8}{44}{Comparison between the leading NLM, \emph {Mikolov et al.}, the vector space model, \emph {VSM}, and the \emph {LSA} model. All models were trained on the $\approx $150m word RCV1 Corpus \protect \citep {lewis2004rcv1}.\relax }{figure.caption.34}{}}
\@writefile{toc}{\contentsline {paragraph}{\bf  Modeling similarity vs. association}{44}{section*.35}}
\citation{collobert2008unified}
\citation{huang2012improving}
\citation{mikolov2013efficient}
\citation{mikolov2013efficient}
\citation{levy2014dependency}
\citation{levy2014dependency}
\@writefile{lof}{\contentsline {figure}{\numberline {2.9}{\ignorespaces  The ability of NLMs to model the similarity of highly-associated concepts versus concepts in general. The two models on the right hand side also demonstrate the effect of training an NLM (the \cite {mikolov2013efficient} model) on running-text (\emph  {Mikolov et al.}) vs. on dependency-based input (\emph  {Levy \& Goldberg}).\relax }}{45}{figure.caption.36}}
\newlabel{fig9}{{2.9}{45}{The ability of NLMs to model the similarity of highly-associated concepts versus concepts in general. The two models on the right hand side also demonstrate the effect of training an NLM (the \protect \cite {mikolov2013efficient} model) on running-text (\emph {Mikolov et al.}) vs. on dependency-based input (\emph {Levy \& Goldberg}).\relax }{figure.caption.36}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.10}{\ignorespaces \nobreakspace  {} Performance of models on POS-based subsets of SimLex-999. The window size for each model is indicated in parentheses. Inter-annotator agreement for each POS is indicated by the dashed horizontal line.\relax }}{46}{figure.caption.37}}
\newlabel{fig11}{{2.10}{46}{~ Performance of models on POS-based subsets of SimLex-999. The window size for each model is indicated in parentheses. Inter-annotator agreement for each POS is indicated by the dashed horizontal line.\relax }{figure.caption.37}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.11}{\ignorespaces  The importance of dependency-focussed contexts (in the Levy \& Goldberg model) for capturing concepts of different POS, when compared to a standard Skipgram (BOW) model trained on the same Wikipedia corpus.\relax }}{46}{figure.caption.38}}
\newlabel{fig12}{{2.11}{46}{The importance of dependency-focussed contexts (in the Levy \& Goldberg model) for capturing concepts of different POS, when compared to a standard Skipgram (BOW) model trained on the same Wikipedia corpus.\relax }{figure.caption.38}{}}
\citation{mikolov2013efficient}
\citation{levy2014dependency}
\citation{mikolov2013efficient}
\citation{markman1997similar}
\citation{sun2008verb}
\newlabel{begin}{{2.5.3}{47}{\bf Learning concepts of different POS}{section*.39}{}}
\@writefile{toc}{\contentsline {paragraph}{\bf  Learning concepts of different POS}{47}{section*.39}}
\newlabel{end}{{2.5.3}{47}{\bf Learning concrete and abstract concepts}{section*.40}{}}
\@writefile{toc}{\contentsline {paragraph}{\bf  Learning concrete and abstract concepts}{47}{figure.caption.41}}
\citation{hill2013concreteness}
\@writefile{lof}{\contentsline {figure}{\numberline {2.12}{\ignorespaces  Performance of models on concreteness-based subsets of SimLex-999. Window size is indicated in parentheses. Horizontal dashed lines indicate inter-annotator agreement between SimLex-999 annotators on the two subsets.\relax }}{48}{figure.caption.41}}
\newlabel{fig13}{{2.12}{48}{Performance of models on concreteness-based subsets of SimLex-999. Window size is indicated in parentheses. Horizontal dashed lines indicate inter-annotator agreement between SimLex-999 annotators on the two subsets.\relax }{figure.caption.41}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.6}Conclusion}{48}{section.2.6}}
\newlabel{conclusion}{{2.6}{48}{Conclusion}{section.2.6}{}}
\citation{levy2015improving,wang2015learning}
\citation{baroni2014don}
\citation{levy2014neural}
\@writefile{toc}{\contentsline {paragraph}{What is so special about neural word embeddings?}{49}{section*.42}}
\citation{levy2015improving}
\citation{levy2015improving}
\citation{gershmanmetaphor}
\citation{barsalou2003grounding}
\@writefile{toc}{\contentsline {paragraph}{The future of word representations}{50}{section*.43}}
\citation{dist}
\citation{barsalou2005situating}
\citation{feng2010visual,bruni2012distributional}
\citation{kiela2015multi}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Representing words with neural language models and diverse data sources}{53}{chapter.3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{CH3}{{3}{53}{Representing words with neural language models and diverse data sources}{chapter.3}{}}
\citation{andrews2009integrating}
\citation{barsalou2005situating}
\citation{feng2010visual,bruni2012distributional}
\citation{silberer2012grounded,rollermultimodal}
\citation{bruni2014multimodal}
\citation{silberer2012grounded}
\citation{rollermultimodal}
\citation{silberer2014learning}
\citation{paivio1991dual,hill2013quantitative}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Grounded acquisition of abstract concepts from multi-modal data}{54}{section.3.1}}
\citation{leech1994claws4}
\citation{nelson2004university}
\citation{crutch2005abstract}
\citation{collobert2008unified,mesnil2012unsupervised}
\citation{mikolov2013efficient}
\citation{von2004labeling}
\citation{devereux2013centre}
\citation{nelson2004university}
\citation{mikolov2013efficient}
\citation{bybee2001frequency,chater2006probabilistic}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}Model Design}{56}{subsection.3.1.1}}
\@writefile{toc}{\contentsline {paragraph}{Language-only model}{56}{section*.44}}
\citation{mikolov2013efficient}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces  Example pseudo-sentences generated for training the model.\relax }}{57}{figure.caption.45}}
\newlabel{examples}{{3.1}{57}{Example pseudo-sentences generated for training the model.\relax }{figure.caption.45}{}}
\citation{von2004labeling}
\citation{devereux2013centre}
\citation{mcrae2005semantic}
\citation{silberer2012grounded,rollermultimodal}
\citation{fodor1983modularity}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.2}Information sources}{58}{subsection.3.1.2}}
\newlabel{percep_sources}{{3.1.2}{58}{Information sources}{subsection.3.1.2}{}}
\@writefile{toc}{\contentsline {paragraph}{ESPGame dataset}{58}{section*.46}}
\@writefile{toc}{\contentsline {paragraph}{CSLB Property Norms}{58}{section*.47}}
\citation{nelson2004university}
\citation{andrews2009integrating,feng2010visual,silberer2012grounded,rollermultimodal}
\citation{huang2012improving,silberer2012grounded}
\citation{leech1994claws4}
\@writefile{lot}{\contentsline {table}{\numberline {3.1}{\ignorespaces  Concepts identified in images in the ESP Game (left) and features produced for concepts by human annotators in the CSLB dataset (with feature strength, max=30).\relax }}{59}{table.caption.49}}
\newlabel{font-table}{{3.1}{59}{Concepts identified in images in the ESP Game (left) and features produced for concepts by human annotators in the CSLB dataset (with feature strength, max=30).\relax }{table.caption.49}{}}
\@writefile{toc}{\contentsline {paragraph}{Linguistic input}{59}{section*.48}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.3}Evaluation}{59}{subsection.3.1.3}}
\@writefile{lot}{\contentsline {table}{\numberline {3.2}{\ignorespaces  Example concept pairs (with mean concreteness rating) and free-association scores from the USF dataset.\relax }}{60}{table.caption.50}}
\newlabel{font-table}{{3.2}{60}{Example concept pairs (with mean concreteness rating) and free-association scores from the USF dataset.\relax }{table.caption.50}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.3}{\ignorespaces  Details the subsets of USF data used in the evaluations\relax }}{60}{table.caption.51}}
\newlabel{set_details}{{3.3}{60}{Details the subsets of USF data used in the evaluations\relax }{table.caption.51}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.4}Results and Discussion}{60}{subsection.3.1.4}}
\citation{mikolov2013efficient}
\citation{silberer2012grounded}
\citation{hardoon2004canonical}
\citation{bruni2014multimodal}
\citation{bruni2014multimodal}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.5}Combining information sources}{61}{subsection.3.1.5}}
\newlabel{conc1}{{3.1.5}{61}{Combining information sources}{subsection.3.1.5}{}}
\citation{johns2012perceptual}
\citation{johns2012perceptual}
\citation{hill2014multi}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.6}Propagating input to abstract concepts}{62}{subsection.3.1.6}}
\newlabel{conc2}{{3.1.6}{62}{Propagating input to abstract concepts}{subsection.3.1.6}{}}
\@writefile{toc}{\contentsline {paragraph}{Johns and Jones}{62}{section*.52}}
\@writefile{toc}{\contentsline {paragraph}{Ridge Regression}{62}{section*.53}}
\citation{mikolov2013efficient}
\citation{feng2010visual,silberer2012grounded,silberer2013models}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces  The proposed approach compared with other methods of information combination (left) and propagation. Dashed lines indicate language-only model baseline.\relax }}{63}{figure.caption.55}}
\newlabel{main_results}{{3.2}{63}{The proposed approach compared with other methods of information combination (left) and propagation. Dashed lines indicate language-only model baseline.\relax }{figure.caption.55}{}}
\@writefile{toc}{\contentsline {paragraph}{Comparisons}{63}{section*.54}}
\citation{katja2005content}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces  {\bf  Top}: Comparing the strategy of directly representing abstract concepts from perceptual information where available (yellow bars) vs. propagating via concrete concepts. {\bf  Bottom}: The effect of increasing \(\alpha \) on correlation with USF pairs (Spearman \(\rho \)) for each concept type. Horizontal dashed lines indicate language-only model baseline.\relax }}{64}{figure.caption.56}}
\newlabel{repprop}{{3.3}{64}{{\bf Top}: Comparing the strategy of directly representing abstract concepts from perceptual information where available (yellow bars) vs. propagating via concrete concepts. {\bf Bottom}: The effect of increasing \(\alpha \) on correlation with USF pairs (Spearman \(\rho \)) for each concept type. Horizontal dashed lines indicate language-only model baseline.\relax }{figure.caption.56}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.7}Direct representation vs. propagation}{65}{subsection.3.1.7}}
\newlabel{conc3}{{3.1.7}{65}{Direct representation vs. propagation}{subsection.3.1.7}{}}
\citation{srivastava2012multimodal,wu2013online}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.8}Source and quantity of perceptual input}{66}{subsection.3.1.8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.9}Conclusions}{66}{subsection.3.1.9}}
\citation{grafton2009embodied,barsalou2010grounded}
\citation{baroni2014don,levy2015improving}
\citation{levy2014neural}
\citation{kalchbrenner13emnlp,devlin2014fast,Sutskever2014sequence}
\@writefile{toc}{\contentsline {paragraph}{Type I}{67}{section*.57}}
\@writefile{toc}{\contentsline {paragraph}{Type II}{67}{section*.58}}
\@writefile{toc}{\contentsline {paragraph}{Type III}{67}{section*.59}}
\citation{kalchbrenner13emnlp,Sutskever2014sequence}
\citation{bahdanau2014neural}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Learning word representations from bilingual data using encoder-decoder models}{68}{section.3.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Neural Machine Translation Models}{68}{subsection.3.2.1}}
\citation{kalchbrenner13emnlp,Cho2014,bahdanau2014neural}
\citation{Haghighi2008Learning,vulic2011identifying,mikolov2013exploiting,Hermann:2014:ICLR,lauly2014autoencoder}
\citation{Haghighi2008Learning,vulic2011identifying,mikolov2013exploiting}
\citation{Klementiev,Hermann:2014:ICLR,lauly2014autoencoder,Kocisky:2014}
\citation{Hermann:2014:ICLR}
\citation{Klementiev}
\citation{Hermann:2014:ICLR}
\citation{lauly2014autoencoder}
\citation{Hermann:2014:ICLR}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Other bilingual models of learning word representations}{69}{subsection.3.2.2}}
\citation{faruqui2014improving}
\citation{faruqui2014improving}
\citation{faruqui2014improving}
\citation{Cho2014}
\citation{bahdanau2014neural}
\citation{kalchbrenner13emnlp}
\citation{devlin2014fast}
\citation{Sutskever2014sequence}
\citation{Cho2014}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.3}Experiments}{70}{subsection.3.2.3}}
\citation{Hermann:2014:ICLR}
\citation{faruqui2014improving}
\citation{mikolov2013distributed}
\citation{Pennington2014}
\citation{collobert2008unified}
\citation{Agirre2009,bruni2014multimodal,baroni2014don}
\citation{Agirre2009}
\citation{bruni2014multimodal}
\citation{hill2014simlex}
\citation{faruqui2014improving}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.3.1}Similarity and relatedness modelling}{71}{subsubsection.3.2.3.1}}
\citation{hill2014simlex}
\citation{nelson2004university}
\@writefile{lot}{\contentsline {table}{\numberline {3.4}{\ignorespaces  NMT embeddings (RNNenc and RNNsearch) clearly outperform alternative embedding-learning architectures on tasks that require modelling similarity (below the dashed line), but not on tasks that reflect relatedness. Bilingual embedding spaces learned without the translation objective are somewhere between these two extremes.\relax }}{72}{table.caption.60}}
\newlabel{table:perf}{{3.4}{72}{NMT embeddings (RNNenc and RNNsearch) clearly outperform alternative embedding-learning architectures on tasks that require modelling similarity (below the dashed line), but not on tasks that reflect relatedness. Bilingual embedding spaces learned without the translation objective are somewhere between these two extremes.\relax }{table.caption.60}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.5}{\ignorespaces Nearest neighbours (excluding plurals) in the embedding spaces of different models. All models were trained for 6 epochs on the translation corpus except CW and FD (as noted previously). NMT embedding spaces are oriented according to similarity, whereas embeddings learned by monolingual models are organized according to relatedness. The other bilingual model BiCVM also exhibits a notable focus on similarity.\relax }}{72}{table.caption.61}}
\newlabel{table:neigh}{{3.5}{72}{Nearest neighbours (excluding plurals) in the embedding spaces of different models. All models were trained for 6 epochs on the translation corpus except CW and FD (as noted previously). NMT embedding spaces are oriented according to similarity, whereas embeddings learned by monolingual models are organized according to relatedness. The other bilingual model BiCVM also exhibits a notable focus on similarity.\relax }{table.caption.61}{}}
\citation{landauer1997solution}
\citation{mikolov2013distributed}
\citation{mikolov2013distributed}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.3.2}Importance of training data quantity}{74}{subsubsection.3.2.3.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces The effect of increasing the amount of training data on the quality of monolingual embeddings, based on similarity-based evaluations (SimLex-999) and two relatedness-based evaluations (MEN and WordSim-353). \emph  {ET} in the legend indicates models trained on the English half of the translation corpus. \emph  {Wiki} indicates models trained on Wikipedia.\relax }}{74}{figure.caption.62}}
\newlabel{fig:size}{{3.4}{74}{The effect of increasing the amount of training data on the quality of monolingual embeddings, based on similarity-based evaluations (SimLex-999) and two relatedness-based evaluations (MEN and WordSim-353). \emph {ET} in the legend indicates models trained on the English half of the translation corpus. \emph {Wiki} indicates models trained on Wikipedia.\relax }{figure.caption.62}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.3.3}Analogy resolution}{74}{subsubsection.3.2.3.3}}
\citation{mikolov2013distributed}
\citation{faruqui2014improving}
\citation{levy2014dependency}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces Translation-based embeddings perform best on syntactic analogies (\emph  {run,ran: hide, hid}). Monolingual skipgram/Glove models are better at semantic analogies (\emph  {father, man; mother, woman})\relax }}{76}{figure.caption.63}}
\newlabel{fig:analogy}{{3.5}{76}{Translation-based embeddings perform best on syntactic analogies (\emph {run,ran: hide, hid}). Monolingual skipgram/Glove models are better at semantic analogies (\emph {father, man; mother, woman})\relax }{figure.caption.63}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Effect of Target Language}{76}{section.3.3}}
\newlabel{lang_effects}{{3.3}{76}{Effect of Target Language}{section.3.3}{}}
\citation{morin2005hierarchical}
\citation{mikolov2013distributed,Hermann:2014:ICLR}
\citation{Jean}
\citation{Bengio+Senecal-2003-small}
\@writefile{lot}{\contentsline {table}{\numberline {3.6}{\ignorespaces Comparison of embeddings learned by RNN Search models translating between English-French (EN-FR) and English-German (EN-DE) on all semantic evaluations (left) and nearest neighbours of selected cue words (right). Bold italics indicate target-language-specific effects. Evaluation items and vocabulary searches were restricted to words common to both models. \relax }}{77}{table.caption.64}}
\newlabel{table:de}{{3.6}{77}{Comparison of embeddings learned by RNN Search models translating between English-French (EN-FR) and English-German (EN-DE) on all semantic evaluations (left) and nearest neighbours of selected cue words (right). Bold italics indicate target-language-specific effects. Evaluation items and vocabulary searches were restricted to words common to both models. \relax }{table.caption.64}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}Overcoming the vocabulary size problem}{77}{subsection.3.3.1}}
\citation{luong2014addressing}
\@writefile{lot}{\contentsline {table}{\numberline {3.7}{\ignorespaces Comparison of embeddings learned by the original (RNN Search - 30k French words, 50k German words) and extended-vocabulary (RNN Search-LV -500k words) models translating from English to French (EN-FR) and from English to German (EN-DE). For fair comparisons, all evaluations were restricted to the intersection of all model vocabularies.\relax }}{78}{table.caption.65}}
\newlabel{table:ex}{{3.7}{78}{Comparison of embeddings learned by the original (RNN Search - 30k French words, 50k German words) and extended-vocabulary (RNN Search-LV -500k words) models translating from English to French (EN-FR) and from English to German (EN-DE). For fair comparisons, all evaluations were restricted to the intersection of all model vocabularies.\relax }{table.caption.65}{}}
\citation{faruqui2014improving}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}How similarity emerges in NMT embeddings}{79}{subsection.3.3.2}}
\newlabel{section:exp}{{3.3.2}{79}{How similarity emerges in NMT embeddings}{subsection.3.3.2}{}}
\citation{hill2014simlex}
\citation{faruqui2014improving}
\citation{Hermann:2014:ICLR}
\citation{faruqui2014improving}
\citation{Kocisky:2014}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.3}Conclusions}{80}{subsection.3.3.3}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Discussion}{81}{section.3.4}}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Representing phrases with neural language models}{83}{chapter.4}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{CH4}{{4}{83}{Representing phrases with neural language models}{chapter.4}{}}
\citation{mikolov2013distributed}
\citation{zock2004word}
\citation{307754,Klementiev,hermann2013multilingual,gouws2014bilbowa}
\citation{mikolov2010recurrent}
\citation{kiros2014unifying}
\citation{bahdanau2014neural}
\citation{mikolov2013distributed}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Neural language model architectures}{85}{section.4.1}}
\citation{bengio1994learning}
\citation{hochreiter1997long}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.1}Long short-term memory}{86}{subsection.4.1.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.2}Bag-of-words NLMs}{87}{subsection.4.1.2}}
\citation{huang2012improving}
\citation{bordes2015large}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.3}Pre-trained input representations}{88}{subsection.4.1.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.4}Training objective}{88}{subsection.4.1.4}}
\citation{faruqui2014retrofitting}
\citation{cho2014learning}
\citation{bergstra+al:2010-scipy}
\citation{zeiler2012adadelta}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.5}Implementation details}{89}{subsection.4.1.5}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Reverse dictionaries}{89}{section.4.2}}
\citation{bilac2003improving,bilac2004dictionary,zock2004word}
\citation{shaw2013building}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}Data collection and training}{90}{subsection.4.2.1}}
\citation{mitchell2010composition}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}Comparisons}{91}{subsection.4.2.2}}
\citation{shaw2013building}
\citation{leech1994claws4}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.3}Reverse dictionary evaluation}{92}{subsection.4.2.3}}
\@writefile{lot}{\contentsline {table}{\numberline {4.1}{\ignorespaces Style difference between \emph  {dictionary definitions} and \emph  {concept descriptions} in the evaluation.\relax }}{93}{table.caption.66}}
\newlabel{tb:tablename}{{4.1}{93}{Style difference between \emph {dictionary definitions} and \emph {concept descriptions} in the evaluation.\relax }{table.caption.66}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.4}Results}{93}{subsection.4.2.4}}
\citation{iyyer2015deep}
\@writefile{lot}{\contentsline {table}{\numberline {4.2}{\ignorespaces Performance of different reverse dictionary models in different evaluation settings. *Low variance in \emph  {mult} models is due to consistently poor scores, so not highlighted.\relax }}{94}{table.caption.67}}
\newlabel{results}{{4.2}{94}{Performance of different reverse dictionary models in different evaluation settings. *Low variance in \emph {mult} models is due to consistently poor scores, so not highlighted.\relax }{table.caption.67}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.5}Qualitative analysis}{95}{subsection.4.2.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.6}Cross-lingual reverse dictionaries}{95}{subsection.4.2.6}}
\citation{hermann2013multilingual,lauly2014autoencoder,gouws2014bilbowa}
\citation{gouws2014bilbowa}
\@writefile{lot}{\contentsline {table}{\numberline {4.3}{\ignorespaces The top-five candidates for example queries (invented by the authors) from different reverse dictionary models. Both the RNN and BOW models are without Word2Vec input and use the cosine loss.\relax }}{96}{table.caption.68}}
\newlabel{qual}{{4.3}{96}{The top-five candidates for example queries (invented by the authors) from different reverse dictionary models. Both the RNN and BOW models are without Word2Vec input and use the cosine loss.\relax }{table.caption.68}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.4}{\ignorespaces Responses from cross-lingual reverse dictionary models to selected queries. Underlined responses are `correct' or potentially useful for a native French speaker.\relax }}{97}{table.caption.69}}
\newlabel{cross}{{4.4}{97}{Responses from cross-lingual reverse dictionary models to selected queries. Underlined responses are `correct' or potentially useful for a native French speaker.\relax }{table.caption.69}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.7}Discussion}{97}{subsection.4.2.7}}
\citation{ferrucci2010building}
\citation{molla2007question}
\citation{Iyyer:Boyd-Graber:Claudino:Socher:Daume-2014,weston2015towards}
\citation{berant14paraphrasing,bordes2014question}
\citation{littman2002probabilistic}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Answering crossword questions}{98}{section.4.3}}
\citation{littman2002probabilistic}
\citation{ginsberg2011dr}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.1}Evaluation}{99}{subsection.4.3.1}}
\@writefile{lot}{\contentsline {table}{\numberline {4.5}{\ignorespaces Examples of the different question types in the crossword question evaluation dataset.\relax }}{99}{table.caption.70}}
\newlabel{tb:tablename}{{4.5}{99}{Examples of the different question types in the crossword question evaluation dataset.\relax }{table.caption.70}{}}
\citation{littman2002probabilistic,ginsberg2011dr}
\@writefile{lot}{\contentsline {table}{\numberline {4.6}{\ignorespaces Performance of different models on crossword questions of different length. The two commercial systems are evaluated via their web interface so only accuracy@10 can be reported in those cases. \relax }}{100}{table.caption.71}}
\newlabel{results2}{{4.6}{100}{Performance of different models on crossword questions of different length. The two commercial systems are evaluated via their web interface so only accuracy@10 can be reported in those cases. \relax }{table.caption.71}{}}
\newlabel{tb:tablename}{{4.6}{100}{Performance of different models on crossword questions of different length. The two commercial systems are evaluated via their web interface so only accuracy@10 can be reported in those cases. \relax }{table.caption.71}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.2}Benchmarks and comparisons}{100}{subsection.4.3.2}}
\@writefile{lot}{\contentsline {table}{\numberline {4.7}{\ignorespaces Responses from different models to example crossword clues. In each case the model output is filtered to include only candidates with the same number of letters as the correct answer (in brackets). BOW and RNN models are trained without Word2Vec input embeddings and cosine loss.\relax }}{101}{table.caption.72}}
\newlabel{egs}{{4.7}{101}{Responses from different models to example crossword clues. In each case the model output is filtered to include only candidates with the same number of letters as the correct answer (in brackets). BOW and RNN models are trained without Word2Vec input embeddings and cosine loss.\relax }{table.caption.72}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.3}Results}{101}{subsection.4.3.3}}
\citation{mikolov2013distributed}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.4}Qualitative analysis}{102}{subsection.4.3.4}}
\citation{bordes2014question,graves2014neural,weston2015towards}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Conclusion}{103}{section.4.4}}
\citation{baroni2014don}
\citation{levy2014neural}
\citation{hill2014simlex}
\citation{collobert2011natural}
\citation{mitchell2008vector,clark2007combining,baroni2014frege,milajevs2014evaluating}
\citation{Sutskever2014sequence}
\citation{mao2014deep}
\citation{serban2015building}
\citation{cho2014learning}
\citation{norman1972memory}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Representing sentences with neural language models}{105}{chapter.5}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{CH5}{{5}{105}{Representing sentences with neural language models}{chapter.5}{}}
\citation{kiros2015skip}
\citation{hill2015learning}
\citation{socher2011semi}
\citation{kalchbrenner2014convolutional}
\citation{milajevs2014evaluating}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Distributed Sentence Representations}{106}{section.5.1}}
\citation{kiros2015skip}
\citation{cho2014learning}
\citation{le2014distributed}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.1}Existing Models Trained on Text}{107}{subsection.5.1.1}}
\citation{mikolov2013distributed}
\citation{mitchell2010composition}
\citation{marcobaronijointly}
\citation{hill2015learning}
\citation{chen2015microsoft}
\citation{szegedy2014going}
\citation{russakovsky2014imagenet}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.2}Models Trained on Structured Resources}{108}{subsection.5.1.2}}
\citation{cho2014learning}
\citation{vincent2008extracting}
\citation{vincent2010stacked}
\citation{dai2015semi}
\citation{iyyer2015deep}
\citation{sutskever2011generating}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.3}Novel Text-Based Models}{109}{subsection.5.1.3}}
\citation{harris1954distributional,polajnar2015exploration}
\newlabel{eqn1}{{5.1}{110}{Novel Text-Based Models}{equation.5.1.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.1}{\ignorespaces  {\bf  Properties of models compared in this study} \hspace  {6mm} {\bf  OS:} requires training corpus of sentences in order. {\bf  R:} requires structured resource for training. {\bf  WO:} encoder sensitive to word order. {\bf  SD:} dimension of sentence representation. {\bf  WD:} dimension of word representation. {\bf  TR:} approximate training time (hours) on the dataset in this paper. * indicates trained on GPU. {\bf  TE:} approximate time (s) taken to encode 0.5m sentences.\relax }}{111}{table.caption.73}}
\newlabel{modelprops}{{5.1}{111}{{\bf Properties of models compared in this study} \hspace {6mm} {\bf OS:} requires training corpus of sentences in order. {\bf R:} requires structured resource for training. {\bf WO:} encoder sensitive to word order. {\bf SD:} dimension of sentence representation. {\bf WD:} dimension of word representation. {\bf TR:} approximate training time (hours) on the dataset in this paper. * indicates trained on GPU. {\bf TE:} approximate time (s) taken to encode 0.5m sentences.\relax }{table.caption.73}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.2}{\ignorespaces  Example sentence pairs and `similarity' ratings from the unsupervised evaluations used in this study.\relax }}{111}{table.caption.74}}
\newlabel{unsex}{{5.2}{111}{Example sentence pairs and `similarity' ratings from the unsupervised evaluations used in this study.\relax }{table.caption.74}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.4}Training and Model Selection}{111}{subsection.5.1.4}}
\citation{collobert2011natural,mikolov2013efficient,kiros2015skip}
\citation{hill2015learning,baroni2014don,levy2015improving}
\citation{dolan2004unsupervised}
\citation{pang2005seeing}
\citation{hu2004mining}
\citation{pang2004sentimental}
\citation{wiebe2005annotating}
\citation{voorhees2002overview}
\citation{kiros2015skip}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Evaluating Sentence Representations}{112}{section.5.2}}
\@writefile{lot}{\contentsline {table}{\numberline {5.3}{\ignorespaces  Performance of sentence representation models on {\bf  supervised} evaluations (Section\nobreakspace  {}\ref  {supersec}). Bold numbers indicate best performance in class. Underlined indicates best overall. \relax }}{112}{table.caption.75}}
\newlabel{supervised}{{5.3}{112}{Performance of sentence representation models on {\bf supervised} evaluations (Section~\ref {supersec}). Bold numbers indicate best performance in class. Underlined indicates best overall. \relax }{table.caption.75}{}}
\citation{marelli2014sick}
\citation{agirre2014semeval}
\@writefile{lot}{\contentsline {table}{\numberline {5.4}{\ignorespaces  Performance of sentence representation models (Spearman/Pearson correlations) on {\bf  unsupervised} (relatedness) evaluations (Section\nobreakspace  {}\ref  {unseval}). Models are grouped according to training data as indicated in Table\nobreakspace  {}\ref  {supervised}.\relax }}{113}{table.caption.76}}
\newlabel{unsupervised}{{5.4}{113}{Performance of sentence representation models (Spearman/Pearson correlations) on {\bf unsupervised} (relatedness) evaluations (Section~\ref {unseval}). Models are grouped according to training data as indicated in Table~\ref {supervised}.\relax }{table.caption.76}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.1}Supervised Evaluations}{113}{subsection.5.2.1}}
\newlabel{supersec}{{5.2.1}{113}{Supervised Evaluations}{subsection.5.2.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.2}Unsupervised Evaluations}{113}{subsection.5.2.2}}
\newlabel{unseval}{{5.2.2}{113}{Unsupervised Evaluations}{subsection.5.2.2}{}}
\citation{ji2013discriminative}
\citation{vincent2008extracting}
\citation{marcobaronijointly}
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Results}{114}{section.5.3}}
\citation{voorhees2002overview}
\citation{bruni2014multimodal}
\citation{almahairi2015learning}
\@writefile{toc}{\contentsline {section}{\numberline {5.4}Discussion}{115}{section.5.4}}
\@writefile{lot}{\contentsline {table}{\numberline {5.5}{\ignorespaces Sample nearest neighbour queries selected from a randomly sampled 0.5m sentences of the Toronto Books Corpus.\relax }}{116}{table.caption.77}}
\newlabel{neighbours}{{5.5}{116}{Sample nearest neighbour queries selected from a randomly sampled 0.5m sentences of the Toronto Books Corpus.\relax }{table.caption.77}{}}
\citation{wiebe2005annotating}
\@writefile{lot}{\contentsline {table}{\numberline {5.6}{\ignorespaces  Internal consistency (Chronbach's \(\alpha \)) among supervised evaluations when individual benchmarks are left out of the cohort. Consistency rank is in parentheses (1 = most consistent with other evaluations).\relax }}{118}{table.caption.78}}
\newlabel{consistency}{{5.6}{118}{Internal consistency (Chronbach's \(\alpha \)) among supervised evaluations when individual benchmarks are left out of the cohort. Consistency rank is in parentheses (1 = most consistent with other evaluations).\relax }{table.caption.78}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.7}{\ignorespaces Internal consistency (Chronbach's \(\alpha \)) among unsupervised evaluations when individual benchmarks are left out of the cohort.\relax }}{118}{table.caption.79}}
\@writefile{toc}{\contentsline {section}{\numberline {5.5}Conclusion}{118}{section.5.5}}
\@writefile{toc}{\contentsline {paragraph}{What is the optimal representation `scope'?}{119}{section*.80}}
\@writefile{toc}{\contentsline {paragraph}{What, if anything, is the correct model prior for sentences?}{119}{section*.81}}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Representing word, phrase and sentence semantics in memory networks}{121}{chapter.6}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{CH6}{{6}{121}{Representing word, phrase and sentence semantics in memory networks}{chapter.6}{}}
\citation{altmann1988interaction,binder2011neurobiology}
\citation{weston2014memory}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Testing representations `in the wild'}{122}{section.6.1}}
\citation{manning2014stanford}
\@writefile{toc}{\contentsline {section}{\numberline {6.2}The Children's Book Test}{123}{section.6.2}}
\citation{baayen1996word}
\citation{zweig2011microsoft}
\@writefile{lof}{\contentsline {figure}{\numberline {6.1}{\ignorespaces {\bf  A Named Entity question from the CBT} (right), created from a book passage (left, in blue). In this case, the candidate answers \(C\) are both entities and common nouns, since fewer than ten named entities are found in the context.\relax }}{124}{figure.caption.82}}
\newlabel{fig:goldilocks}{{6.1}{124}{{\bf A Named Entity question from the CBT} (right), created from a book passage (left, in blue). In this case, the candidate answers \(C\) are both entities and common nouns, since fewer than ten named entities are found in the context.\relax }{figure.caption.82}{}}
\citation{nips15_hermann}
\citation{richardson2013mctest}
\newlabel{tab:cbt_stat}{{\caption@xref {tab:cbt_stat}{ on input line 43}}{125}{The Children's Book Test}{table.caption.83}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.1}{\ignorespaces  {\bf  Statistics of the CBT.} Breakdown by question class is provided with the data set files.\relax }}{125}{table.caption.83}}
\newlabel{tab:cbt_stat}{{6.1}{125}{{\bf Statistics of the CBT.} Breakdown by question class is provided with the data set files.\relax }{table.caption.83}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.1}Related resources}{125}{subsection.6.2.1}}
\citation{sukhbaatar2015end}
\citation{sukhbaatar2015end}
\@writefile{toc}{\contentsline {section}{\numberline {6.3}Memory representation in memory networks}{126}{section.6.3}}
\newlabel{sec:memnn}{{6.3}{126}{Memory representation in memory networks}{section.6.3}{}}
\citation{sukhbaatar2015end}
\citation{bordes2015large}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.1}End-to-end training}{127}{subsection.6.3.1}}
\newlabel{sec:mod_o}{{6.3.1}{127}{End-to-end training}{subsection.6.3.1}{}}
\newlabel{eq:eq_o}{{6.1}{127}{End-to-end training}{equation.6.3.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.2}Self-supervision for Window Memories}{127}{subsection.6.3.2}}
\newlabel{sec:ssup}{{6.3.2}{127}{Self-supervision for Window Memories}{subsection.6.3.2}{}}
\citation{xu2015show}
\citation{xu2015show}
\citation{williams1992simple}
\newlabel{eq:salient}{{6.2}{128}{Self-supervision for Window Memories}{equation.6.3.2}{}}
\citation{richardson2013mctest}
\citation{nips15_hermann}
\citation{Heafield-estimate}
\citation{kuhn1990cache}
\citation{weston2010large}
\@writefile{toc}{\contentsline {section}{\numberline {6.4}Baseline and ocmparison models}{129}{section.6.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4.1}Non-learning baselines}{129}{subsection.6.4.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4.2}N-gram language models}{129}{subsection.6.4.2}}
\citation{mikolov2012context}
\citation{rush2015neural}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4.3}Supervised embedding models}{130}{subsection.6.4.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4.4}Recurrent language models}{130}{subsection.6.4.4}}
\citation{mikolov2012context}
\citation{huang2012improving}
\citation{weston2014memory}
\citation{sukhbaatar2015end}
\citation{kumar2015ask}
\citation{nips15_hermann}
\citation{kumar2015ask}
\citation{nips15_hermann}
\citation{joulin2015inferring,dyer2015transition,grefenstette2015learning}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4.5}Human performance}{131}{subsection.6.4.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4.6}Other related approaches}{131}{subsection.6.4.6}}
\citation{nips15_hermann}
\@writefile{lot}{\contentsline {table}{\numberline {6.2}{\ignorespaces  {\bf  Results on CBT test set.} $^{(*)}$Human results were collected on 10\% of the test set.\relax }}{132}{table.caption.84}}
\newlabel{tab:cbt_res}{{6.2}{132}{{\bf Results on CBT test set.} $^{(*)}$Human results were collected on 10\% of the test set.\relax }{table.caption.84}{}}
\newlabel{tab:cbt_res}{{6.2}{132}{{\bf Results on CBT test set.} $^{(*)}$Human results were collected on 10\% of the test set.\relax }{table.caption.84}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.5}Results}{132}{section.6.5}}
\newlabel{sec:results}{{6.5}{132}{Results}{section.6.5}{}}
\@writefile{toc}{\contentsline {paragraph}{The form of memory representations}{132}{section*.85}}
\citation{bengio1994learning}
\@writefile{toc}{\contentsline {paragraph}{Modelling syntactic flow}{133}{section*.86}}
\@writefile{toc}{\contentsline {paragraph}{Capturing semantic coherence}{133}{section*.87}}
\citation{nips15_hermann}
\citation{nips15_hermann}
\citation{nips15_hermann}
\@writefile{lof}{\contentsline {figure}{\numberline {6.2}{\ignorespaces  {\bf  Correct predictions of MemNNs (window memory + self-supervision) on CBT} on Named Entity (left) and Verb (right). Circled phrases indicate all considered windows; red ones are the ones corresponding to the returned (correct) answer; the blue windows represent the queries.\relax }}{134}{figure.caption.88}}
\newlabel{tab:ex_pred_cbt}{{6.2}{134}{{\bf Correct predictions of MemNNs (window memory + self-supervision) on CBT} on Named Entity (left) and Verb (right). Circled phrases indicate all considered windows; red ones are the ones corresponding to the returned (correct) answer; the blue windows represent the queries.\relax }{figure.caption.88}{}}
\newlabel{tab:ex_pred_cbt}{{6.2}{134}{{\bf Correct predictions of MemNNs (window memory + self-supervision) on CBT} on Named Entity (left) and Verb (right). Circled phrases indicate all considered windows; red ones are the ones corresponding to the returned (correct) answer; the blue windows represent the queries.\relax }{figure.caption.88}{}}
\@writefile{toc}{\contentsline {paragraph}{Self-supervised memory retrieval}{134}{section*.89}}
\newlabel{tab:qacnn_res}{{\caption@xref {tab:qacnn_res}{ on input line 452}}{134}{Self-supervised memory retrieval}{table.caption.90}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.3}{\ignorespaces  {\bf  Results on CNN QA.} $^{(*)}$Results taken from \cite  {nips15_hermann}.\relax }}{134}{table.caption.90}}
\newlabel{tab:qacnn_res}{{6.3}{134}{{\bf Results on CNN QA.} $^{(*)}$Results taken from \cite {nips15_hermann}.\relax }{table.caption.90}{}}
\newlabel{cnn}{{6.3}{134}{{\bf Results on CNN QA.} $^{(*)}$Results taken from \cite {nips15_hermann}.\relax }{table.caption.90}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.5.1}News Article Question Answering}{134}{subsection.6.5.1}}
\newlabel{dmind}{{6.5.1}{134}{News Article Question Answering}{subsection.6.5.1}{}}
\citation{nips15_hermann}
\citation{hinton2012improving}
\citation{nips15_hermann}
\citation{wan2013regularization}
\citation{graves2008unconstrained}
\citation{luong2015effective}
\@writefile{toc}{\contentsline {section}{\numberline {6.6}Conclusion}{136}{section.6.6}}
\@writefile{toc}{\contentsline {chapter}{\numberline {7}Conclusion}{137}{chapter.7}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {7.1}Contributions of this thesis}{137}{section.7.1}}
\@writefile{toc}{\contentsline {paragraph}{A new resource for the evaluation of distributed word representations}{138}{section*.91}}
\@writefile{toc}{\contentsline {paragraph}{Two novel methods for acquiring distributed word representations}{138}{section*.92}}
\@writefile{toc}{\contentsline {paragraph}{Learning phrase representations by training NLMs on dictionaries or encyclopedias}{139}{section*.93}}
\@writefile{toc}{\contentsline {paragraph}{Two novel models for learning distributed sentence representations from text}{139}{section*.94}}
\@writefile{toc}{\contentsline {paragraph}{Representing naturally-occurring language in memory networks}{139}{section*.95}}
\@writefile{toc}{\contentsline {section}{\numberline {7.2}Future work}{140}{section.7.2}}
\bibstyle{plainnat}
\bibdata{thesis}
\bibcite{Agirre2009}{{1}{2009{a}}{{Agirre et~al.}}{{Agirre, Alfonseca, Hall, Kravalova, Pasca, and Soroa}}}
\bibcite{agirre2009study}{{2}{2009{b}}{{Agirre et~al.}}{{Agirre, Alfonseca, Hall, Kravalova, Pa{\c {s}}ca, and Soroa}}}
\bibcite{agirre2014semeval}{{3}{2014}{{Agirre et~al.}}{{Agirre, Banea, Cardie, Cer, Diab, Gonzalez-Agirre, Guo, Mihalcea, Rigau, and Wiebe}}}
\bibcite{alfonseca2002extending}{{4}{2002}{{Alfonseca and Manandhar}}{{}}}
\bibcite{almahairi2015learning}{{5}{2015}{{Almahairi et~al.}}{{Almahairi, Kastner, Cho, and Courville}}}
\bibcite{altmann1988interaction}{{6}{1988}{{Altmann and Steedman}}{{}}}
\bibcite{andrews2009integrating}{{7}{2009}{{Andrews et~al.}}{{Andrews, Vigliocco, and Vinson}}}
\bibcite{baayen1996word}{{8}{1996}{{Baayen and Lieber}}{{}}}
\@writefile{toc}{\contentsline {chapter}{Bibliography}{143}{section*.96}}
\bibcite{bahdanau2014neural}{{9}{2015}{{Bahdanau et~al.}}{{Bahdanau, Cho, and Bengio}}}
\bibcite{bansal2014tailoring}{{10}{2014}{{Bansal et~al.}}{{Bansal, Gimpel, and Livescu}}}
\bibcite{baroni2014frege}{{11}{2014{a}}{{Baroni et~al.}}{{Baroni, Bernardi, and Zamparelli}}}
\bibcite{baroni2014don}{{12}{2014{b}}{{Baroni et~al.}}{{Baroni, Dinu, and Kruszewski}}}
\bibcite{barsalou2010grounded}{{13}{2010}{{Barsalou}}{{}}}
\bibcite{barsalou2005situating}{{14}{2005}{{Barsalou and Wiemer-Hastings}}{{}}}
\bibcite{barsalou2003grounding}{{15}{2003}{{Barsalou et~al.}}{{Barsalou, Kyle~Simmons, Barbey, and Wilson}}}
\bibcite{beltagysemantic}{{16}{2014}{{Beltagy et~al.}}{{Beltagy, Erk, and Mooney}}}
\bibcite{Bengio+Senecal-2003-small}{{17}{2003}{{Bengio and S\'en\'ecal}}{{}}}
\bibcite{bengio1994learning}{{18}{1994}{{Bengio et~al.}}{{Bengio, Simard, and Frasconi}}}
\bibcite{Bengio2003lm}{{19}{2003{a}}{{Bengio et~al.}}{{Bengio, Ducharme, Vincent, and Janvin}}}
\bibcite{bengio2003neural}{{20}{2003{b}}{{Bengio et~al.}}{{Bengio, Ducharme, Vincent, and Janvin}}}
\bibcite{berant14paraphrasing}{{21}{2014}{{Berant and Liang}}{{}}}
\bibcite{bergstra+al:2010-scipy}{{22}{2010}{{Bergstra et~al.}}{{Bergstra, Breuleux, Bastien, Lamblin, Pascanu, Desjardins, Turian, Warde-Farley, and Bengio}}}
\bibcite{bernardi2013relatedness}{{23}{2013}{{Bernardi et~al.}}{{Bernardi, Dinu, Marelli, and Baroni}}}
\bibcite{bilac2003improving}{{24}{2003}{{Bilac et~al.}}{{Bilac, Baldwin, and Tanaka}}}
\bibcite{bilac2004dictionary}{{25}{2004}{{Bilac et~al.}}{{Bilac, Watanabe, Hashimoto, Tokunaga, and Tanaka}}}
\bibcite{binder2011neurobiology}{{26}{2011}{{Binder and Desai}}{{}}}
\bibcite{bird2006nltk}{{27}{2006}{{Bird}}{{}}}
\bibcite{blei2003latent}{{28}{2003}{{Blei et~al.}}{{Blei, Ng, and Jordan}}}
\bibcite{bordes2014question}{{29}{2014}{{Bordes et~al.}}{{Bordes, Chopra, and Weston}}}
\bibcite{bordes2015large}{{30}{2015}{{Bordes et~al.}}{{Bordes, Usunier, Chopra, and Weston}}}
\bibcite{bruni2012distributional}{{31}{2012{a}}{{Bruni et~al.}}{{Bruni, Boleda, Baroni, and Tran}}}
\bibcite{bruni2012distributional2}{{32}{2012{b}}{{Bruni et~al.}}{{Bruni, Uijlings, Baroni, and Sebe}}}
\bibcite{bruni2014multimodal}{{33}{2014}{{Bruni et~al.}}{{Bruni, Tran, and Baroni}}}
\bibcite{budanitsky2006evaluating}{{34}{2006}{{Budanitsky and Hirst}}{{}}}
\bibcite{bybee2001frequency}{{35}{2001}{{Bybee and Hopper}}{{}}}
\bibcite{lauly2014autoencoder}{{36}{2014}{{Chandar et~al.}}{{Chandar, Lauly, Larochelle, Khapra, Ravindran, Raykar, and Saha}}}
\bibcite{chater2006probabilistic}{{37}{2006}{{Chater and Manning}}{{}}}
\bibcite{chen2015microsoft}{{38}{2015}{{Chen et~al.}}{{Chen, Fang, Lin, Vedantam, Gupta, Dollar, and Zitnick}}}
\bibcite{cho2014learning}{{39}{2014{a}}{{Cho et~al.}}{{Cho, Van~Merri{\"e}nboer, Gulcehre, Bahdanau, Bougares, Schwenk, and Bengio}}}
\bibcite{Cho2014}{{40}{2014{b}}{{Cho et~al.}}{{Cho, van Merrienboer, Gulcehre, Bougares, Schwenk, and Bengio}}}
\bibcite{clark2007combining}{{41}{2007}{{Clark and Pulman}}{{}}}
\bibcite{collobert2008unified}{{42}{2008}{{Collobert and Weston}}{{}}}
\bibcite{collobert2011natural}{{43}{2011}{{Collobert et~al.}}{{Collobert, Weston, Bottou, Karlen, Kavukcuoglu, and Kuksa}}}
\bibcite{cordier1965}{{44}{1965}{{Cordier}}{{}}}
\bibcite{cruse1986lexical}{{45}{1986}{{Cruse}}{{}}}
\bibcite{crutch2005abstract}{{46}{2005}{{Crutch and Warrington}}{{}}}
\bibcite{crutch2009different}{{47}{2009}{{Crutch et~al.}}{{Crutch, Connell, and Warrington}}}
\bibcite{cunningham2005information}{{48}{2005}{{Cunningham}}{{}}}
\bibcite{dai2015semi}{{49}{2015}{{Dai and Le}}{{}}}
\bibcite{devereux2013centre}{{50}{2013}{{Devereux et~al.}}{{Devereux, Tyler, Geertzen, and Randall}}}
\bibcite{devlin2014fast}{{51}{2014}{{Devlin et~al.}}{{Devlin, Zbib, Huang, Lamar, Schwartz, and Makhoul}}}
\bibcite{dolan2004unsupervised}{{52}{2004}{{Dolan et~al.}}{{Dolan, Quirk, and Brockett}}}
\bibcite{dyer2015transition}{{53}{2015}{{Dyer et~al.}}{{Dyer, Ballesteros, Ling, Matthews, and Smith}}}
\bibcite{faruqui2014improving}{{54}{2014}{{Faruqui and Dyer}}{{}}}
\bibcite{faruqui2014retrofitting}{{55}{2014}{{Faruqui et~al.}}{{Faruqui, Dodge, Jauhar, Dyer, Hovy, and Smith}}}
\bibcite{fellbaum1999wordnet}{{56}{1999}{{Fellbaum}}{{}}}
\bibcite{feng2010visual}{{57}{2010}{{Feng and Lapata}}{{}}}
\bibcite{ferrucci2010building}{{58}{2010}{{Ferrucci et~al.}}{{Ferrucci, Brown, Chu-Carroll, Fan, Gondek, Kalyanpur, Lally, Murdock, Nyberg, Prager, Schlaefer, and Welty}}}
\bibcite{finkelstein2001placing}{{59}{2001}{{Finkelstein et~al.}}{{Finkelstein, Gabrilovich, Matias, Rivlin, Solan, Wolfman, and Ruppin}}}
\bibcite{dist}{{60}{1957}{{Firth}}{{}}}
\bibcite{fodor1983modularity}{{61}{1983}{{Fodor}}{{}}}
\bibcite{gentner1978relational}{{62}{1978}{{Gentner}}{{}}}
\bibcite{gentner2006verbs}{{63}{2006}{{Gentner}}{{}}}
\bibcite{gershmanmetaphor}{{64}{2014}{{Gershman and Dyer}}{{}}}
\bibcite{ginsberg2011dr}{{65}{2011}{{Ginsberg}}{{}}}
\bibcite{golub1970singular}{{66}{1970}{{Golub and Reinsch}}{{}}}
\bibcite{gouws2014bilbowa}{{67}{2014}{{Gouws et~al.}}{{Gouws, Bengio, and Corrado}}}
\bibcite{grafton2009embodied}{{68}{2009}{{Grafton}}{{}}}
\bibcite{graves2008unconstrained}{{69}{2008}{{Graves et~al.}}{{Graves, Liwicki, Bunke, Schmidhuber, and Fern{\'a}ndez}}}
\bibcite{graves2014neural}{{70}{2014}{{Graves et~al.}}{{Graves, Wayne, and Danihelka}}}
\bibcite{grefenstette2015learning}{{71}{2015}{{Grefenstette et~al.}}{{Grefenstette, Hermann, Suleyman, and Blunsom}}}
\bibcite{griffiths2007topics}{{72}{2007}{{Griffiths et~al.}}{{Griffiths, Steyvers, and Tenenbaum}}}
\bibcite{grigg2009lacan}{{73}{2009}{{Grigg}}{{}}}
\bibcite{Haghighi2008Learning}{{74}{2008}{{Haghighi et~al.}}{{Haghighi, Liang, Berg-Kirkpatrick, and Klein}}}
\bibcite{hardoon2004canonical}{{75}{2004}{{Hardoon et~al.}}{{Hardoon, Szedmak, and Shawe-Taylor}}}
\bibcite{harper1965}{{76}{1965}{{Harper}}{{}}}
\bibcite{harris1954distributional}{{77}{1954}{{Harris}}{{}}}
\bibcite{hassan2011semantic}{{78}{2011}{{Hassan and Mihalcea}}{{}}}
\bibcite{he2008indirect}{{79}{2008}{{He et~al.}}{{He, Yang, Gao, Nguyen, and Moore}}}
\bibcite{Heafield-estimate}{{80}{2013}{{Heafield et~al.}}{{Heafield, Pouzyrevsky, Clark, and Koehn}}}
\bibcite{hermann2013multilingual}{{81}{2013}{{Hermann and Blunsom}}{{}}}
\bibcite{Hermann:2014:ICLR}{{82}{2014}{{Hermann and Blunsom}}{{}}}
\bibcite{nips15_hermann}{{83}{2015}{{Hermann et~al.}}{{Hermann, Ko\v {c}isk\'y, Grefenstette, Espeholt, Kay, Suleyman, and Blunsom}}}
\bibcite{hill2013concreteness}{{84}{2013{a}}{{Hill et~al.}}{{Hill, Kiela, and Korhonen}}}
\bibcite{hill2013quantitative}{{85}{2013{b}}{{Hill et~al.}}{{Hill, Korhonen, and Bentz}}}
\bibcite{hill2014multi}{{86}{2014}{{Hill et~al.}}{{Hill, Reichart, and Korhonen}}}
\bibcite{hill2015learning}{{87}{2015{a}}{{Hill et~al.}}{{Hill, Cho, Korhonen, and Bengio}}}
\bibcite{hill2014simlex}{{88}{2015{b}}{{Hill et~al.}}{{Hill, Reichart, and Korhonen}}}
\bibcite{hinton2012improving}{{89}{2012}{{Hinton et~al.}}{{Hinton, Srivastava, Krizhevsky, Sutskever, and Salakhutdinov}}}
\bibcite{hochreiter1997long}{{90}{1997}{{Hochreiter and Schmidhuber}}{{}}}
\bibcite{hu2004mining}{{91}{2004}{{Hu and Liu}}{{}}}
\bibcite{huang2012improving}{{92}{2012}{{Huang et~al.}}{{Huang, Socher, Manning, and Ng}}}
\bibcite{Iyyer:Boyd-Graber:Claudino:Socher:Daume-2014}{{93}{2014}{{Iyyer et~al.}}{{Iyyer, Boyd-Graber, Claudino, Socher, and {Daum\'e III}}}}
\bibcite{iyyer2015deep}{{94}{2015}{{Iyyer et~al.}}{{Iyyer, Manjunatha, Boyd-Graber, and III}}}
\bibcite{Jean}{{95}{2015}{{Jean et~al.}}{{Jean, Cho, Memisevic, and Bengio}}}
\bibcite{ji2013discriminative}{{96}{2013}{{Ji and Eisenstein}}{{}}}
\bibcite{johns2012perceptual}{{97}{2012}{{Johns and Jones}}{{}}}
\bibcite{joulin2015inferring}{{98}{2015}{{Joulin and Mikolov}}{{}}}
\bibcite{kalchbrenner13emnlp}{{99}{2013}{{Kalchbrenner and Blunsom}}{{}}}
\bibcite{kalchbrenner2014convolutional}{{100}{2014}{{Kalchbrenner et~al.}}{{Kalchbrenner, Grefenstette, and Blunsom}}}
\bibcite{kiela2014systematic}{{101}{2014}{{Kiela and Clark}}{{}}}
\bibcite{kiela2015multi}{{102}{2015}{{Kiela and Clark}}{{}}}
\bibcite{kielaimproving}{{103}{2014}{{Kiela et~al.}}{{Kiela, Hill, Korhonen, and Clark}}}
\bibcite{kiros2014unifying}{{104}{2015{a}}{{Kiros et~al.}}{{Kiros, Salakhutdinov, and Zemel}}}
\bibcite{kiros2015skip}{{105}{2015{b}}{{Kiros et~al.}}{{Kiros, Zhu, Salakhutdinov, Zemel, Urtasun, Torralba, and Fidler}}}
\bibcite{Klementiev}{{106}{2012}{{Klementiev et~al.}}{{Klementiev, Titov, and Bhattarai}}}
\bibcite{Kocisky:2014}{{107}{2014}{{Ko\v {c}isk\'{y} et~al.}}{{Ko\v {c}isk\'{y}, Hermann, and Blunsom}}}
\bibcite{kuhn1990cache}{{108}{1990}{{Kuhn and De~Mori}}{{}}}
\bibcite{kumar2015ask}{{109}{2015}{{Kumar et~al.}}{{Kumar, Irsoy, Su, Bradbury, English, Pierce, Ondruska, Gulrajani, and Socher}}}
\bibcite{landauer1997solution}{{110}{1997}{{Landauer and Dumais}}{{}}}
\bibcite{le2014distributed}{{111}{2014}{{Le and Mikolov}}{{}}}
\bibcite{leech1994claws4}{{112}{1994}{{Leech et~al.}}{{Leech, Garside, and Bryant}}}
\bibcite{levy2014dependency}{{113}{2014{a}}{{Levy and Goldberg}}{{}}}
\bibcite{levy2014neural}{{114}{2014{b}}{{Levy and Goldberg}}{{}}}
\bibcite{levy2015improving}{{115}{2015{a}}{{Levy et~al.}}{{Levy, Goldberg, and Dagan}}}
\bibcite{levy2015supervised}{{116}{2015{b}}{{Levy et~al.}}{{Levy, Remus, Biemann, and Dagan}}}
\bibcite{lewis2004rcv1}{{117}{2004}{{Lewis et~al.}}{{Lewis, Yang, Rose, and Li}}}
\bibcite{littman2002probabilistic}{{118}{2002}{{Littman et~al.}}{{Littman, Keim, and Shazeer}}}
\bibcite{lucas2000semantic}{{119}{2000}{{Lucas}}{{}}}
\bibcite{luong2013better}{{120}{2013}{{Luong et~al.}}{{Luong, Socher, and Manning}}}
\bibcite{luong2015effective}{{121}{2015{a}}{{Luong et~al.}}{{Luong, Pham, and Manning}}}
\bibcite{luong2014addressing}{{122}{2015{b}}{{Luong et~al.}}{{Luong, Sutskever, Le, Vinyals, and Zaremba}}}
\bibcite{manning2014stanford}{{123}{2014}{{Manning et~al.}}{{Manning, Surdeanu, Bauer, Finkel, Bethard, and McClosky}}}
\bibcite{mao2014deep}{{124}{2015}{{Mao et~al.}}{{Mao, Xu, Yang, Wang, and Yulle}}}
\bibcite{marelli2014sick}{{125}{2014}{{Marelli et~al.}}{{Marelli, Menini, Baroni, Bentivogli, Bernardi, and Zamparelli}}}
\bibcite{markman1997similar}{{126}{1997}{{Markman and Wisniewski}}{{}}}
\bibcite{marton2009improved}{{127}{2009}{{Marton et~al.}}{{Marton, Callison-Burch, and Resnik}}}
\bibcite{mcrae2005semantic}{{128}{2005}{{McRae et~al.}}{{McRae, Cree, Seidenberg, and McNorgan}}}
\bibcite{mcrae2012semantic}{{129}{2012}{{McRae et~al.}}{{McRae, Khalkhali, and Hare}}}
\bibcite{mesnil2012unsupervised}{{130}{2012}{{Mesnil et~al.}}{{Mesnil, Dauphin, Glorot, Rifai, Bengio, Goodfellow, Lavoie, Muller, Desjardins, Warde-Farley, et~al.}}}
\bibcite{mikolov2012context}{{131}{2012}{{Mikolov and Zweig}}{{}}}
\bibcite{mikolov2010recurrent}{{132}{2010}{{Mikolov et~al.}}{{Mikolov, Karafi{\'a}t, Burget, Cernock{\`y}, and Khudanpur}}}
\bibcite{mikolov2013efficient}{{133}{2013{a}}{{Mikolov et~al.}}{{Mikolov, Chen, Corrado, and Dean}}}
\bibcite{mikolov2013exploiting}{{134}{2013{b}}{{Mikolov et~al.}}{{Mikolov, Le, and Sutskever}}}
\bibcite{mikolov2013distributed}{{135}{2013{c}}{{Mikolov et~al.}}{{Mikolov, Sutskever, Chen, Corrado, and Dean}}}
\bibcite{milajevs2014evaluating}{{136}{2014}{{Milajevs et~al.}}{{Milajevs, Kartsaklis, Sadrzadeh, and Purver}}}
\bibcite{mitchell2008vector}{{137}{2008}{{Mitchell and Lapata}}{{}}}
\bibcite{mitchell2010composition}{{138}{2010}{{Mitchell and Lapata}}{{}}}
\bibcite{molla2007question}{{139}{2007}{{Moll{\'a} and Vicedo}}{{}}}
\bibcite{morin2005hierarchical}{{140}{2005}{{Morin and Bengio}}{{}}}
\bibcite{navigli2009word}{{141}{2009}{{Navigli}}{{}}}
\bibcite{nelson2004university}{{142}{2004}{{Nelson et~al.}}{{Nelson, McEvoy, and Schreiber}}}
\bibcite{norman1972memory}{{143}{1972}{{Norman}}{{}}}
\bibcite{pado2007flexible}{{144}{2007}{{Pad\'o et~al.}}{{Pad\'o, Pad\'o, and Erk}}}
\bibcite{paivio1991dual}{{145}{1991}{{Paivio}}{{}}}
\bibcite{pang2004sentimental}{{146}{2004}{{Pang and Lee}}{{}}}
\bibcite{pang2005seeing}{{147}{2005}{{Pang and Lee}}{{}}}
\bibcite{pedersen2004wordnet}{{148}{2004}{{Pedersen et~al.}}{{Pedersen, Patwardhan, and Michelizzi}}}
\bibcite{Pennington2014}{{149}{2014}{{Pennington et~al.}}{{Pennington, Socher, and Manning}}}
\bibcite{marcobaronijointly}{{150}{2015}{{Pham et~al.}}{{Pham, Kruszewski, Lazaridou, and Baroni}}}
\bibcite{phan2008learning}{{151}{2008}{{Phan et~al.}}{{Phan, Nguyen, and Horiguchi}}}
\bibcite{plaut1995semantic}{{152}{1995}{{Plaut}}{{}}}
\bibcite{polajnar2015exploration}{{153}{2015}{{Polajnar et~al.}}{{Polajnar, Rimell, and Clark}}}
\bibcite{recchia2009more}{{154}{2009}{{Recchia and Jones}}{{}}}
\bibcite{reisinger2010mixture}{{155}{2010{a}}{{Reisinger and Mooney}}{{}}}
\bibcite{reisinger2010multi}{{156}{2010{b}}{{Reisinger and Mooney}}{{}}}
\bibcite{resnik1995using}{{157}{1995}{{Resnik}}{{}}}
\bibcite{resnik201011}{{158}{2010}{{Resnik and Lin}}{{}}}
\bibcite{richardson2013mctest}{{159}{2013}{{Richardson et~al.}}{{Richardson, Burges, and Renshaw}}}
\bibcite{rollermultimodal}{{160}{2013}{{Roller and Schulte~im Walde}}{{}}}
\bibcite{rosch1976structural}{{161}{1976}{{Rosch et~al.}}{{Rosch, Simpson, and Miller}}}
\bibcite{rose2002reuters}{{162}{2002}{{Rose et~al.}}{{Rose, Stevenson, and Whitehead}}}
\bibcite{rubenstein1965contextual}{{163}{1965}{{Rubenstein and Goodenough}}{{}}}
\bibcite{rush2015neural}{{164}{2015}{{Rush et~al.}}{{Rush, Chopra, and Weston}}}
\bibcite{russakovsky2014imagenet}{{165}{2014}{{Russakovsky et~al.}}{{Russakovsky, Deng, Su, Krause, Satheesh, Ma, Huang, Karpathy, Khosla, Bernstein, et~al.}}}
\bibcite{serban2015building}{{166}{2015}{{Serban et~al.}}{{Serban, Sordoni, Bengio, Courville, and Pineau}}}
\bibcite{shaw2013building}{{167}{2013}{{Shaw et~al.}}{{Shaw, Datta, VanderMeer, and Dutta}}}
\bibcite{silberer2014learning}{{168}{2014}{{Silberer and Lapata}}{{}}}
\bibcite{silberer2012grounded}{{169}{2012}{{Silberer and Lapata}}{{}}}
\bibcite{silberer2013models}{{170}{2013}{{Silberer et~al.}}{{Silberer, Ferrari, and Lapata}}}
\bibcite{socher2011semi}{{171}{2011}{{Socher et~al.}}{{Socher, Pennington, Huang, Ng, and Manning}}}
\bibcite{srivastava2012multimodal}{{172}{2012}{{Srivastava and Salakhutdinov}}{{}}}
\bibcite{sukhbaatar2015end}{{173}{2015}{{Sukhbaatar et~al.}}{{Sukhbaatar, Szlam, Weston, and Fergus}}}
\bibcite{sun2008verb}{{174}{2008}{{Sun et~al.}}{{Sun, Korhonen, and Krymolowski}}}
\bibcite{sutskever2011generating}{{175}{2011}{{Sutskever et~al.}}{{Sutskever, Martens, and Hinton}}}
\bibcite{Sutskever2014sequence}{{176}{2014}{{Sutskever et~al.}}{{Sutskever, Vinyals, and Le}}}
\bibcite{szegedy2014going}{{177}{2014}{{Szegedy et~al.}}{{Szegedy, Liu, Jia, Sermanet, Reed, Anguelov, Erhan, Vanhoucke, and Rabinovich}}}
\bibcite{turian2010word}{{178}{2010}{{Turian et~al.}}{{Turian, Ratinov, and Bengio}}}
\bibcite{turney2012domain}{{179}{2012}{{Turney}}{{}}}
\bibcite{turney2010frequency}{{180}{2010}{{Turney and Pantel}}{{}}}
\bibcite{tversky1977features}{{181}{1977}{{Tversky}}{{}}}
\bibcite{vincent2008extracting}{{182}{2008}{{Vincent et~al.}}{{Vincent, Larochelle, Bengio, and Manzagol}}}
\bibcite{vincent2010stacked}{{183}{2010}{{Vincent et~al.}}{{Vincent, Larochelle, Lajoie, Bengio, and Manzagol}}}
\bibcite{von2004labeling}{{184}{2004}{{Von~Ahn and Dabbish}}{{}}}
\bibcite{voorhees2002overview}{{185}{2002}{{Voorhees}}{{}}}
\bibcite{307754}{{186}{2011}{{Vulic et~al.}}{{Vulic, De~Smet, and Moens}}}
\bibcite{vulic2011identifying}{{187}{2011}{{Vuli{\'c} et~al.}}{{Vuli{\'c}, De~Smet, and Moens}}}
\bibcite{wan2013regularization}{{188}{2013}{{Wan et~al.}}{{Wan, Zeiler, Zhang, Cun, and Fergus}}}
\bibcite{wang2015learning}{{189}{2015}{{Wang et~al.}}{{Wang, Mohamed, and Hirst}}}
\bibcite{weston2010large}{{190}{2010}{{Weston et~al.}}{{Weston, Bengio, and Usunier}}}
\bibcite{weston2015towards}{{191}{2015{a}}{{Weston et~al.}}{{Weston, Bordes, Chopra, and Mikolov}}}
\bibcite{weston2014memory}{{192}{2015{b}}{{Weston et~al.}}{{Weston, Chopra, and Bordes}}}
\bibcite{wiebe2000learning}{{193}{2000}{{Wiebe}}{{}}}
\bibcite{wiebe2005annotating}{{194}{2005}{{Wiebe et~al.}}{{Wiebe, Wilson, and Cardie}}}
\bibcite{katja2005content}{{195}{2005}{{Wiemer-Hastings and Xu}}{{}}}
\bibcite{williams2009predicting}{{196}{2009}{{Williams and Anand}}{{}}}
\bibcite{williams1992simple}{{197}{1992}{{Williams}}{{}}}
\bibcite{wu2013online}{{198}{2013}{{Wu et~al.}}{{Wu, Hoi, Xia, Zhao, Wang, and Miao}}}
\bibcite{wu1994verbs}{{199}{1994}{{Wu and Palmer}}{{}}}
\bibcite{xu2015show}{{200}{2015}{{Xu et~al.}}{{Xu, Ba, Kiros, Courville, Salakhutdinov, Zemel, and Bengio}}}
\bibcite{yong1999case}{{201}{1999}{{Yong and Foo}}{{}}}
\bibcite{zeiler2012adadelta}{{202}{2012}{{Zeiler}}{{}}}
\bibcite{zock2004word}{{203}{2004}{{Zock and Bilac}}{{}}}
\bibcite{zweig2011microsoft}{{204}{2011}{{Zweig and Burges}}{{}}}
\@writefile{toc}{\contentsline {chapter}{\numberline {A}}{163}{appendix.A}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{appendixmem}{{A}{163}{}{appendix.A}{}}
\@writefile{toc}{\contentsline {section}{\numberline {A.1}Experimental Details}{163}{section.A.1}}
\newlabel{ap:hp}{{A.1}{163}{Experimental Details}{section.A.1}{}}
\@writefile{toc}{\contentsline {paragraph}{Setting}{163}{section*.98}}
\@writefile{toc}{\contentsline {paragraph}{Optimal hyper-parameter values on CBT:}{163}{section*.99}}
\@writefile{toc}{\contentsline {paragraph}{Optimal hyper-parameter values on CNN QA:}{164}{section*.100}}
\@writefile{toc}{\contentsline {section}{\numberline {A.2}Results on CBT Validation Set}{164}{section.A.2}}
\newlabel{ap:res_val}{{A.2}{164}{Results on CBT Validation Set}{section.A.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {A.3}Ablation Study on CNN QA}{165}{section.A.3}}
\newlabel{ap:qa_cnn_ab_study}{{A.3}{165}{Ablation Study on CNN QA}{section.A.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {A.4}Effects of Anonymising Entities in CBT}{165}{section.A.4}}
\newlabel{ap:anon}{{A.4}{165}{Effects of Anonymising Entities in CBT}{section.A.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {A.5}Candidates and Window Memories in CBT}{165}{section.A.5}}
\newlabel{ap:nonsparse-windows}{{A.5}{165}{Candidates and Window Memories in CBT}{section.A.5}{}}
\@writefile{lot}{\contentsline {table}{\numberline {A.1}{\ignorespaces  {\bf  Results on CBT test set when considering all windows or candidates.}\relax }}{166}{table.caption.101}}
\newlabel{ap:windoze}{{A.1}{166}{{\bf Results on CBT test set when considering all windows or candidates.}\relax }{table.caption.101}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {B}}{167}{appendix.B}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{appendixmedia}{{B}{167}{}{appendix.B}{}}
