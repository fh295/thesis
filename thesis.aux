\relax 
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\@newglossary{main}{glg}{gls}{glo}
\@istfilename{thesis.ist}
\@glsorder{word}
\@writefile{toc}{\thispagestyle {empty}}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{15}{chapter.1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Neural Language Models}{17}{chapter.2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\citation{andrews2009integrating}
\citation{barsalou2005situating}
\citation{feng2010visual}
\citation{bruni2012distributional}
\citation{silberer2012grounded}
\citation{rollermultimodal}
\citation{bruni2014multimodal}
\citation{silberer2012grounded}
\citation{rollermultimodal}
\citation{silberer2014learning}
\citation{paivio1991dual}
\citation{hill2013quantitative}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Distributed Representations of Words}{19}{chapter.3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Learning Distributed Word Representations from Text}{19}{section.3.1}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Modelling Word Acquisition with Multi-Modal Data and Neural Language Models}{19}{section.3.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Introduction}{19}{subsection.3.2.1}}
\citation{leech1994claws4}
\citation{nelson2004university}
\citation{crutch2005abstract}
\citation{collobert2008unified}
\citation{mesnil2012unsupervised}
\citation{mikolov2013efficient}
\citation{von2004labeling}
\citation{devereux2013centre}
\citation{nelson2004university}
\citation{mikolov2013efficient}
\citation{turian2010word}
\citation{mikolov2013efficient}
\citation{mikolov2013efficient}
\citation{mikolov2013efficient}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Model Design}{21}{subsection.3.2.2}}
\@writefile{toc}{\contentsline {paragraph}{Language-only Model}{21}{section*.5}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Our multi-modal model architecture. Light boxes are elements of the original \cite  {mikolov2013efficient} model. For target words \relax $w_n\relax \GenericError  {               }{LaTeX Error: Bad math environment delimiter}{See the LaTeX manual or LaTeX Companion for explanation.}{Your command was ignored.\MessageBreak Type  I <command> <return>  to replace it with another command,\MessageBreak or  <return>  to continue without it.} in the domain of \relax $\mathbf  {P}\relax \GenericError  {               }{LaTeX Error: Bad math environment delimiter}{See the LaTeX manual or LaTeX Companion for explanation.}{Your command was ignored.\MessageBreak Type  I <command> <return>  to replace it with another command,\MessageBreak or  <return>  to continue without it.}, the model updates based on corpus context words \relax $ w_{n+i} \relax \GenericError  {               }{LaTeX Error: Bad math environment delimiter}{See the LaTeX manual or LaTeX Companion for explanation.}{Your command was ignored.\MessageBreak Type  I <command> <return>  to replace it with another command,\MessageBreak or  <return>  to continue without it.} then on words \relax $p^w_{n+i}\relax \GenericError  {               }{LaTeX Error: Bad math environment delimiter}{See the LaTeX manual or LaTeX Companion for explanation.}{Your command was ignored.\MessageBreak Type  I <command> <return>  to replace it with another command,\MessageBreak or  <return>  to continue without it.} in perceptual psuedo-sentences. Otherwise, updates are based solely on the \relax $ w_{n+i}. \relax \GenericError  {               }{LaTeX Error: Bad math environment delimiter}{See the LaTeX manual or LaTeX Companion for explanation.}{Your command was ignored.\MessageBreak Type  I <command> <return>  to replace it with another command,\MessageBreak or  <return>  to continue without it.}\relax }}{21}{figure.caption.6}}
\citation{duchi2011adaptive}
\citation{morin2005hierarchical}
\citation{mikolov2013efficient}
\citation{bybee2001frequency}
\citation{chater2006probabilistic}
\citation{mikolov2013efficient}
\@writefile{toc}{\contentsline {paragraph}{Multi-modal Extension}{23}{section*.7}}
\citation{von2004labeling}
\citation{devereux2013centre}
\citation{mcrae2005semantic}
\citation{silberer2012grounded}
\citation{rollermultimodal}
\citation{fodor1983modularity}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces  Example pseudo-sentences generated by our model.\relax }}{24}{figure.caption.8}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{examples}{{3.2}{24}{Example pseudo-sentences generated by our model.\relax \relax }{figure.caption.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.3}Information Sources}{24}{subsection.3.2.3}}
\newlabel{percep_sources}{{3.2.3}{24}{Information Sources\relax }{subsection.3.2.3}{}}
\@writefile{toc}{\contentsline {paragraph}{ESPGame Dataset}{24}{section*.9}}
\@writefile{toc}{\contentsline {paragraph}{CSLB Property Norms}{24}{section*.10}}
\citation{nelson2004university}
\citation{andrews2009integrating}
\citation{feng2010visual}
\citation{silberer2012grounded}
\citation{rollermultimodal}
\citation{huang2012improving}
\citation{silberer2012grounded}
\citation{leech1994claws4}
\@writefile{lot}{\contentsline {table}{\numberline {3.1}{\ignorespaces  Concepts identified in images in the ESP Game (left) and features produced for concepts by human annotators in the CSLB dataset (with feature strength, max=30).\relax }}{25}{table.caption.12}}
\newlabel{font-table}{{3.1}{25}{Concepts identified in images in the ESP Game (left) and features produced for concepts by human annotators in the CSLB dataset (with feature strength, max=30).\relax \relax }{table.caption.12}{}}
\@writefile{toc}{\contentsline {paragraph}{Linguistic Input}{25}{section*.11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.4}Evaluation}{25}{subsection.3.2.4}}
\@writefile{lot}{\contentsline {table}{\numberline {3.2}{\ignorespaces  Example concept pairs (with mean concreteness rating) and free-association scores from the USF dataset.\relax }}{26}{table.caption.13}}
\newlabel{font-table}{{3.2}{26}{Example concept pairs (with mean concreteness rating) and free-association scores from the USF dataset.\relax \relax }{table.caption.13}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.3}{\ignorespaces  Details the subsets of USF data used in our evaluations, downloadable from our website.\relax }}{26}{table.caption.14}}
\newlabel{font-table}{{3.3}{26}{Details the subsets of USF data used in our evaluations, downloadable from our website.\relax \relax }{table.caption.14}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.5}Results and Discussion}{26}{subsection.3.2.5}}
\citation{mikolov2013efficient}
\citation{silberer2012grounded}
\citation{hardoon2004canonical}
\citation{bruni2014multimodal}
\citation{bruni2014multimodal}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.6}Combining information sources}{27}{subsection.3.2.6}}
\citation{johns2012perceptual}
\citation{johns2012perceptual}
\citation{myers1990classical}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.7}Propagating input to abstract concepts}{28}{subsection.3.2.7}}
\@writefile{toc}{\contentsline {paragraph}{Johns and Jones}{28}{section*.15}}
\@writefile{toc}{\contentsline {paragraph}{Ridge Regression}{28}{section*.16}}
\citation{mikolov2013efficient}
\citation{feng2010visual}
\citation{silberer2012grounded}
\citation{silberer2013models}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces  The proposed approach compared with other methods of information combination (left) and propagation. Dashed lines indicate language-only model baseline.\relax }}{29}{figure.caption.18}}
\newlabel{main_results}{{3.3}{29}{The proposed approach compared with other methods of information combination (left) and propagation. Dashed lines indicate language-only model baseline.\relax \relax }{figure.caption.18}{}}
\@writefile{toc}{\contentsline {paragraph}{Comparisons}{29}{section*.17}}
\citation{katja2005content}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces  Top: Comparing the strategy of directly representing abstract concepts from perceptual information where available (yellow bars) vs. propagating via concrete concepts. Bottom: The effect of increasing \relax $\alpha \relax \GenericError  {               }{LaTeX Error: Bad math environment delimiter}{See the LaTeX manual or LaTeX Companion for explanation.}{Your command was ignored.\MessageBreak Type  I <command> <return>  to replace it with another command,\MessageBreak or  <return>  to continue without it.} on correlation with USF pairs (Spearman \relax $\rho \relax \GenericError  {               }{LaTeX Error: Bad math environment delimiter}{See the LaTeX manual or LaTeX Companion for explanation.}{Your command was ignored.\MessageBreak Type  I <command> <return>  to replace it with another command,\MessageBreak or  <return>  to continue without it.}) for each concept type. Horizontal dashed lines indicate language-only model baseline.\relax }}{30}{figure.caption.19}}
\newlabel{repprop}{{3.4}{30}{Top: Comparing the strategy of directly representing abstract concepts from perceptual information where available (yellow bars) vs. propagating via concrete concepts. Bottom: The effect of increasing \(\alpha \) on correlation with USF pairs (Spearman \(\rho \)) for each concept type. Horizontal dashed lines indicate language-only model baseline.\relax \relax }{figure.caption.19}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.8}Direct representation vs. propagation}{31}{subsection.3.2.8}}
\citation{srivastava2012multimodal}
\citation{wu2013online}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.9}Source and quantity of perceptual input}{32}{subsection.3.2.9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.10}Conclusions}{32}{subsection.3.2.10}}
\citation{grafton2009embodied}
\citation{barsalou2010grounded}
\@writefile{toc}{\contentsline {paragraph}{Type I}{33}{section*.20}}
\@writefile{toc}{\contentsline {paragraph}{Type II}{33}{section*.21}}
\@writefile{toc}{\contentsline {paragraph}{Type III}{33}{section*.22}}
\citation{landauer1997solution}
\citation{turney2010frequency}
\citation{Bengio2003lm}
\citation{mnih2009scalable}
\citation{collobert2008unified}
\citation{mikolov2013distributed}
\citation{Pennington2014}
\citation{baroni2014don}
\citation{kalchbrenner13emnlp}
\citation{devlin2014fast}
\citation{Sutskever2014sequence}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Improving the Evaluation of Word Representations}{34}{section.3.3}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Sequence-to-Sequence Learning of Word Representations From Bilingual Data}{34}{section.3.4}}
\@writefile{toc}{\contentsline {section}{\numberline {3.5}Introduction}{34}{section.3.5}}
\citation{Bengio2003lm}
\citation{collobert2008unified}
\citation{mikolov2013distributed}
\citation{Pennington2014}
\citation{mikolov2013distributed}
\citation{Hill2014EMNLP}
\citation{levy2014dependency}
\@writefile{toc}{\contentsline {section}{\numberline {3.6}Learning Embeddings with Neural Language Models}{35}{section.3.6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6.1}Monolingual Models}{35}{subsection.3.6.1}}
\citation{haghighi2008learning}
\citation{vulic2011identifying}
\citation{mikolov2013exploiting}
\citation{Hermann:2014:ICLR}
\citation{lauly2014autoencoder}
\citation{haghighi2008learning}
\citation{vulic2011identifying}
\citation{mikolov2013exploiting}
\citation{Klementiev}
\citation{Hermann:2014:ICLR}
\citation{lauly2014autoencoder}
\citation{Kocisky:2014}
\citation{Hermann:2014:ICLR}
\citation{Klementiev}
\citation{Hermann:2014:ICLR}
\citation{lauly2014autoencoder}
\citation{Hermann:2014:ICLR}
\citation{faruqui2014improving}
\citation{faruqui2014improving}
\citation{faruqui2014improving}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6.2}Bilingual Representation-learning Models}{36}{subsection.3.6.2}}
\citation{kalchbrenner13emnlp}
\citation{Sutskever2014sequence}
\citation{bahdanau2014neural}
\citation{kalchbrenner13emnlp}
\citation{Cho2014}
\citation{bahdanau2014neural}
\citation{Cho2014}
\citation{bahdanau2014neural}
\citation{kalchbrenner13emnlp}
\citation{devlin2014fast}
\citation{Sutskever2014sequence}
\citation{Cho2014}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6.3}Neural Machine Translation Models}{37}{subsection.3.6.3}}
\citation{Hermann:2014:ICLR}
\citation{faruqui2014improving}
\citation{mikolov2013distributed}
\citation{Pennington2014}
\citation{collobert2008unified}
\citation{Agirre2009}
\citation{bruni2014multimodal}
\citation{baroni2014don}
\citation{Agirre2009}
\citation{bruni2014multimodal}
\citation{hill2014simlex}
\@writefile{toc}{\contentsline {section}{\numberline {3.7}Experiments}{38}{section.3.7}}
\citation{hill2014simlex}
\citation{faruqui2014improving}
\citation{hill2014simlex}
\citation{nelson2004university}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.7.1}Similarity and relatedness modelling}{39}{subsection.3.7.1}}
\citation{landauer1997solution}
\@writefile{lot}{\contentsline {table}{\numberline {3.4}{\ignorespaces  NMT embeddings (RNNenc and RNNsearch) clearly outperform alternative embedding-learning architectures on tasks that require modelling similarity (below the dashed line), but not on tasks that reflect relatedness. Bilingual embedding spaces learned without the translation objective are somewhere between these two extremes.\relax }}{40}{table.caption.23}}
\newlabel{table:perf}{{3.4}{40}{NMT embeddings (RNNenc and RNNsearch) clearly outperform alternative embedding-learning architectures on tasks that require modelling similarity (below the dashed line), but not on tasks that reflect relatedness. Bilingual embedding spaces learned without the translation objective are somewhere between these two extremes.\relax \relax }{table.caption.23}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.5}{\ignorespaces Nearest neighbours (excluding plurals) in the embedding spaces of different models. All models were trained for 6 epochs on the translation corpus except CW and FD (as noted previously). NMT embedding spaces are oriented according to similarity, whereas embeddings learned by monolingual models are organized according to relatedness. The other bilingual model BiCVM also exhibits a notable focus on similarity.\relax }}{40}{table.caption.24}}
\newlabel{table:neigh}{{3.5}{40}{Nearest neighbours (excluding plurals) in the embedding spaces of different models. All models were trained for 6 epochs on the translation corpus except CW and FD (as noted previously). NMT embedding spaces are oriented according to similarity, whereas embeddings learned by monolingual models are organized according to relatedness. The other bilingual model BiCVM also exhibits a notable focus on similarity.\relax \relax }{table.caption.24}{}}
\citation{mikolov2013distributed}
\citation{mikolov2013distributed}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.7.2}Importance of training data quantity}{41}{subsection.3.7.2}}
\citation{mikolov2013distributed}
\citation{faruqui2014improving}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces The effect of increasing the amount of training data on the quality of monolingual embeddings, based on similarity-based evaluations (SimLex-999) and two relatedness-based evaluations (MEN and WordSim-353). \emph  {ET} in the legend indicates models trained on the English half of the translation corpus. \emph  {Wiki} indicates models trained on Wikipedia.\relax }}{42}{figure.caption.25}}
\newlabel{fig:size}{{3.5}{42}{The effect of increasing the amount of training data on the quality of monolingual embeddings, based on similarity-based evaluations (SimLex-999) and two relatedness-based evaluations (MEN and WordSim-353). \emph {ET} in the legend indicates models trained on the English half of the translation corpus. \emph {Wiki} indicates models trained on Wikipedia.\relax \relax }{figure.caption.25}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.7.3}Analogy Resolution}{42}{subsection.3.7.3}}
\citation{levy2014dependency}
\@writefile{toc}{\contentsline {section}{\numberline {3.8}Effect of Target Language}{43}{section.3.8}}
\newlabel{lang_effects}{{3.8}{43}{Effect of Target Language\relax }{section.3.8}{}}
\citation{morin2005hierarchical}
\citation{mikolov2013distributed}
\citation{Hermann:2014:ICLR}
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces Translation-based embeddings perform best on syntactic analogies (\emph  {run,ran: hide, hid}). Monolingual skipgram/Glove models are better at semantic analogies (\emph  {father, man; mother, woman})\relax }}{44}{figure.caption.26}}
\newlabel{fig:analogy}{{3.6}{44}{Translation-based embeddings perform best on syntactic analogies (\emph {run,ran: hide, hid}). Monolingual skipgram/Glove models are better at semantic analogies (\emph {father, man; mother, woman})\relax \relax }{figure.caption.26}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.6}{\ignorespaces Comparison of embeddings learned by RNN Search models translating between English-French (EN-FR) and English-German (EN-DE) on all semantic evaluations (left) and nearest neighbours of selected cue words (right). Bold italics indicate target-language-specific effects. Evaluation items and vocabulary searches were restricted to words common to both models. \relax }}{44}{table.caption.27}}
\newlabel{table:de}{{3.6}{44}{Comparison of embeddings learned by RNN Search models translating between English-French (EN-FR) and English-German (EN-DE) on all semantic evaluations (left) and nearest neighbours of selected cue words (right). Bold italics indicate target-language-specific effects. Evaluation items and vocabulary searches were restricted to words common to both models. \relax \relax }{table.caption.27}{}}
\citation{Jean}
\citation{Bengio+Senecal-2003-small}
\citation{luong2014addressing}
\@writefile{toc}{\contentsline {section}{\numberline {3.9}Overcoming the Vocabulary Size Problem}{45}{section.3.9}}
\@writefile{lot}{\contentsline {table}{\numberline {3.7}{\ignorespaces Comparison of embeddings learned by the original (RNN Search - 30k French words, 50k German words) and extended-vocabulary (RNN Search-LV -500k words) models translating from English to French (EN-FR) and from English to German (EN-DE). For fair comparisons, all evaluations were restricted to the intersection of all model vocabularies.\relax }}{46}{table.caption.28}}
\newlabel{table:ex}{{3.7}{46}{Comparison of embeddings learned by the original (RNN Search - 30k French words, 50k German words) and extended-vocabulary (RNN Search-LV -500k words) models translating from English to French (EN-FR) and from English to German (EN-DE). For fair comparisons, all evaluations were restricted to the intersection of all model vocabularies.\relax \relax }{table.caption.28}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.10}How Similarity Emerges}{46}{section.3.10}}
\newlabel{section:exp}{{3.10}{46}{How Similarity Emerges\relax }{section.3.10}{}}
\citation{faruqui2014improving}
\citation{hill2014simlex}
\citation{faruqui2014improving}
\citation{Hermann:2014:ICLR}
\citation{faruqui2014improving}
\citation{Kocisky:2014}
\citation{dist}
\@writefile{toc}{\contentsline {section}{\numberline {3.11}Conclusion}{48}{section.3.11}}
\citation{mikolov2013distributed}
\citation{zock2004word}
\citation{307754}
\citation{Klementiev}
\citation{hermann2013multilingual}
\citation{gouws2014bilbowa}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Learning to Represent Phrases}{51}{chapter.4}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Introduction}{51}{section.4.1}}
\citation{mikolov2010recurrent}
\citation{kiros2014unifying}
\citation{bahdanau2014neural}
\citation{mikolov2013distributed}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Neural Language Model Architectures}{52}{section.4.2}}
\citation{bengio1994learning}
\citation{hochreiter1997long}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}Long Short Term Memory}{53}{subsection.4.2.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}Bag-of-Words NLMs}{54}{subsection.4.2.2}}
\citation{huang2012improving}
\citation{bordes2015large}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.3}Pre-trained Input Representations}{55}{subsection.4.2.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.4}Training Objective}{55}{subsection.4.2.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.5}Implementation Details}{55}{subsection.4.2.5}}
\citation{faruqui2014retrofitting}
\citation{cho2014learning}
\citation{bergstra+al:2010-scipy}
\citation{zeiler2012adadelta}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Reverse Dictionaries}{56}{section.4.3}}
\citation{bilac2003improving}
\citation{bilac2004dictionary}
\citation{zock2004word}
\citation{shaw2013building}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.1}Data Collection and Training}{57}{subsection.4.3.1}}
\citation{mitchell2010composition}
\@writefile{lot}{\contentsline {table}{\numberline {4.1}{\ignorespaces Performance of different reverse dictionary models in different evaluation settings. *Low variance in \emph  {mult} models is due to consistently poor scores, so not highlighted.\relax }}{58}{table.caption.29}}
\newlabel{results}{{4.1}{58}{Performance of different reverse dictionary models in different evaluation settings. *Low variance in \emph {mult} models is due to consistently poor scores, so not highlighted.\relax \relax }{table.caption.29}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.2}Comparisons}{58}{subsection.4.3.2}}
\citation{shaw2013building}
\citation{leech1994claws4}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.3}Reverse Dictionary Evaluation}{59}{subsection.4.3.3}}
\@writefile{lot}{\contentsline {table}{\numberline {4.2}{\ignorespaces Style difference between \emph  {dictionary definitions} and \emph  {concept descriptions} in the evaluation.\relax }}{60}{table.caption.30}}
\newlabel{tb:tablename}{{4.2}{60}{Style difference between \emph {dictionary definitions} and \emph {concept descriptions} in the evaluation.\relax \relax }{table.caption.30}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.4}Results}{60}{subsection.4.3.4}}
\citation{iyyer2015deep}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.5}Qualitative Analysis}{61}{subsection.4.3.5}}
\@writefile{lot}{\contentsline {table}{\numberline {4.3}{\ignorespaces The top-five candidates for example queries (invented by the authors) from different reverse dictionary models. Both the RNN and BOW models are without Word2Vec input and use the cosine loss.\relax }}{62}{table.caption.31}}
\newlabel{qual}{{4.3}{62}{The top-five candidates for example queries (invented by the authors) from different reverse dictionary models. Both the RNN and BOW models are without Word2Vec input and use the cosine loss.\relax \relax }{table.caption.31}{}}
\citation{hermann2013multilingual}
\citation{lauly2014autoencoder}
\citation{gouws2014bilbowa}
\citation{gouws2014bilbowa}
\@writefile{lot}{\contentsline {table}{\numberline {4.4}{\ignorespaces Responses from cross-lingual reverse dictionary models to selected queries. Underlined responses are `correct' or potentially useful for a native French speaker.\relax }}{63}{table.caption.32}}
\newlabel{cross}{{4.4}{63}{Responses from cross-lingual reverse dictionary models to selected queries. Underlined responses are `correct' or potentially useful for a native French speaker.\relax \relax }{table.caption.32}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.6}Cross-Lingual Reverse Dictionaries}{63}{subsection.4.3.6}}
\citation{ferrucci2010building}
\citation{molla2007question}
\citation{Iyyer:Boyd-Graber:Claudino:Socher:Daume-2014}
\citation{weston2015towards}
\citation{berant14paraphrasing}
\citation{bordes2014question}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.7}Discussion}{64}{subsection.4.3.7}}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}General Knowledge (crossword) Question Answering}{64}{section.4.4}}
\citation{littman2002probabilistic}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.1}Evaluation}{65}{subsection.4.4.1}}
\citation{littman2002probabilistic}
\citation{ginsberg2011dr}
\@writefile{lot}{\contentsline {table}{\numberline {4.5}{\ignorespaces Examples of the different question types in the crossword question evaluation dataset.\relax }}{66}{table.caption.33}}
\newlabel{tb:tablename}{{4.5}{66}{Examples of the different question types in the crossword question evaluation dataset.\relax \relax }{table.caption.33}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.6}{\ignorespaces Performance of different models on crossword questions of different length. The two commercial systems are evaluated via their web interface so only accuracy@10 can be reported in those cases. \relax }}{66}{table.caption.34}}
\newlabel{results2}{{4.6}{66}{Performance of different models on crossword questions of different length. The two commercial systems are evaluated via their web interface so only accuracy@10 can be reported in those cases. \relax \relax }{table.caption.34}{}}
\newlabel{tb:tablename}{{4.6}{66}{Performance of different models on crossword questions of different length. The two commercial systems are evaluated via their web interface so only accuracy@10 can be reported in those cases. \relax \relax }{table.caption.34}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.2}Benchmarks and Comparisons}{66}{subsection.4.4.2}}
\citation{littman2002probabilistic}
\citation{ginsberg2011dr}
\citation{mikolov2013distributed}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.3}Results}{67}{subsection.4.4.3}}
\@writefile{lot}{\contentsline {table}{\numberline {4.7}{\ignorespaces Responses from different models to example crossword clues. In each case the model output is filtered to exclude any candidates that are not of the same length as the correct answer. BOW and RNN models are trained without Word2Vec input embeddings and cosine loss.\relax }}{68}{table.caption.35}}
\newlabel{egs}{{4.7}{68}{Responses from different models to example crossword clues. In each case the model output is filtered to exclude any candidates that are not of the same length as the correct answer. BOW and RNN models are trained without Word2Vec input embeddings and cosine loss.\relax \relax }{table.caption.35}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.4}Qualitative Analysis}{68}{subsection.4.4.4}}
\citation{bordes2014question}
\citation{graves2014neural}
\citation{weston2015towards}
\@writefile{toc}{\contentsline {section}{\numberline {4.5}Conclusion}{69}{section.4.5}}
\citation{graves2014neural}
\citation{weston2015towards}
\citation{baroni2014don}
\citation{levy2014neural}
\citation{hill2014simlex}
\citation{collobert2011natural}
\citation{mitchell2008vector}
\citation{clark2007combining}
\citation{baroni2014frege}
\citation{milajevs2014evaluating}
\citation{Sutskever2014sequence}
\citation{mao2014deep}
\citation{serban2015building}
\citation{cho2014learning}
\citation{norman1972memory}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Learning to Represent Sentences}{71}{chapter.5}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.0.1}Introduction}{71}{subsection.5.0.1}}
\citation{kiros2015skip}
\citation{hill2015learning}
\citation{socher2011semi}
\citation{kalchbrenner2014convolutional}
\citation{milajevs2014evaluating}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.0.2}Distributed Sentence Representations}{72}{subsection.5.0.2}}
\citation{kiros2015skip}
\citation{cho2014learning}
\citation{le2014distributed}
\citation{mikolov2013distributed}
\citation{mitchell2010composition}
\citation{marcobaronijointly}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.0.3}Existing Models Trained on Text}{73}{subsection.5.0.3}}
\citation{hill2015learning}
\citation{chen2015microsoft}
\citation{szegedy2014going}
\citation{russakovsky2014imagenet}
\citation{cho2014learning}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.0.4}Models Trained on Structured Resources}{74}{subsection.5.0.4}}
\citation{vincent2008extracting}
\citation{vincent2010stacked}
\citation{dai2015semi}
\citation{iyyer2015deep}
\citation{sutskever2011generating}
\citation{harris1954distributional}
\citation{polajnar2015exploration}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.0.5}Novel Text-Based Models}{75}{subsection.5.0.5}}
\newlabel{eqn1}{{5.1}{76}{Novel Text-Based Models\relax }{equation.5.0.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.0.6}Training and Model Selection}{76}{subsection.5.0.6}}
\citation{collobert2011natural}
\citation{mikolov2013efficient}
\citation{kiros2015skip}
\citation{hill2015learning}
\citation{baroni2014don}
\citation{levy2015improving}
\@writefile{lot}{\contentsline {table}{\numberline {5.1}{\ignorespaces  {\bf  Properties of models compared in this study} \hspace  {6mm} {\bf  OS:} requires training corpus of sentences in order. {\bf  R:} requires structured resource for training. {\bf  WO:} encoder sensitive to word order. {\bf  SD:} dimension of sentence representation. {\bf  WD:} dimension of word representation. {\bf  TR:} approximate training time (hours) on the dataset in this paper. * indicates trained on GPU. {\bf  TE:} approximate time (s) taken to encode 0.5m sentences.\relax }}{77}{table.caption.36}}
\newlabel{modelprops}{{5.1}{77}{{\bf Properties of models compared in this study} \hspace {6mm} {\bf OS:} requires training corpus of sentences in order. {\bf R:} requires structured resource for training. {\bf WO:} encoder sensitive to word order. {\bf SD:} dimension of sentence representation. {\bf WD:} dimension of word representation. {\bf TR:} approximate training time (hours) on the dataset in this paper. * indicates trained on GPU. {\bf TE:} approximate time (s) taken to encode 0.5m sentences.\relax \relax }{table.caption.36}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.2}{\ignorespaces  Example sentence pairs and `similarity' ratings from the unsupervised evaluations used in this study.\relax }}{77}{table.caption.37}}
\newlabel{unsex}{{5.2}{77}{Example sentence pairs and `similarity' ratings from the unsupervised evaluations used in this study.\relax \relax }{table.caption.37}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.0.7}Evaluating Sentence Representations}{77}{subsection.5.0.7}}
\citation{dolan2004unsupervised}
\citation{pang2005seeing}
\citation{hu2004mining}
\citation{pang2004sentimental}
\citation{wiebe2005annotating}
\citation{voorhees2002overview}
\citation{kiros2015skip}
\citation{marelli2014sick}
\citation{agirre2014semeval}
\@writefile{lot}{\contentsline {table}{\numberline {5.3}{\ignorespaces  Performance of sentence representation models on {\bf  supervised} evaluations (Section\nobreakspace  {}\ref  {supersec}). Bold numbers indicate best performance in class. Underlined indicates best overall. \relax }}{78}{table.caption.38}}
\newlabel{supervised}{{5.3}{78}{Performance of sentence representation models on {\bf supervised} evaluations (Section~\ref {supersec}). Bold numbers indicate best performance in class. Underlined indicates best overall. \relax \relax }{table.caption.38}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.0.8}Supervised Evaluations}{78}{subsection.5.0.8}}
\newlabel{supersec}{{5.0.8}{78}{Supervised Evaluations\relax }{subsection.5.0.8}{}}
\citation{ji2013discriminative}
\citation{vincent2008extracting}
\@writefile{lot}{\contentsline {table}{\numberline {5.4}{\ignorespaces  Performance of sentence representation models (Spearman/Pearson correlations) on {\bf  unsupervised} (relatedness) evaluations (Section\nobreakspace  {}\ref  {unseval}). Models are grouped according to training data as indicated in Table\nobreakspace  {}\ref  {supervised}.\relax }}{79}{table.caption.39}}
\newlabel{unsupervised}{{5.4}{79}{Performance of sentence representation models (Spearman/Pearson correlations) on {\bf unsupervised} (relatedness) evaluations (Section~\ref {unseval}). Models are grouped according to training data as indicated in Table~\ref {supervised}.\relax \relax }{table.caption.39}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.0.9}Unsupervised Evaluations}{79}{subsection.5.0.9}}
\newlabel{unseval}{{5.0.9}{79}{Unsupervised Evaluations\relax }{subsection.5.0.9}{}}
\citation{marcobaronijointly}
\citation{voorhees2002overview}
\citation{bruni2014multimodal}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.0.10}Results}{80}{subsection.5.0.10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.0.11}Discussion}{80}{subsection.5.0.11}}
\citation{almahairi2015learning}
\@writefile{lot}{\contentsline {table}{\numberline {5.5}{\ignorespaces Sample nearest neighbour queries selected from a randomly sampled 0.5m sentences of the Toronto Books Corpus.\relax }}{82}{table.caption.40}}
\newlabel{neighbours}{{5.5}{82}{Sample nearest neighbour queries selected from a randomly sampled 0.5m sentences of the Toronto Books Corpus.\relax \relax }{table.caption.40}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.6}{\ignorespaces  Internal consistency (Chronbach's \relax $\alpha \relax \GenericError  {               }{LaTeX Error: Bad math environment delimiter}{See the LaTeX manual or LaTeX Companion for explanation.}{Your command was ignored.\MessageBreak Type  I <command> <return>  to replace it with another command,\MessageBreak or  <return>  to continue without it.}) among evaluations when individual benchmarks are left out of the (supervised or unsupervised) cohorts. Consistency rank within cohort is in parentheses (1 = most consistent with other evaluations).\relax }}{82}{table.caption.41}}
\newlabel{consistency}{{5.6}{82}{Internal consistency (Chronbach's \(\alpha \)) among evaluations when individual benchmarks are left out of the (supervised or unsupervised) cohorts. Consistency rank within cohort is in parentheses (1 = most consistent with other evaluations).\relax \relax }{table.caption.41}{}}
\citation{wiebe2005annotating}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.0.12}Conclusion}{83}{subsection.5.0.12}}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Representation Learning}{85}{chapter.6}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {chapter}{\numberline {7}Conclusion}{87}{chapter.7}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\bibstyle{plain}
\bibdata{thesis}
\bibcite{Agirre2009}{1}
\bibcite{agirre2014semeval}{2}
\bibcite{almahairi2015learning}{3}
\bibcite{andrews2009integrating}{4}
\bibcite{bahdanau2014neural}{5}
\bibcite{baroni2014frege}{6}
\bibcite{baroni2014don}{7}
\@writefile{toc}{\contentsline {chapter}{Bibliography}{89}{section*.42}}
\bibcite{barsalou2010grounded}{8}
\bibcite{barsalou2005situating}{9}
\bibcite{Bengio2003lm}{10}
\bibcite{Bengio+Senecal-2003-small}{11}
\bibcite{bengio1994learning}{12}
\bibcite{berant14paraphrasing}{13}
\bibcite{bergstra+al:2010-scipy}{14}
\bibcite{bilac2003improving}{15}
\bibcite{bilac2004dictionary}{16}
\bibcite{bordes2014question}{17}
\bibcite{bordes2015large}{18}
\bibcite{bruni2012distributional}{19}
\bibcite{bruni2014multimodal}{20}
\bibcite{bybee2001frequency}{21}
\bibcite{lauly2014autoencoder}{22}
\bibcite{chater2006probabilistic}{23}
\bibcite{chen2015microsoft}{24}
\bibcite{cho2014learning}{25}
\bibcite{Cho2014}{26}
\bibcite{clark2007combining}{27}
\bibcite{collobert2008unified}{28}
\bibcite{collobert2011natural}{29}
\bibcite{crutch2005abstract}{30}
\bibcite{dai2015semi}{31}
\bibcite{devereux2013centre}{32}
\bibcite{devlin2014fast}{33}
\bibcite{dolan2004unsupervised}{34}
\bibcite{duchi2011adaptive}{35}
\bibcite{faruqui2014retrofitting}{36}
\bibcite{faruqui2014improving}{37}
\bibcite{feng2010visual}{38}
\bibcite{ferrucci2010building}{39}
\bibcite{dist}{40}
\bibcite{fodor1983modularity}{41}
\bibcite{ginsberg2011dr}{42}
\bibcite{gouws2014bilbowa}{43}
\bibcite{grafton2009embodied}{44}
\bibcite{graves2014neural}{45}
\bibcite{haghighi2008learning}{46}
\bibcite{hardoon2004canonical}{47}
\bibcite{harris1954distributional}{48}
\bibcite{hermann2013multilingual}{49}
\bibcite{Hermann:2014:ICLR}{50}
\bibcite{hill2015learning}{51}
\bibcite{Hill2014EMNLP}{52}
\bibcite{hill2013quantitative}{53}
\bibcite{hill2014simlex}{54}
\bibcite{hochreiter1997long}{55}
\bibcite{hu2004mining}{56}
\bibcite{huang2012improving}{57}
\bibcite{Iyyer:Boyd-Graber:Claudino:Socher:Daume-2014}{58}
\bibcite{iyyer2015deep}{59}
\bibcite{Jean}{60}
\bibcite{ji2013discriminative}{61}
\bibcite{johns2012perceptual}{62}
\bibcite{kalchbrenner13emnlp}{63}
\bibcite{kalchbrenner2014convolutional}{64}
\bibcite{kiros2014unifying}{65}
\bibcite{kiros2015skip}{66}
\bibcite{Klementiev}{67}
\bibcite{Kocisky:2014}{68}
\bibcite{landauer1997solution}{69}
\bibcite{le2014distributed}{70}
\bibcite{leech1994claws4}{71}
\bibcite{levy2014dependency}{72}
\bibcite{levy2014neural}{73}
\bibcite{levy2015improving}{74}
\bibcite{littman2002probabilistic}{75}
\bibcite{luong2014addressing}{76}
\bibcite{mao2014deep}{77}
\bibcite{marelli2014sick}{78}
\bibcite{mcrae2005semantic}{79}
\bibcite{mesnil2012unsupervised}{80}
\bibcite{mikolov2013efficient}{81}
\bibcite{mikolov2010recurrent}{82}
\bibcite{mikolov2013exploiting}{83}
\bibcite{mikolov2013distributed}{84}
\bibcite{milajevs2014evaluating}{85}
\bibcite{mitchell2008vector}{86}
\bibcite{mitchell2010composition}{87}
\bibcite{mnih2009scalable}{88}
\bibcite{molla2007question}{89}
\bibcite{morin2005hierarchical}{90}
\bibcite{myers1990classical}{91}
\bibcite{nelson2004university}{92}
\bibcite{norman1972memory}{93}
\bibcite{paivio1991dual}{94}
\bibcite{pang2004sentimental}{95}
\bibcite{pang2005seeing}{96}
\bibcite{Pennington2014}{97}
\bibcite{marcobaronijointly}{98}
\bibcite{polajnar2015exploration}{99}
\bibcite{rollermultimodal}{100}
\bibcite{russakovsky2014imagenet}{101}
\bibcite{serban2015building}{102}
\bibcite{shaw2013building}{103}
\bibcite{silberer2014learning}{104}
\bibcite{silberer2013models}{105}
\bibcite{silberer2012grounded}{106}
\bibcite{socher2011semi}{107}
\bibcite{srivastava2012multimodal}{108}
\bibcite{sutskever2011generating}{109}
\bibcite{Sutskever2014sequence}{110}
\bibcite{szegedy2014going}{111}
\bibcite{turian2010word}{112}
\bibcite{turney2010frequency}{113}
\bibcite{vincent2008extracting}{114}
\bibcite{vincent2010stacked}{115}
\bibcite{von2004labeling}{116}
\bibcite{voorhees2002overview}{117}
\bibcite{vulic2011identifying}{118}
\bibcite{307754}{119}
\bibcite{weston2015towards}{120}
\bibcite{wiebe2005annotating}{121}
\bibcite{katja2005content}{122}
\bibcite{wu2013online}{123}
\bibcite{zeiler2012adadelta}{124}
\bibcite{zock2004word}{125}
\@writefile{toc}{\contentsline {chapter}{\numberline {A}Extra Information}{103}{appendix.A}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
