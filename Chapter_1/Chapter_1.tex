% This chapter is the Introduction to the Thesis

The question of how children come to be competent language users is vigorously debated by researchers across the human sciences. However, there is a clear consensus that children do not learn language because they are taught it.\footnote{See e.g. \scriptsize{\url{http://www.linguisticsociety.org/resource/faq-how-do-we-learn-language}}} While instances of caregivers or teachers instructing children how to pronounce or use words are not uncommon, there is little or no evidence that such interventions are necessary for fluent language use to emerge~\citep{tomasello2009constructing,pinker2009language}. On the other hand, two aspects of the child's environment do appear critical for language to flourish. The first of these is the massive amounts of correct language to which the developing brain is exposed. The second is the fact that many of the things that humans naturally desire or aspire to require linguistic interaction with other humans. This latter fact explains the motivation that may be necessary for learning, while the external information or signal required is a consequence of both. 

This thesis is about computational models that learn to represent or understand language under similar conditions to a child listening to adults speak. That is, by making sense of everything that is observed, rather than by asking for additional information when it is uncertain, or receiving explicit corrections or explanations. This sort of learning is often referred to as \emph{unsupervised}. The models in this thesis mimic human language acquisition in that they learn passively from massive amounts of data, and allow linguistic concepts to emerge rather than being instructed according to some oracle or theory. On the other hand, they are a less faithful reflection of the post-verbal interactive stage of human language learning, in which ongoing passive learning is combined with goal-driven linguistic interaction. Realistically modelling this combination poses many practical challenges, and research in this direction is left for immediate future work. 

\section{Distributed representations of knowledge} The way in which the models considered in this thesis effect unsupervised learning is by acquiring \emph{distributed representations} of linguistic units (words, phrases, sentences and eventually short passages)~\citep{hinton1986learning}. Given a set \(S\) of \(s\) semantic (in this case, linguistic) concepts, we can encode each concept according to how strongly it activates each of a set of \(n\) potentially independent features. If each feature takes \(k\) possible values, we can unambiguously encode a maximum of \(k^n\) concepts in such a representation space. This contrasts with a situation in which the presence of each concept is represented by the activation of one of \(s\) discrete features. Such an encoding strategy is typically referred to as a \emph{symbolic} or \emph{localist representation}. The advantage of a symbolic approach is that, for a given view of the world (and thus a given set features and their activations) we can know immediately and unambiguously which concepts are present. On the other hand, in a system capable of representing \(n\) such features, a symbolic approach can only deal with \(n\) distinct concepts as possible inputs, whereas a system based on distributed representations could, in theory, reason about \(k^n\) concepts. 

There are additional reasons why systems built on distributed representations are popular when aiming to replicate core human capabilities such as object recognition or language understanding. First, distributed models naturally generalise because of shared attributes. While we may not be able to associate words or easily-delimited meaning with individual features in a distributed representation, we can observe that representations of related or similar concepts such as \(dog\) and \(wolf\) share common patterns of activation. A model that is trained exclusively on images of dogs (not wolves) eating meat may learn that concepts with a dog-like pattern of activation are carnivorous, from which it might infer that wolves are also carnivorous. Such generalisation is not possible using purely symbolic models, which do not naturally induce a similarity space on the concepts they encounter. It is, on the other hand, highly characteristic of human reasoning and cognition~\citep{rosch1976basic}. 

Second, a distributed model can acquire new concepts (beyond the original set \(S\)) without requiring additional memory, since concepts can `emerge' as (relatively) stable patterns of activation across its feature set. In the present example, the concept of \emph{carnivore} might be acquired if the model observes sufficient examples of animals eating meat (and different animals not eating meat). Over time, \emph{carnivore} would come to behave just like any other concept in the memory of the model (i.e. associated with a stable pattern of feature activations). This is not the case with symbolic models. While the attribute \emph{carnivore} could be associated with a subset of concepts in a symbolic model, it would not have the status of concept in itself unless the model added to its memory (and feature space). The existence of emergent concepts, of varying degree of stability or salience, is also a commonly-observed trait of human cognition~\citep{patterson2007you}. 

Third, like human semantic memory, knowledge encoded in distributed representations is content-addressable. If humans see a partially obscured object or hear an incomplete word, they may be able to infer the correct entity quickly based on this information. Similarly, if a distributed model receives partial information about a concept (in the form of an incomplete activation pattern), it may be able to access the full concept using this information (and indeed, may be able to learn to organise its memory such that its ability to perform such reconstructions improves). By contrast, in a symbolic model it is not generally possible to search for knowledge based on aspects of the information in question. Additional information about its location in memory is normally required. 

Finally, if we encode concepts via symbols, or even pre-specified semantic features (sub-concepts), we introduce a large degree of bias or ad hocness into our representations~\citep{miikkulainen1991natural}. The world is a large, chaotic continuum, and we have very little idea of how the brain rationalises the enormous stream of input data it receives. By training and then interrogating models that acquire representations of this data that are optimal for achieving realistic linguistic objectives, we can learn in a comparatively unbiased way which aspects of the data are important for realising the particular objective. This in turn can provides at least circumstantial evidence that the human brain exploits the same characteristics of the signal in order to achieve its learning goals.     

These natural effects of computing with distributed representations are particularly appropriate for modelling linguistic cognition. Semantic categories such as the referents of a word are notoriously hard to delimit in terms of defining properties~\citep{fauconnier1994mental}. English speakers can easily use and understand a word like \emph{chair}, and a seat with four legs and a back would typically be classed as a chair, but these properties are neither necessary nor sufficient for classifying an object as such. A longer seat with four legs and a back is not really a chair, but rather a \emph{bench}. Similarly, an armchair does not typically have four legs, and we might call a tree stump a chair if people sat on it regularly enough~\citep{prinz2004furnishing}. 

Western linguistic theory and education often involves more abstract categories referring to language itself, such as \emph{noun} or \emph{subject}, but even these can prove frustratingly intangible upon thorough consideration. Nouns are often said to refer to entities or objects, but \emph{failure} or \emph{unravelling} do not fall into either category. The type of word considered to be an \emph{verb} or an \emph{adjective} can be very different to speakers of different languages~\citep{anward1997parts}. How can we realise effective communication with others using and referring to concepts if we ourselves do not know how to unambiguously define or describe them? The solution advocated in this thesis is that we do so in the same way as a computational models that perform computations on distributed representations. This hypothesis also seems to be consistent with our (limited) knowledge the biological hardware of the brain~\citep{kiefer2012conceptual}.
 
\section{Deep learning} The terms \emph{artificial neural networks}, \emph{connectionism} and, now, \emph{deep learning} refer to a family of computational models that store knowledge in distributed representations and perform computations on these representations. In these approaches, distributed representations of the concepts observed in data are learned as part of the same process as learning how to interpret or manipulate these representations in some optimal way (e.g. to perform accurate prediction or classification based on the input). In many deep learning models, `manipulating' representations corresponds to computing additional layers of representation of the same input (via continuous but non-linear mathematical operations). In theory, these representations then encode increasingly abstract or sophisticated characteristics of the input concepts. For example, in a deep convolutional network trained to classify images of objects into (semantic) object categories, features in the initial representations correspond to crude visual properties such as the edges of objects, whereas higher-level representations (which are the ones on which serve as input to the output classifier) seem to encode properties that can be much more directly connected to the model's objective (such as the presence of eyes)~\citep{zeiler2014visualizing}. 

Every deep learning model acquires (at least one layer of) distributed representations for each concept that it observes. Unlike representations or encodings defined manually, the representations learned for a particular model are likely to be optimal or close to optimal with respect to objective of the model (given the constraints of its architecture). On the other hand, post-hoc analysis may be required to understand the representational strategy learned by the model, particularly since features in the model do not generally correspond to concepts that humans can label with words.        

\subsection{Neural language models} Deep learning models whose objective corresponds to linguistic tasks, such as predicting missing words, answering questions or classifying the sentiment of statements, are referred to as \emph{neural language models} (NLMs)~\citep{bengio2003neural}. The first NLM built on the scale of today's deep learning models was developed for the task of \emph{language modelling}; estimating the probability of a word sequence, or the related objective of predicting the next word in a text.\footnote{This should not discredit the various smaller-scale connectionist models of written language proposed before then (See e.g. ~\citealt{elman1990finding,miikkulainen1991natural})} In recent years, neural language models have begun to achieve state-of-the-art performance on a range of language processing tasks, including summarisation~\citep{rush2015neural}, visual question answering~\citep{antol2015vqa}, machine comprehension~\citep{hill2015goldilocks}, factoid question answering~\citep{bordes2014question}, entailment detection~\citep{rocktaschel2015reasoning} and machine translation~\citep{bahdanau2014neural}. In all of these cases, NLMs achieve at or close to state-of-the-art performance on the established benchmarks.  

Beyond this clear empirical case for focusing on NLMs (which was much less compelling at the beginning of my doctoral research) there are important scientific reasons for addressing language processing problems within the connectionist paradigm. Symbolic approaches that are able to learn and generalise about language typically rely on a pre-specified categorisation of linguistic input (for instance into classes like \emph{noun}, \emph{subject} or \emph{adjunct}). Such syntactic or semantic categories are not instantiated in the physical world, and in many cases there can be significant disagreement among language users regarding category membership~\citep{anward1997parts}. NLMs avoid this minefield by inducing such categories as (soft) clusterings of concepts in the similarity spaces of their distributed representations, inferring principles about these categories as computations on the spaces. By working with NLMs, we allow a (relatively) unbiased model to teach us more about the data, rather than relying on a human view of the data - and its inevitable social and cultural biases - to inform the design of the model. 

\section{Unsupervised learning} 

The term \emph{supervised learning} typically refers to a setting for a computational or statistical inference problem in which a model is presented with data consisting of a set of inputs and corresponding outputs. Based on these training data, the model learns to adjust its internal computation so that it can predict outputs from inputs as accurately as possible. Models trained in this way are typically evaluated on their ability to generalise by testing their accuracy on a subset of input and output pairs that are deliberately excluded from the training data. 

In the vast majority cases to date, NLMs have been trained and applied in this supervised way. This thesis, however, focuses on applications of NLMs that do not align with this paradigm. That said, the designation of an approach as \emph{unsupervised} is not unambiguous, The aforementioned case of language modelling being a notable case in point. Language models are supervised in that they learn by considering some words in a text, making a prediction about neighbouring words and then adjusting their internal computations based on what they observe the actual neighbouring words to be in the text. Moreover, as with other supervised methods, they are evaluated by measuring the performance of the model on similar text that was deliberately excluded from the training data. Nevertheless, they can also be considered unsupervised because the training data (text) is not explicitly divided into inputs and outputs (words play the role of both), and a correct label is not explicitly associated (e.g. by some annotation process) with the correct answer for a given input. A child listening to two adults converse could presumably implement his or her own language model-style learning (by making and correcting predictions about future words), but we would be unlikely to say that the child's learning is being supervised by the adults in this case.

Beyond language modelling, methods in NLP that are normally considered unsupervised include semantic models that build representations from the statistics of word occurrences in raw text (such as LSA, LDA and related approaches~\citep{landauer1997solution,blei2012probabilistic}) or clustering approaches to problems like word sense induction~\citep{agirre2007semeval} and lexical category acquisition~\citep{sun2009improving}. Two characteristics stand out as distinguishing these methods from others more often described as supervised. First, (and like language modelling) in these methods the training data (usually raw text) would exist without the intervention of NLP researchers and/or annotators. Second, unlike the canonical language modelling setting, these methods are not evaluated on a task that mirrors the objective guiding their learning. 

In this thesis, I therefore consider a method or approach to be unsupervised if {\bf it is applied to tasks that differ from the training objective \emph{and} does not require training data produced with machine learning in mind}. From a practical perspective, the first of these criteria is desirable, in theory at least, because it indicates that the model has learned something that may ultimately be useful for more than one particular tasks or application. The advantage of the second is less esoteric; it simply implies negligible cost for generating training data and a greater likelihood that such data exists for various domains and/or languages. Longer term (and more speculatively) the benefit of unsupervised learning may lie in the fact that it appears to be an important part of human learning and development. Human behaviour is not just characterised by strong performance on tasks that are well-practiced, but rather on the ability to adapt and apply knowledge acquired in a diverse range of settings to tasks that may resemble, but are not necessarily identical to, tasks that the human has attempted previously.  

This characterisation of unsupervised methods does not preclude the involvement of humans whatsoever in the production of training data. In so much as natural languages are by definition a product of human activity, the training data for any language application will likely involve humans. Neither does it preclude learning from knowledge of the right answer. Indeed, most neural networks learn by backpropagating an error associated (via a mathematical cost function) with the `correctness' of their predictions. 

\section*{Structure of the thesis}

Chapter~\ref{CH2} concerns what is to-date the most popular and effective unsupervised application of NLMs, namely the acquisition of distributed word representations or \emph{embeddings}. The principal contribution is a novel resource, SimLex-999, that enables more principled evaluation and analysis of these representations. In Chapter~\ref{CH3}, I present a novel method for learning word representations with NLMs, one that exploits a new sequence-to-sequence learning architecture and parallel bilingual corpora, and show that these representations exhibit potentially valuable properties that are not observed when learning via previous methods. In Chapter~\ref{CH4}, I propose an objective for learning distributed representations of phrases with NLMs that exploits the structured textual information in dictionaries and encyclopedias. I show that the encoding of the information in dictionaries into a distributed semantic memory enables models to generalise and transfer their lexical knowledge to crossword question-answering. In Chapter~\ref{CH5}, I place these observations in wider perspective by systematically comparing various ways of learning encode phrases and sentences in a distributed memory. This analysis includes two novel methods for acquiring such distributed representations, each of which exhibits certain advantages over existing approaches. Finally, in Chapter~\ref{CH6}, I consider a new class of NLMs, \emph{memory networks}, which couple the process of encoding phrases or sentences in a semantic memory with a particular language prediction task. By allowing us to measure the effect of particular representational strategies on an extrinsic prediction task, this end-to-end architecture enables a more functionally-grounded undertanding of the most useful strategies for encoding linguistic content in distributed memories. 

