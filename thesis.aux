\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\providecommand\@newglossary[4]{}
\@newglossary{main}{glg}{gls}{glo}
\providecommand\@glsorder[1]{}
\providecommand\@istfilename[1]{}
\@istfilename{thesis.ist}
\@glsorder{word}
\@writefile{toc}{\thispagestyle {empty}}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{13}{chapter.1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\citation{dist}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Understanding and evaluating unsupervised models of word representation}{15}{chapter.2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\citation{gentner1978relational}
\citation{hill2013quantitative}
\citation{huang2012improving}
\citation{collobert2008unified}
\citation{mikolov2013efficient}
\citation{turney2010frequency}
\citation{landauer1997solution}
\citation{agirre2009study}
\citation{levy2014dependency}
\citation{agirre2009study}
\citation{kiela2014systematic}
\citation{finkelstein2001placing}
\citation{bruni2012distributional}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.0.1}Comparing word representations to human conceptual space}{17}{subsection.2.0.1}}
\citation{budanitsky2006evaluating}
\citation{agirre2009study}
\citation{hatzivassiloglou2001simfinder}
\citation{turney2012domain}
\citation{tversky1977features}
\citation{cimiano2005learning}
\citation{biemann2005ontology}
\citation{li2006exploring}
\citation{he2008indirect}
\citation{marton2009improved}
\citation{collobert2008unified}
\citation{baroni2010distributional}
\citation{huang2012improving}
\citation{reisinger2010multi}
\citation{medelyan2009mining}
\citation{li2014obtaining}
\citation{rubenstein1965contextual}
\citation{agirre2009study}
\citation{yong1999case}
\citation{cunningham2005information}
\citation{resnik201011}
\citation{plaut1995semantic}
\citation{mcrae2012semantic}
\citation{mcrae2012semantic}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.0.2}Design motivation}{19}{subsection.2.0.2}}
\newlabel{motivation}{{2.0.2}{19}{Design motivation}{subsection.2.0.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.0.2.1}Similarity and Association}{19}{subsubsection.2.0.2.1}}
\citation{fellbaum1999wordnet}
\citation{wu1994verbs}
\citation{wu1994verbs}
\citation{nelson2004university}
\citation{he2008indirect}
\citation{Haghighi2008Learning}
\citation{marton2009improved}
\citation{beltagysemantic}
\citation{navigli2009word}
\citation{phan2008learning}
\citation{rose2002reuters}
\@writefile{toc}{\contentsline {paragraph}{Association and similarity in NLP}{20}{section*.6}}
\citation{huang2012improving}
\citation{reisinger2010multi}
\citation{luong2013better}
\citation{budanitsky2006evaluating}
\citation{turney2012domain}
\citation{agirre2009study}
\citation{agirre2009study}
\@writefile{lot}{\contentsline {table}{\numberline {2.1}{\ignorespaces  Top: Concept pairs with the lowest WupSim scores in the USF dataset overall. Bottom: Pairs with the largest discrepancy in rank between association strength (high) and WupSim (low).\relax }}{21}{table.caption.5}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{tab1}{{2.1}{21}{Top: Concept pairs with the lowest WupSim scores in the USF dataset overall. Bottom: Pairs with the largest discrepancy in rank between association strength (high) and WupSim (low).\relax }{table.caption.5}{}}
\citation{andrews2009integrating}
\citation{kiela2014systematic}
\citation{levy2014dependency}
\citation{agirre2009study}
\citation{levy2014dependency}
\citation{agirre2009study}
\citation{kiela2014systematic}
\citation{gentner2006verbs}
\citation{markman1997similar}
\citation{hill2014multi}
\citation{paivio1991dual}
\citation{hill2013quantitative}
\citation{hill2013concreteness}
\citation{kielaimproving}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.0.2.2}Concepts, part-of-speech and concreteness}{22}{subsubsection.2.0.2.2}}
\citation{finkelstein2001placing}
\citation{huang2012improving}
\citation{bansal2014tailoring}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.0.3}Existing gold standards and evaluation resources}{23}{subsection.2.0.3}}
\newlabel{existing}{{2.0.3}{23}{Existing gold standards and evaluation resources}{subsection.2.0.3}{}}
\@writefile{toc}{\contentsline {paragraph}{Representative}{23}{section*.7}}
\@writefile{toc}{\contentsline {paragraph}{Clearly-defined}{23}{section*.8}}
\@writefile{toc}{\contentsline {paragraph}{Consistent and reliable}{23}{section*.9}}
\@writefile{toc}{\contentsline {paragraph}{}{23}{section*.10}}
\@writefile{toc}{\contentsline {paragraph}{\bf  WordSim-353}{23}{section*.11}}
\citation{collobert2008unified}
\citation{huang2012improving}
\citation{huang2012improving}
\citation{pado2007flexible}
\citation{reisinger2010mixture}
\citation{silberer2014learning}
\citation{yong1999case}
\citation{cunningham2005information}
\citation{resnik201011}
\citation{alfonseca2002extending}
\citation{agirre2009study}
\@writefile{toc}{\contentsline {paragraph}{\bf  WS-Sim}{24}{section*.12}}
\citation{agirre2009study}
\citation{agirre2009study}
\citation{hassan2011semantic}
\citation{rubenstein1965contextual}
\citation{bruni2012distributional}
\citation{bruni2012distributional2}
\citation{bernardi2013relatedness}
\citation{bruni2012distributional}
\@writefile{toc}{\contentsline {paragraph}{\bf  Rubenstein \& Goodenough}{25}{section*.13}}
\@writefile{toc}{\contentsline {paragraph}{\bf  The MEN Test Collection}{25}{section*.14}}
\citation{landauer1997solution}
\citation{griffiths2007topics}
\citation{turney2010frequency}
\@writefile{toc}{\contentsline {paragraph}{\bf  Synonym detection sets}{26}{section*.15}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}The SimLex-999 Dataset}{26}{section.2.1}}
\newlabel{simlex}{{2.1}{26}{The SimLex-999 Dataset}{section.2.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Choice of Concepts}{26}{subsection.2.1.1}}
\@writefile{toc}{\contentsline {paragraph}{Separating similarity from association}{26}{section*.16}}
\citation{leech1994claws4}
\citation{fellbaum1999wordnet}
\citation{hill2014multi}
\citation{kielaimproving}
\@writefile{toc}{\contentsline {paragraph}{POS category}{27}{section*.17}}
\@writefile{toc}{\contentsline {paragraph}{Concreteness}{27}{section*.18}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces  Boxplots showing the interaction between concreteness and POS for concepts in USF. The white boxes range from the first to third quartiles and the central vertical line indicates the median.\relax }}{28}{figure.caption.19}}
\newlabel{fig1}{{2.1}{28}{Boxplots showing the interaction between concreteness and POS for concepts in USF. The white boxes range from the first to third quartiles and the central vertical line indicates the median.\relax }{figure.caption.19}{}}
\@writefile{toc}{\contentsline {paragraph}{Final sampling}{28}{section*.20}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}Question Design}{29}{subsection.2.1.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces  Instructions for SimLex-999 annotators.\relax }}{29}{figure.caption.21}}
\newlabel{fig2}{{2.2}{29}{Instructions for SimLex-999 annotators.\relax }{figure.caption.21}{}}
\citation{huang2012improving}
\citation{resnik1995using}
\citation{pedersen2004wordnet}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.3}Context-free rating}{30}{subsection.2.1.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces  A group of noun pairs to be rated by moving the sliders. The rating slider was initially at position 0, and it was possible to attribute a rating of 0, although it was necessary to have actively moved the slider to that position to proceed to the next page.\relax }}{30}{figure.2.3}}
\newlabel{fig3}{{2.3}{30}{A group of noun pairs to be rated by moving the sliders. The rating slider was initially at position 0, and it was possible to attribute a rating of 0, although it was necessary to have actively moved the slider to that position to proceed to the next page.\relax }{figure.2.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.4}Questionnaire structure}{31}{subsection.2.1.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.5}Participants}{31}{subsection.2.1.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.6}Post-processing}{32}{subsection.2.1.6}}
\citation{pado2007flexible}
\citation{reisinger2010mixture}
\citation{silberer2014learning}
\citation{bruni2012distributional2}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Analysis of Dataset}{33}{section.2.2}}
\newlabel{analysis}{{2.2}{33}{Analysis of Dataset}{section.2.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Inter-annotator agreement}{33}{subsection.2.2.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces  {\bf  Left:} Inter-annotator agreement, measured by average pairwise Spearman \(\rho \) correlation, for ratings of concept types in SimLex-999. {\bf  Right:} Response consistency, reflecting the standard deviation of annotator ratings for each pair, averaged over all pairs in the concept category.\relax }}{33}{figure.caption.22}}
\newlabel{fig4}{{2.4}{33}{{\bf Left:} Inter-annotator agreement, measured by average pairwise Spearman \(\rho \) correlation, for ratings of concept types in SimLex-999. {\bf Right:} Response consistency, reflecting the standard deviation of annotator ratings for each pair, averaged over all pairs in the concept category.\relax }{figure.caption.22}{}}
\citation{paivio1991dual}
\citation{williams2009predicting}
\citation{wiebe2000learning}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Response validity: Similarity not association}{34}{subsection.2.2.2}}
\citation{agirre2009study}
\citation{cruse1986lexical}
\@writefile{lot}{\contentsline {table}{\numberline {2.2}{\ignorespaces  {\bf  Top: Similarity aligns with association} Pairs with a small difference in rank between USF (association) and SimLex-999 (similarity) scores for each POS category. {\bf  Bottom: Similarity contrasts with association} Pairs with a high difference in rank for each POS category. *Note that the distribution of USF association scores on the interval [0,10] is highly skewed towards the lower bound in both SimLex-999 and the USF dataset as a whole.\relax }}{35}{table.caption.23}}
\newlabel{tab2}{{2.2}{35}{{\bf Top: Similarity aligns with association} Pairs with a small difference in rank between USF (association) and SimLex-999 (similarity) scores for each POS category. {\bf Bottom: Similarity contrasts with association} Pairs with a high difference in rank for each POS category. *Note that the distribution of USF association scores on the interval [0,10] is highly skewed towards the lower bound in both SimLex-999 and the USF dataset as a whole.\relax }{table.caption.23}{}}
\citation{levy2015supervised}
\citation{rosch1976structural}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces  {\bf  (a)} Pairs rated by WS-353 annotators (blue points, ranked by rating) and the corresponding rating of annotators following the SimLex-999 instructions (red points). {\bf  (b-c)} The same analysis, restricted to pairs in the WS-Sim or WS-Rel subsets of WS-353.\relax }}{36}{figure.caption.24}}
\newlabel{fig5}{{2.5}{36}{{\bf (a)} Pairs rated by WS-353 annotators (blue points, ranked by rating) and the corresponding rating of annotators following the SimLex-999 instructions (red points). {\bf (b-c)} The same analysis, restricted to pairs in the WS-Sim or WS-Rel subsets of WS-353.\relax }{figure.caption.24}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}Finer-grained Semantic Relations}{36}{subsection.2.2.3}}
\citation{baroni2014don}
\citation{Bengio2003lm}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Evaluating Models with SimLex-999}{37}{section.2.3}}
\newlabel{evaluation}{{2.3}{37}{Evaluating Models with SimLex-999}{section.2.3}{}}
\citation{collobert2008unified}
\citation{turian2010word}
\citation{collobert2008unified}
\citation{lewis2004rcv1}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces  Average SimLex and USF free association scores across pairs representing different fine-grained semantic relations. All relations were extracted from WordNet. \(n\_hypernym\) refers to a direct hypernymy path of length \(n\). Note that the average SimLex rating across all 999 word pairs (dashed red line) is much higher than the average USF rating (dashed golden line) because of differences in the rating procedure. The more interesting differences concern the relative strength of similarity vs. association across the different relation types.\relax }}{38}{figure.caption.25}}
\newlabel{fig6}{{2.6}{38}{Average SimLex and USF free association scores across pairs representing different fine-grained semantic relations. All relations were extracted from WordNet. \(n\_hypernym\) refers to a direct hypernymy path of length \(n\). Note that the average SimLex rating across all 999 word pairs (dashed red line) is much higher than the average USF rating (dashed golden line) because of differences in the rating procedure. The more interesting differences concern the relative strength of similarity vs. association across the different relation types.\relax }{figure.caption.25}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Semantic models}{38}{subsection.2.3.1}}
\newlabel{prev}{{2.3.1}{38}{Semantic models}{subsection.2.3.1}{}}
\@writefile{toc}{\contentsline {paragraph}{\bf  Collobert \& Weston}{38}{section*.26}}
\citation{huang2012improving}
\citation{collobert2008unified}
\citation{collobert2008unified}
\citation{mikolov2013efficient}
\citation{mikolov2013efficient}
\citation{mikolov2013distributed}
\@writefile{toc}{\contentsline {paragraph}{\bf  Huang et al.}{39}{section*.27}}
\@writefile{toc}{\contentsline {paragraph}{\bf  Mikolov et al.}{39}{section*.28}}
\citation{kiela2014systematic}
\citation{bird2006nltk}
\citation{recchia2009more}
\citation{landauer1997solution}
\citation{golub1970singular}
\citation{mikolov2013efficient}
\citation{huang2012improving}
\citation{collobert2008unified}
\citation{mikolov2013efficient}
\citation{huang2012improving}
\citation{collobert2008unified}
\citation{lewis2004rcv1}
\citation{lewis2004rcv1}
\citation{huang2012improving}
\citation{huang2012improving}
\citation{huang2012improving}
\citation{huang2012improving}
\citation{collobert2008unified}
\citation{mikolov2013efficient}
\@writefile{toc}{\contentsline {paragraph}{\bf  Simple vector space model (VSM)}{41}{section*.29}}
\@writefile{toc}{\contentsline {paragraph}{\bf  SVD}{41}{section*.30}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Results}{41}{subsection.2.3.2}}
\newlabel{assoc}{{2.3.2}{41}{\bf Overall performance on SimLex-999}{section*.31}{}}
\@writefile{toc}{\contentsline {paragraph}{\bf  Overall performance on SimLex-999}{41}{figure.caption.33}}
\citation{mikolov2013efficient}
\citation{lewis2004rcv1}
\citation{baroni2014don}
\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces Performance of NLMs on WS-353, MEN and SimLex-999. All models are trained on Wikipedia; note that as Wikipedia is constantly growing, the \cite {mikolov2013efficient} model exploited slightly more training data ($\approx $1000m tokens) than the \cite {huang2012improving} model ($\approx $990m), which in turn exploited more than the \cite {collobert2008unified} model ($\approx $852m). Dashed horizontal lines indicate the level of inter-annotator agreement for the three datasets.\relax }}{42}{figure.caption.32}}
\newlabel{fig7}{{2.7}{42}{Performance of NLMs on WS-353, MEN and SimLex-999. All models are trained on Wikipedia; note that as Wikipedia is constantly growing, the \protect \cite {mikolov2013efficient} model exploited slightly more training data ($\approx $1000m tokens) than the \protect \cite {huang2012improving} model ($\approx $990m), which in turn exploited more than the \protect \cite {collobert2008unified} model ($\approx $852m). Dashed horizontal lines indicate the level of inter-annotator agreement for the three datasets.\relax }{figure.caption.32}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.8}{\ignorespaces Comparison between the leading NLM, \emph  {Mikolov et al.}, the vector space model, \emph  {VSM}, and the \emph  {SVD} model. All models were trained on the $\approx $150m word RCV1 Corpus \cite {lewis2004rcv1}.\relax }}{42}{figure.caption.33}}
\newlabel{fig8}{{2.8}{42}{Comparison between the leading NLM, \emph {Mikolov et al.}, the vector space model, \emph {VSM}, and the \emph {SVD} model. All models were trained on the $\approx $150m word RCV1 Corpus \protect \cite {lewis2004rcv1}.\relax }{figure.caption.33}{}}
\citation{huang2012improving}
\citation{collobert2008unified}
\citation{collobert2008unified}
\citation{huang2012improving}
\citation{mikolov2013efficient}
\citation{mikolov2013efficient}
\@writefile{toc}{\contentsline {paragraph}{\bf  Modeling similarity vs. association}{43}{section*.34}}
\citation{levy2014dependency}
\citation{mikolov2013efficient}
\@writefile{lof}{\contentsline {figure}{\numberline {2.9}{\ignorespaces  The ability of NLMs to model the similarity of highly-associated concepts versus concepts in general. The two models on the right hand side also demonstrate the effect of training an NLM (the \cite {mikolov2013efficient} model) on running-text (\emph  {Mikolov et al.}) vs. on dependency-based input (\emph  {Levy \& Goldberg}).\relax }}{44}{figure.caption.35}}
\newlabel{fig9}{{2.9}{44}{The ability of NLMs to model the similarity of highly-associated concepts versus concepts in general. The two models on the right hand side also demonstrate the effect of training an NLM (the \protect \cite {mikolov2013efficient} model) on running-text (\emph {Mikolov et al.}) vs. on dependency-based input (\emph {Levy \& Goldberg}).\relax }{figure.caption.35}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.10}{\ignorespaces  The effect of different window sizes (indicated in square brackets [ ]) on NLM and SVD models. \relax }}{44}{figure.caption.36}}
\newlabel{fig10}{{2.10}{44}{The effect of different window sizes (indicated in square brackets [ ]) on NLM and SVD models. \relax }{figure.caption.36}{}}
\citation{mikolov2013efficient}
\@writefile{lof}{\contentsline {figure}{\numberline {2.11}{\ignorespaces \nobreakspace  {} Performance of models on POS-based subsets of SimLex-999. The window size for each model is indicated in parentheses. Inter-annotator agreement for each POS is indicated by the dashed horizontal line.\relax }}{45}{figure.caption.37}}
\newlabel{fig11}{{2.11}{45}{~ Performance of models on POS-based subsets of SimLex-999. The window size for each model is indicated in parentheses. Inter-annotator agreement for each POS is indicated by the dashed horizontal line.\relax }{figure.caption.37}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.12}{\ignorespaces  The importance of dependency-focussed contexts (in the Levy \& Goldberg model) for capturing concepts of different POS, when compared to a standard Skipgram (BOW) model trained on the same Wikipedia corpus.\relax }}{45}{figure.caption.38}}
\newlabel{fig12}{{2.12}{45}{The importance of dependency-focussed contexts (in the Levy \& Goldberg model) for capturing concepts of different POS, when compared to a standard Skipgram (BOW) model trained on the same Wikipedia corpus.\relax }{figure.caption.38}{}}
\newlabel{begin}{{2.3.2}{45}{\bf Learning concepts of different POS}{section*.39}{}}
\@writefile{toc}{\contentsline {paragraph}{\bf  Learning concepts of different POS}{45}{section*.39}}
\citation{levy2014dependency}
\citation{mikolov2013efficient}
\citation{markman1997similar}
\citation{sun2008verb}
\newlabel{end}{{2.3.2}{46}{\bf Learning concrete and abstract concepts}{section*.40}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.13}{\ignorespaces  Performance of models on concreteness-based subsets of SimLex-999. Window size is indicated in parentheses. Horizontal dashed lines indicate inter-annotator agreement between SimLex-999 annotators on the two subsets.\relax }}{46}{figure.caption.41}}
\newlabel{fig13}{{2.13}{46}{Performance of models on concreteness-based subsets of SimLex-999. Window size is indicated in parentheses. Horizontal dashed lines indicate inter-annotator agreement between SimLex-999 annotators on the two subsets.\relax }{figure.caption.41}{}}
\@writefile{toc}{\contentsline {paragraph}{\bf  Learning concrete and abstract concepts}{46}{figure.caption.41}}
\citation{hill2013concreteness}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Conclusion}{47}{section.2.4}}
\citation{gershmanmetaphor}
\citation{barsalou2003grounding}
\citation{dist}
\citation{andrews2009integrating}
\citation{barsalou2005situating}
\citation{feng2010visual}
\citation{bruni2012distributional}
\citation{silberer2012grounded}
\citation{rollermultimodal}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Improved distributed word representations by learning from more than text}{51}{chapter.3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Learning Distributed Word Representations from Text}{51}{section.3.1}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Modelling Word Acquisition with Multi-Modal Data and Neural Language Models}{51}{section.3.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Introduction}{51}{subsection.3.2.1}}
\citation{bruni2014multimodal}
\citation{silberer2012grounded}
\citation{rollermultimodal}
\citation{silberer2014learning}
\citation{paivio1991dual}
\citation{hill2013quantitative}
\citation{leech1994claws4}
\citation{nelson2004university}
\citation{crutch2005abstract}
\citation{collobert2008unified}
\citation{mesnil2012unsupervised}
\citation{mikolov2013efficient}
\citation{von2004labeling}
\citation{devereux2013centre}
\citation{nelson2004university}
\citation{mikolov2013efficient}
\citation{turian2010word}
\citation{mikolov2013efficient}
\citation{mikolov2013efficient}
\citation{mikolov2013efficient}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Model Design}{53}{subsection.3.2.2}}
\@writefile{toc}{\contentsline {paragraph}{Language-only Model}{53}{section*.42}}
\citation{duchi2011adaptive}
\citation{morin2005hierarchical}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Our multi-modal model architecture. Light boxes are elements of the original \cite  {mikolov2013efficient} model. For target words \(w_n\) in the domain of \(\mathbf  {P}\), the model updates based on corpus context words \( w_{n+i} \) then on words \(p^w_{n+i}\) in perceptual psuedo-sentences. Otherwise, updates are based solely on the \( w_{n+i}. \)\relax }}{54}{figure.caption.43}}
\citation{mikolov2013efficient}
\citation{bybee2001frequency}
\citation{chater2006probabilistic}
\citation{mikolov2013efficient}
\@writefile{toc}{\contentsline {paragraph}{Multi-modal Extension}{55}{section*.44}}
\citation{von2004labeling}
\citation{devereux2013centre}
\citation{mcrae2005semantic}
\citation{silberer2012grounded}
\citation{rollermultimodal}
\citation{fodor1983modularity}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces  Example pseudo-sentences generated by our model.\relax }}{56}{figure.caption.45}}
\newlabel{examples}{{3.2}{56}{Example pseudo-sentences generated by our model.\relax }{figure.caption.45}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.3}Information Sources}{56}{subsection.3.2.3}}
\newlabel{percep_sources}{{3.2.3}{56}{Information Sources}{subsection.3.2.3}{}}
\@writefile{toc}{\contentsline {paragraph}{ESPGame Dataset}{56}{section*.46}}
\@writefile{toc}{\contentsline {paragraph}{CSLB Property Norms}{56}{section*.47}}
\citation{nelson2004university}
\citation{andrews2009integrating}
\citation{feng2010visual}
\citation{silberer2012grounded}
\citation{rollermultimodal}
\citation{huang2012improving}
\citation{silberer2012grounded}
\@writefile{lot}{\contentsline {table}{\numberline {3.1}{\ignorespaces  Concepts identified in images in the ESP Game (left) and features produced for concepts by human annotators in the CSLB dataset (with feature strength, max=30).\relax }}{57}{table.caption.49}}
\newlabel{font-table}{{3.1}{57}{Concepts identified in images in the ESP Game (left) and features produced for concepts by human annotators in the CSLB dataset (with feature strength, max=30).\relax }{table.caption.49}{}}
\@writefile{toc}{\contentsline {paragraph}{Linguistic Input}{57}{section*.48}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.4}Evaluation}{57}{subsection.3.2.4}}
\citation{leech1994claws4}
\@writefile{lot}{\contentsline {table}{\numberline {3.2}{\ignorespaces  Example concept pairs (with mean concreteness rating) and free-association scores from the USF dataset.\relax }}{58}{table.caption.50}}
\newlabel{font-table}{{3.2}{58}{Example concept pairs (with mean concreteness rating) and free-association scores from the USF dataset.\relax }{table.caption.50}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.3}{\ignorespaces  Details the subsets of USF data used in our evaluations, downloadable from our website.\relax }}{58}{table.caption.51}}
\newlabel{font-table}{{3.3}{58}{Details the subsets of USF data used in our evaluations, downloadable from our website.\relax }{table.caption.51}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.5}Results and Discussion}{58}{subsection.3.2.5}}
\citation{mikolov2013efficient}
\citation{silberer2012grounded}
\citation{hardoon2004canonical}
\citation{bruni2014multimodal}
\citation{bruni2014multimodal}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.6}Combining information sources}{59}{subsection.3.2.6}}
\citation{johns2012perceptual}
\citation{johns2012perceptual}
\citation{myers1990classical}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.7}Propagating input to abstract concepts}{60}{subsection.3.2.7}}
\@writefile{toc}{\contentsline {paragraph}{Johns and Jones}{60}{section*.52}}
\@writefile{toc}{\contentsline {paragraph}{Ridge Regression}{60}{section*.53}}
\citation{mikolov2013efficient}
\citation{feng2010visual}
\citation{silberer2012grounded}
\citation{silberer2013models}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces  The proposed approach compared with other methods of information combination (left) and propagation. Dashed lines indicate language-only model baseline.\relax }}{61}{figure.caption.55}}
\newlabel{main_results}{{3.3}{61}{The proposed approach compared with other methods of information combination (left) and propagation. Dashed lines indicate language-only model baseline.\relax }{figure.caption.55}{}}
\@writefile{toc}{\contentsline {paragraph}{Comparisons}{61}{section*.54}}
\citation{katja2005content}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.8}Direct representation vs. propagation}{62}{subsection.3.2.8}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces  Top: Comparing the strategy of directly representing abstract concepts from perceptual information where available (yellow bars) vs. propagating via concrete concepts. Bottom: The effect of increasing \(\alpha \) on correlation with USF pairs (Spearman \(\rho \)) for each concept type. Horizontal dashed lines indicate language-only model baseline.\relax }}{63}{figure.caption.56}}
\newlabel{repprop}{{3.4}{63}{Top: Comparing the strategy of directly representing abstract concepts from perceptual information where available (yellow bars) vs. propagating via concrete concepts. Bottom: The effect of increasing \(\alpha \) on correlation with USF pairs (Spearman \(\rho \)) for each concept type. Horizontal dashed lines indicate language-only model baseline.\relax }{figure.caption.56}{}}
\citation{srivastava2012multimodal}
\citation{wu2013online}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.9}Source and quantity of perceptual input}{64}{subsection.3.2.9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.10}Conclusions}{64}{subsection.3.2.10}}
\citation{grafton2009embodied}
\citation{barsalou2010grounded}
\@writefile{toc}{\contentsline {paragraph}{Type I}{65}{section*.57}}
\@writefile{toc}{\contentsline {paragraph}{Type II}{65}{section*.58}}
\@writefile{toc}{\contentsline {paragraph}{Type III}{65}{section*.59}}
\citation{landauer1997solution}
\citation{turney2010frequency}
\citation{Bengio2003lm}
\citation{mnih2009scalable}
\citation{collobert2008unified}
\citation{mikolov2013distributed}
\citation{Pennington2014}
\citation{baroni2014don}
\citation{kalchbrenner13emnlp}
\citation{devlin2014fast}
\citation{Sutskever2014sequence}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Multi-modal fusion based on image dispersion}{66}{section.3.3}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Sequence-to-Sequence Learning of Word Representations From Bilingual Data}{66}{section.3.4}}
\citation{Bengio2003lm}
\citation{collobert2008unified}
\citation{mikolov2013distributed}
\citation{Pennington2014}
\citation{mikolov2013distributed}
\citation{Hill2014EMNLP}
\citation{levy2014dependency}
\@writefile{toc}{\contentsline {section}{\numberline {3.5}Learning Embeddings with Neural Language Models}{67}{section.3.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.1}Monolingual Models}{67}{subsection.3.5.1}}
\citation{Haghighi2008Learning}
\citation{vulic2011identifying}
\citation{mikolov2013exploiting}
\citation{Hermann:2014:ICLR}
\citation{lauly2014autoencoder}
\citation{Haghighi2008Learning}
\citation{vulic2011identifying}
\citation{mikolov2013exploiting}
\citation{Klementiev}
\citation{Hermann:2014:ICLR}
\citation{lauly2014autoencoder}
\citation{Kocisky:2014}
\citation{Hermann:2014:ICLR}
\citation{Klementiev}
\citation{Hermann:2014:ICLR}
\citation{lauly2014autoencoder}
\citation{Hermann:2014:ICLR}
\citation{faruqui2014improving}
\citation{faruqui2014improving}
\citation{faruqui2014improving}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.2}Bilingual Representation-learning Models}{68}{subsection.3.5.2}}
\citation{kalchbrenner13emnlp}
\citation{Sutskever2014sequence}
\citation{bahdanau2014neural}
\citation{kalchbrenner13emnlp}
\citation{Cho2014}
\citation{bahdanau2014neural}
\citation{Cho2014}
\citation{bahdanau2014neural}
\citation{kalchbrenner13emnlp}
\citation{devlin2014fast}
\citation{Sutskever2014sequence}
\citation{Cho2014}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.3}Neural Machine Translation Models}{69}{subsection.3.5.3}}
\citation{Hermann:2014:ICLR}
\citation{faruqui2014improving}
\citation{mikolov2013distributed}
\citation{Pennington2014}
\citation{collobert2008unified}
\citation{Agirre2009}
\citation{bruni2014multimodal}
\citation{baroni2014don}
\citation{Agirre2009}
\citation{bruni2014multimodal}
\citation{hill2014simlex}
\@writefile{toc}{\contentsline {section}{\numberline {3.6}Experiments}{70}{section.3.6}}
\citation{hill2014simlex}
\citation{faruqui2014improving}
\citation{hill2014simlex}
\citation{nelson2004university}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6.1}Similarity and relatedness modelling}{71}{subsection.3.6.1}}
\citation{landauer1997solution}
\@writefile{lot}{\contentsline {table}{\numberline {3.4}{\ignorespaces  NMT embeddings (RNNenc and RNNsearch) clearly outperform alternative embedding-learning architectures on tasks that require modelling similarity (below the dashed line), but not on tasks that reflect relatedness. Bilingual embedding spaces learned without the translation objective are somewhere between these two extremes.\relax }}{72}{table.caption.60}}
\newlabel{table:perf}{{3.4}{72}{NMT embeddings (RNNenc and RNNsearch) clearly outperform alternative embedding-learning architectures on tasks that require modelling similarity (below the dashed line), but not on tasks that reflect relatedness. Bilingual embedding spaces learned without the translation objective are somewhere between these two extremes.\relax }{table.caption.60}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.5}{\ignorespaces Nearest neighbours (excluding plurals) in the embedding spaces of different models. All models were trained for 6 epochs on the translation corpus except CW and FD (as noted previously). NMT embedding spaces are oriented according to similarity, whereas embeddings learned by monolingual models are organized according to relatedness. The other bilingual model BiCVM also exhibits a notable focus on similarity.\relax }}{72}{table.caption.61}}
\newlabel{table:neigh}{{3.5}{72}{Nearest neighbours (excluding plurals) in the embedding spaces of different models. All models were trained for 6 epochs on the translation corpus except CW and FD (as noted previously). NMT embedding spaces are oriented according to similarity, whereas embeddings learned by monolingual models are organized according to relatedness. The other bilingual model BiCVM also exhibits a notable focus on similarity.\relax }{table.caption.61}{}}
\citation{mikolov2013distributed}
\citation{mikolov2013distributed}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6.2}Importance of training data quantity}{73}{subsection.3.6.2}}
\citation{mikolov2013distributed}
\citation{faruqui2014improving}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces The effect of increasing the amount of training data on the quality of monolingual embeddings, based on similarity-based evaluations (SimLex-999) and two relatedness-based evaluations (MEN and WordSim-353). \emph  {ET} in the legend indicates models trained on the English half of the translation corpus. \emph  {Wiki} indicates models trained on Wikipedia.\relax }}{74}{figure.caption.62}}
\newlabel{fig:size}{{3.5}{74}{The effect of increasing the amount of training data on the quality of monolingual embeddings, based on similarity-based evaluations (SimLex-999) and two relatedness-based evaluations (MEN and WordSim-353). \emph {ET} in the legend indicates models trained on the English half of the translation corpus. \emph {Wiki} indicates models trained on Wikipedia.\relax }{figure.caption.62}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6.3}Analogy Resolution}{74}{subsection.3.6.3}}
\citation{levy2014dependency}
\@writefile{toc}{\contentsline {section}{\numberline {3.7}Effect of Target Language}{75}{section.3.7}}
\newlabel{lang_effects}{{3.7}{75}{Effect of Target Language}{section.3.7}{}}
\citation{morin2005hierarchical}
\citation{mikolov2013distributed}
\citation{Hermann:2014:ICLR}
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces Translation-based embeddings perform best on syntactic analogies (\emph  {run,ran: hide, hid}). Monolingual skipgram/Glove models are better at semantic analogies (\emph  {father, man; mother, woman})\relax }}{76}{figure.caption.63}}
\newlabel{fig:analogy}{{3.6}{76}{Translation-based embeddings perform best on syntactic analogies (\emph {run,ran: hide, hid}). Monolingual skipgram/Glove models are better at semantic analogies (\emph {father, man; mother, woman})\relax }{figure.caption.63}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.6}{\ignorespaces Comparison of embeddings learned by RNN Search models translating between English-French (EN-FR) and English-German (EN-DE) on all semantic evaluations (left) and nearest neighbours of selected cue words (right). Bold italics indicate target-language-specific effects. Evaluation items and vocabulary searches were restricted to words common to both models. \relax }}{76}{table.caption.64}}
\newlabel{table:de}{{3.6}{76}{Comparison of embeddings learned by RNN Search models translating between English-French (EN-FR) and English-German (EN-DE) on all semantic evaluations (left) and nearest neighbours of selected cue words (right). Bold italics indicate target-language-specific effects. Evaluation items and vocabulary searches were restricted to words common to both models. \relax }{table.caption.64}{}}
\citation{Jean}
\citation{Bengio+Senecal-2003-small}
\citation{luong2014addressing}
\@writefile{toc}{\contentsline {section}{\numberline {3.8}Overcoming the Vocabulary Size Problem}{77}{section.3.8}}
\@writefile{lot}{\contentsline {table}{\numberline {3.7}{\ignorespaces Comparison of embeddings learned by the original (RNN Search - 30k French words, 50k German words) and extended-vocabulary (RNN Search-LV -500k words) models translating from English to French (EN-FR) and from English to German (EN-DE). For fair comparisons, all evaluations were restricted to the intersection of all model vocabularies.\relax }}{78}{table.caption.65}}
\newlabel{table:ex}{{3.7}{78}{Comparison of embeddings learned by the original (RNN Search - 30k French words, 50k German words) and extended-vocabulary (RNN Search-LV -500k words) models translating from English to French (EN-FR) and from English to German (EN-DE). For fair comparisons, all evaluations were restricted to the intersection of all model vocabularies.\relax }{table.caption.65}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.9}How Similarity Emerges}{78}{section.3.9}}
\newlabel{section:exp}{{3.9}{78}{How Similarity Emerges}{section.3.9}{}}
\citation{faruqui2014improving}
\citation{hill2014simlex}
\citation{faruqui2014improving}
\citation{Hermann:2014:ICLR}
\citation{faruqui2014improving}
\citation{Kocisky:2014}
\citation{dist}
\@writefile{toc}{\contentsline {section}{\numberline {3.10}Conclusion}{80}{section.3.10}}
\citation{mikolov2013distributed}
\citation{zock2004word}
\citation{307754}
\citation{Klementiev}
\citation{hermann2013multilingual}
\citation{gouws2014bilbowa}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Representing phrases with neural language models}{83}{chapter.4}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Introduction}{83}{section.4.1}}
\citation{mikolov2010recurrent}
\citation{kiros2014unifying}
\citation{bahdanau2014neural}
\citation{mikolov2013distributed}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Neural Language Model Architectures}{84}{section.4.2}}
\citation{bengio1994learning}
\citation{hochreiter1997long}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}Long Short Term Memory}{85}{subsection.4.2.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}Bag-of-Words NLMs}{86}{subsection.4.2.2}}
\citation{huang2012improving}
\citation{bordes2015large}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.3}Pre-trained Input Representations}{87}{subsection.4.2.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.4}Training Objective}{87}{subsection.4.2.4}}
\citation{faruqui2014retrofitting}
\citation{cho2014learning}
\citation{bergstra+al:2010-scipy}
\citation{zeiler2012adadelta}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.5}Implementation Details}{88}{subsection.4.2.5}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Reverse Dictionaries}{88}{section.4.3}}
\citation{bilac2003improving}
\citation{bilac2004dictionary}
\citation{zock2004word}
\citation{shaw2013building}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.1}Data Collection and Training}{89}{subsection.4.3.1}}
\citation{mitchell2010composition}
\@writefile{lot}{\contentsline {table}{\numberline {4.1}{\ignorespaces Performance of different reverse dictionary models in different evaluation settings. *Low variance in \emph  {mult} models is due to consistently poor scores, so not highlighted.\relax }}{90}{table.caption.66}}
\newlabel{results}{{4.1}{90}{Performance of different reverse dictionary models in different evaluation settings. *Low variance in \emph {mult} models is due to consistently poor scores, so not highlighted.\relax }{table.caption.66}{}}
\citation{shaw2013building}
\citation{leech1994claws4}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.2}Comparisons}{91}{subsection.4.3.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.3}Reverse Dictionary Evaluation}{91}{subsection.4.3.3}}
\@writefile{lot}{\contentsline {table}{\numberline {4.2}{\ignorespaces Style difference between \emph  {dictionary definitions} and \emph  {concept descriptions} in the evaluation.\relax }}{92}{table.caption.67}}
\newlabel{tb:tablename}{{4.2}{92}{Style difference between \emph {dictionary definitions} and \emph {concept descriptions} in the evaluation.\relax }{table.caption.67}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.4}Results}{92}{subsection.4.3.4}}
\citation{iyyer2015deep}
\@writefile{lot}{\contentsline {table}{\numberline {4.3}{\ignorespaces The top-five candidates for example queries (invented by the authors) from different reverse dictionary models. Both the RNN and BOW models are without Word2Vec input and use the cosine loss.\relax }}{94}{table.caption.68}}
\newlabel{qual}{{4.3}{94}{The top-five candidates for example queries (invented by the authors) from different reverse dictionary models. Both the RNN and BOW models are without Word2Vec input and use the cosine loss.\relax }{table.caption.68}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.5}Qualitative Analysis}{94}{subsection.4.3.5}}
\citation{hermann2013multilingual}
\citation{lauly2014autoencoder}
\citation{gouws2014bilbowa}
\citation{gouws2014bilbowa}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.6}Cross-Lingual Reverse Dictionaries}{95}{subsection.4.3.6}}
\@writefile{lot}{\contentsline {table}{\numberline {4.4}{\ignorespaces Responses from cross-lingual reverse dictionary models to selected queries. Underlined responses are `correct' or potentially useful for a native French speaker.\relax }}{95}{table.caption.69}}
\newlabel{cross}{{4.4}{95}{Responses from cross-lingual reverse dictionary models to selected queries. Underlined responses are `correct' or potentially useful for a native French speaker.\relax }{table.caption.69}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.7}Discussion}{96}{subsection.4.3.7}}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}General Knowledge (crossword) Question Answering}{96}{section.4.4}}
\citation{ferrucci2010building}
\citation{molla2007question}
\citation{Iyyer:Boyd-Graber:Claudino:Socher:Daume-2014}
\citation{weston2015towards}
\citation{berant14paraphrasing}
\citation{bordes2014question}
\citation{littman2002probabilistic}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.1}Evaluation}{97}{subsection.4.4.1}}
\citation{littman2002probabilistic}
\citation{ginsberg2011dr}
\@writefile{lot}{\contentsline {table}{\numberline {4.5}{\ignorespaces Examples of the different question types in the crossword question evaluation dataset.\relax }}{98}{table.caption.70}}
\newlabel{tb:tablename}{{4.5}{98}{Examples of the different question types in the crossword question evaluation dataset.\relax }{table.caption.70}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.6}{\ignorespaces Performance of different models on crossword questions of different length. The two commercial systems are evaluated via their web interface so only accuracy@10 can be reported in those cases. \relax }}{98}{table.caption.71}}
\newlabel{results2}{{4.6}{98}{Performance of different models on crossword questions of different length. The two commercial systems are evaluated via their web interface so only accuracy@10 can be reported in those cases. \relax }{table.caption.71}{}}
\newlabel{tb:tablename}{{4.6}{98}{Performance of different models on crossword questions of different length. The two commercial systems are evaluated via their web interface so only accuracy@10 can be reported in those cases. \relax }{table.caption.71}{}}
\citation{littman2002probabilistic}
\citation{ginsberg2011dr}
\citation{mikolov2013distributed}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.2}Benchmarks and Comparisons}{99}{subsection.4.4.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.3}Results}{99}{subsection.4.4.3}}
\@writefile{lot}{\contentsline {table}{\numberline {4.7}{\ignorespaces Responses from different models to example crossword clues. In each case the model output is filtered to exclude any candidates that are not of the same length as the correct answer. BOW and RNN models are trained without Word2Vec input embeddings and cosine loss.\relax }}{100}{table.caption.72}}
\newlabel{egs}{{4.7}{100}{Responses from different models to example crossword clues. In each case the model output is filtered to exclude any candidates that are not of the same length as the correct answer. BOW and RNN models are trained without Word2Vec input embeddings and cosine loss.\relax }{table.caption.72}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.4}Qualitative Analysis}{100}{subsection.4.4.4}}
\citation{bordes2014question}
\citation{graves2014neural}
\citation{weston2015towards}
\@writefile{toc}{\contentsline {section}{\numberline {4.5}Conclusion}{101}{section.4.5}}
\citation{graves2014neural}
\citation{weston2015towards}
\citation{baroni2014don}
\citation{levy2014neural}
\citation{hill2014simlex}
\citation{collobert2011natural}
\citation{mitchell2008vector}
\citation{clark2007combining}
\citation{baroni2014frege}
\citation{milajevs2014evaluating}
\citation{Sutskever2014sequence}
\citation{mao2014deep}
\citation{serban2015building}
\citation{cho2014learning}
\citation{norman1972memory}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Representing sentences with neural language models}{103}{chapter.5}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.0.1}Introduction}{103}{subsection.5.0.1}}
\citation{kiros2015skip}
\citation{hill2015learning}
\citation{socher2011semi}
\citation{kalchbrenner2014convolutional}
\citation{milajevs2014evaluating}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.0.2}Distributed Sentence Representations}{104}{subsection.5.0.2}}
\citation{kiros2015skip}
\citation{cho2014learning}
\citation{le2014distributed}
\citation{mikolov2013distributed}
\citation{mitchell2010composition}
\citation{marcobaronijointly}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.0.3}Existing Models Trained on Text}{105}{subsection.5.0.3}}
\citation{hill2015learning}
\citation{chen2015microsoft}
\citation{szegedy2014going}
\citation{russakovsky2014imagenet}
\citation{cho2014learning}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.0.4}Models Trained on Structured Resources}{106}{subsection.5.0.4}}
\citation{vincent2008extracting}
\citation{vincent2010stacked}
\citation{dai2015semi}
\citation{iyyer2015deep}
\citation{sutskever2011generating}
\citation{harris1954distributional}
\citation{polajnar2015exploration}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.0.5}Novel Text-Based Models}{107}{subsection.5.0.5}}
\newlabel{eqn1}{{5.1}{108}{Novel Text-Based Models}{equation.5.0.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.0.6}Training and Model Selection}{108}{subsection.5.0.6}}
\citation{collobert2011natural}
\citation{mikolov2013efficient}
\citation{kiros2015skip}
\citation{hill2015learning}
\citation{baroni2014don}
\citation{levy2015improving}
\@writefile{lot}{\contentsline {table}{\numberline {5.1}{\ignorespaces  {\bf  Properties of models compared in this study} \hspace  {6mm} {\bf  OS:} requires training corpus of sentences in order. {\bf  R:} requires structured resource for training. {\bf  WO:} encoder sensitive to word order. {\bf  SD:} dimension of sentence representation. {\bf  WD:} dimension of word representation. {\bf  TR:} approximate training time (hours) on the dataset in this paper. * indicates trained on GPU. {\bf  TE:} approximate time (s) taken to encode 0.5m sentences.\relax }}{109}{table.caption.73}}
\newlabel{modelprops}{{5.1}{109}{{\bf Properties of models compared in this study} \hspace {6mm} {\bf OS:} requires training corpus of sentences in order. {\bf R:} requires structured resource for training. {\bf WO:} encoder sensitive to word order. {\bf SD:} dimension of sentence representation. {\bf WD:} dimension of word representation. {\bf TR:} approximate training time (hours) on the dataset in this paper. * indicates trained on GPU. {\bf TE:} approximate time (s) taken to encode 0.5m sentences.\relax }{table.caption.73}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.2}{\ignorespaces  Example sentence pairs and `similarity' ratings from the unsupervised evaluations used in this study.\relax }}{109}{table.caption.74}}
\newlabel{unsex}{{5.2}{109}{Example sentence pairs and `similarity' ratings from the unsupervised evaluations used in this study.\relax }{table.caption.74}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.0.7}Evaluating Sentence Representations}{109}{subsection.5.0.7}}
\citation{dolan2004unsupervised}
\citation{pang2005seeing}
\citation{hu2004mining}
\citation{pang2004sentimental}
\citation{wiebe2005annotating}
\citation{voorhees2002overview}
\citation{kiros2015skip}
\citation{marelli2014sick}
\citation{agirre2014semeval}
\@writefile{lot}{\contentsline {table}{\numberline {5.3}{\ignorespaces  Performance of sentence representation models on {\bf  supervised} evaluations (Section\nobreakspace  {}\ref  {supersec}). Bold numbers indicate best performance in class. Underlined indicates best overall. \relax }}{110}{table.caption.75}}
\newlabel{supervised}{{5.3}{110}{Performance of sentence representation models on {\bf supervised} evaluations (Section~\ref {supersec}). Bold numbers indicate best performance in class. Underlined indicates best overall. \relax }{table.caption.75}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.0.8}Supervised Evaluations}{110}{subsection.5.0.8}}
\newlabel{supersec}{{5.0.8}{110}{Supervised Evaluations}{subsection.5.0.8}{}}
\citation{ji2013discriminative}
\citation{vincent2008extracting}
\@writefile{lot}{\contentsline {table}{\numberline {5.4}{\ignorespaces  Performance of sentence representation models (Spearman/Pearson correlations) on {\bf  unsupervised} (relatedness) evaluations (Section\nobreakspace  {}\ref  {unseval}). Models are grouped according to training data as indicated in Table\nobreakspace  {}\ref  {supervised}.\relax }}{111}{table.caption.76}}
\newlabel{unsupervised}{{5.4}{111}{Performance of sentence representation models (Spearman/Pearson correlations) on {\bf unsupervised} (relatedness) evaluations (Section~\ref {unseval}). Models are grouped according to training data as indicated in Table~\ref {supervised}.\relax }{table.caption.76}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.0.9}Unsupervised Evaluations}{111}{subsection.5.0.9}}
\newlabel{unseval}{{5.0.9}{111}{Unsupervised Evaluations}{subsection.5.0.9}{}}
\citation{marcobaronijointly}
\citation{voorhees2002overview}
\citation{bruni2014multimodal}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.0.10}Results}{112}{subsection.5.0.10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.0.11}Discussion}{112}{subsection.5.0.11}}
\citation{almahairi2015learning}
\@writefile{lot}{\contentsline {table}{\numberline {5.5}{\ignorespaces Sample nearest neighbour queries selected from a randomly sampled 0.5m sentences of the Toronto Books Corpus.\relax }}{114}{table.caption.77}}
\newlabel{neighbours}{{5.5}{114}{Sample nearest neighbour queries selected from a randomly sampled 0.5m sentences of the Toronto Books Corpus.\relax }{table.caption.77}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.6}{\ignorespaces  Internal consistency (Chronbach's \(\alpha \)) among evaluations when individual benchmarks are left out of the (supervised or unsupervised) cohorts. Consistency rank within cohort is in parentheses (1 = most consistent with other evaluations).\relax }}{114}{table.caption.78}}
\newlabel{consistency}{{5.6}{114}{Internal consistency (Chronbach's \(\alpha \)) among evaluations when individual benchmarks are left out of the (supervised or unsupervised) cohorts. Consistency rank within cohort is in parentheses (1 = most consistent with other evaluations).\relax }{table.caption.78}{}}
\citation{wiebe2005annotating}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.0.12}Conclusion}{115}{subsection.5.0.12}}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Representing semantics in memory networks}{117}{chapter.6}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {chapter}{\numberline {7}Conclusion}{119}{chapter.7}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\bibstyle{plain}
\bibdata{thesis}
\bibcite{agirre2009study}{1}
\bibcite{Agirre2009}{2}
\bibcite{agirre2014semeval}{3}
\bibcite{alfonseca2002extending}{4}
\bibcite{almahairi2015learning}{5}
\bibcite{andrews2009integrating}{6}
\bibcite{bahdanau2014neural}{7}
\@writefile{toc}{\contentsline {chapter}{Bibliography}{121}{section*.79}}
\bibcite{bansal2014tailoring}{8}
\bibcite{baroni2014frege}{9}
\bibcite{baroni2014don}{10}
\bibcite{baroni2010distributional}{11}
\bibcite{barsalou2010grounded}{12}
\bibcite{barsalou2003grounding}{13}
\bibcite{barsalou2005situating}{14}
\bibcite{beltagysemantic}{15}
\bibcite{Bengio2003lm}{16}
\bibcite{Bengio+Senecal-2003-small}{17}
\bibcite{bengio1994learning}{18}
\bibcite{berant14paraphrasing}{19}
\bibcite{bergstra+al:2010-scipy}{20}
\bibcite{bernardi2013relatedness}{21}
\bibcite{biemann2005ontology}{22}
\bibcite{bilac2003improving}{23}
\bibcite{bilac2004dictionary}{24}
\bibcite{bird2006nltk}{25}
\bibcite{bordes2014question}{26}
\bibcite{bordes2015large}{27}
\bibcite{bruni2012distributional}{28}
\bibcite{bruni2014multimodal}{29}
\bibcite{bruni2012distributional2}{30}
\bibcite{budanitsky2006evaluating}{31}
\bibcite{bybee2001frequency}{32}
\bibcite{lauly2014autoencoder}{33}
\bibcite{chater2006probabilistic}{34}
\bibcite{chen2015microsoft}{35}
\bibcite{cho2014learning}{36}
\bibcite{Cho2014}{37}
\bibcite{cimiano2005learning}{38}
\bibcite{clark2007combining}{39}
\bibcite{collobert2008unified}{40}
\bibcite{collobert2011natural}{41}
\bibcite{cruse1986lexical}{42}
\bibcite{crutch2005abstract}{43}
\bibcite{cunningham2005information}{44}
\bibcite{dai2015semi}{45}
\bibcite{devereux2013centre}{46}
\bibcite{devlin2014fast}{47}
\bibcite{dolan2004unsupervised}{48}
\bibcite{duchi2011adaptive}{49}
\bibcite{faruqui2014retrofitting}{50}
\bibcite{faruqui2014improving}{51}
\bibcite{fellbaum1999wordnet}{52}
\bibcite{feng2010visual}{53}
\bibcite{ferrucci2010building}{54}
\bibcite{finkelstein2001placing}{55}
\bibcite{dist}{56}
\bibcite{fodor1983modularity}{57}
\bibcite{gentner1978relational}{58}
\bibcite{gentner2006verbs}{59}
\bibcite{gershmanmetaphor}{60}
\bibcite{ginsberg2011dr}{61}
\bibcite{golub1970singular}{62}
\bibcite{gouws2014bilbowa}{63}
\bibcite{grafton2009embodied}{64}
\bibcite{graves2014neural}{65}
\bibcite{griffiths2007topics}{66}
\bibcite{Haghighi2008Learning}{67}
\bibcite{hardoon2004canonical}{68}
\bibcite{harris1954distributional}{69}
\bibcite{hassan2011semantic}{70}
\bibcite{hatzivassiloglou2001simfinder}{71}
\bibcite{he2008indirect}{72}
\bibcite{hermann2013multilingual}{73}
\bibcite{Hermann:2014:ICLR}{74}
\bibcite{hill2015learning}{75}
\bibcite{hill2013concreteness}{76}
\bibcite{Hill2014EMNLP}{77}
\bibcite{hill2013quantitative}{78}
\bibcite{hill2014multi}{79}
\bibcite{hill2014simlex}{80}
\bibcite{hochreiter1997long}{81}
\bibcite{hu2004mining}{82}
\bibcite{huang2012improving}{83}
\bibcite{Iyyer:Boyd-Graber:Claudino:Socher:Daume-2014}{84}
\bibcite{iyyer2015deep}{85}
\bibcite{Jean}{86}
\bibcite{ji2013discriminative}{87}
\bibcite{johns2012perceptual}{88}
\bibcite{kalchbrenner13emnlp}{89}
\bibcite{kalchbrenner2014convolutional}{90}
\bibcite{kiela2014systematic}{91}
\bibcite{kielaimproving}{92}
\bibcite{kiros2014unifying}{93}
\bibcite{kiros2015skip}{94}
\bibcite{Klementiev}{95}
\bibcite{Kocisky:2014}{96}
\bibcite{landauer1997solution}{97}
\bibcite{le2014distributed}{98}
\bibcite{leech1994claws4}{99}
\bibcite{levy2014dependency}{100}
\bibcite{levy2014neural}{101}
\bibcite{levy2015improving}{102}
\bibcite{levy2015supervised}{103}
\bibcite{lewis2004rcv1}{104}
\bibcite{li2014obtaining}{105}
\bibcite{li2006exploring}{106}
\bibcite{littman2002probabilistic}{107}
\bibcite{luong2013better}{108}
\bibcite{luong2014addressing}{109}
\bibcite{mao2014deep}{110}
\bibcite{marelli2014sick}{111}
\bibcite{markman1997similar}{112}
\bibcite{marton2009improved}{113}
\bibcite{mcrae2005semantic}{114}
\bibcite{mcrae2012semantic}{115}
\bibcite{medelyan2009mining}{116}
\bibcite{mesnil2012unsupervised}{117}
\bibcite{mikolov2013efficient}{118}
\bibcite{mikolov2010recurrent}{119}
\bibcite{mikolov2013exploiting}{120}
\bibcite{mikolov2013distributed}{121}
\bibcite{milajevs2014evaluating}{122}
\bibcite{mitchell2008vector}{123}
\bibcite{mitchell2010composition}{124}
\bibcite{mnih2009scalable}{125}
\bibcite{molla2007question}{126}
\bibcite{morin2005hierarchical}{127}
\bibcite{myers1990classical}{128}
\bibcite{navigli2009word}{129}
\bibcite{nelson2004university}{130}
\bibcite{norman1972memory}{131}
\bibcite{pado2007flexible}{132}
\bibcite{paivio1991dual}{133}
\bibcite{pang2004sentimental}{134}
\bibcite{pang2005seeing}{135}
\bibcite{pedersen2004wordnet}{136}
\bibcite{Pennington2014}{137}
\bibcite{marcobaronijointly}{138}
\bibcite{phan2008learning}{139}
\bibcite{plaut1995semantic}{140}
\bibcite{polajnar2015exploration}{141}
\bibcite{recchia2009more}{142}
\bibcite{reisinger2010mixture}{143}
\bibcite{reisinger2010multi}{144}
\bibcite{resnik1995using}{145}
\bibcite{resnik201011}{146}
\bibcite{rollermultimodal}{147}
\bibcite{rosch1976structural}{148}
\bibcite{rose2002reuters}{149}
\bibcite{rubenstein1965contextual}{150}
\bibcite{russakovsky2014imagenet}{151}
\bibcite{serban2015building}{152}
\bibcite{shaw2013building}{153}
\bibcite{silberer2014learning}{154}
\bibcite{silberer2013models}{155}
\bibcite{silberer2012grounded}{156}
\bibcite{socher2011semi}{157}
\bibcite{srivastava2012multimodal}{158}
\bibcite{sun2008verb}{159}
\bibcite{sutskever2011generating}{160}
\bibcite{Sutskever2014sequence}{161}
\bibcite{szegedy2014going}{162}
\bibcite{turian2010word}{163}
\bibcite{turney2012domain}{164}
\bibcite{turney2010frequency}{165}
\bibcite{tversky1977features}{166}
\bibcite{vincent2008extracting}{167}
\bibcite{vincent2010stacked}{168}
\bibcite{von2004labeling}{169}
\bibcite{voorhees2002overview}{170}
\bibcite{vulic2011identifying}{171}
\bibcite{307754}{172}
\bibcite{weston2015towards}{173}
\bibcite{wiebe2000learning}{174}
\bibcite{wiebe2005annotating}{175}
\bibcite{katja2005content}{176}
\bibcite{williams2009predicting}{177}
\bibcite{wu2013online}{178}
\bibcite{wu1994verbs}{179}
\bibcite{yong1999case}{180}
\bibcite{zeiler2012adadelta}{181}
\bibcite{zock2004word}{182}
\@writefile{toc}{\contentsline {chapter}{\numberline {A}Extra Information}{141}{appendix.A}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
