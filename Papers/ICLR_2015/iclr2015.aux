\relax 
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\citation{landauer1997solution,turney2010frequency}
\citation{Bengio2003lm,mnih2009scalable,collobert2008unified}
\citation{mikolov2013distributed,Pennington2014}
\citation{baroni2014don}
\citation{kalchbrenner13emnlp,devlin2014fast,Sutskever2014sequence}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\citation{Bengio2003lm}
\citation{collobert2008unified}
\citation{mikolov2013distributed,Pennington2014}
\citation{mikolov2013distributed}
\citation{Hill2014EMNLP,levy2014dependency}
\citation{haghighi2008learning,vulic2011identifying,mikolov2013exploiting,Hermann:2014:ICLR,Chandar}
\citation{haghighi2008learning,vulic2011identifying,mikolov2013exploiting}
\citation{klementiev2012inducing,Hermann:2014:ICLR,Chandar,Kocisky:2014}
\@writefile{toc}{\contentsline {section}{\numberline {2}Learning Embeddings with Neural Language Models}{2}{section.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Monolingual Models}{2}{subsection.2.1}}
\citation{Hermann:2014:ICLR}
\citation{Klementiev}
\citation{Hermann:2014:ICLR}
\citation{Chandar}
\citation{Hermann:2014:ICLR}
\citation{faruqui2014improving}
\citation{faruqui2014improving}
\citation{faruqui2014improving}
\citation{kalchbrenner13emnlp,Sutskever2014sequence}
\citation{Bahdanau2014}
\citation{kalchbrenner13emnlp,Cho2014,Bahdanau2014}
\citation{Cho2014}
\citation{Bahdanau2014}
\citation{kalchbrenner13emnlp}
\citation{devlin2014fast}
\citation{Sutskever2014sequence}
\citation{Cho2014}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Bilingual Representation-learning Models}{3}{subsection.2.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Neural Machine Translation Models}{3}{subsection.2.3}}
\citation{Hermann:2014:ICLR}
\citation{faruqui2014improving}
\citation{mikolov2013distributed}
\citation{Pennington2014}
\citation{collobert2008unified}
\citation{Agirre2009,Bruni2014,baroni2014don}
\citation{Agirre2009}
\citation{Bruni2014}
\citation{hill2014simlex}
\citation{hill2014simlex}
\citation{faruqui2014improving}
\@writefile{toc}{\contentsline {section}{\numberline {3}Experiments}{4}{section.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Similarity and relatedness modelling}{4}{subsection.3.1}}
\citation{hill2014simlex}
\citation{nelson2004university}
\citation{landauer1997solution}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces  NMT embeddings (RNNenc and RNNsearch) clearly outperform alternative embedding-learning architectures on tasks that require modelling similarity (below the dashed line), but not on tasks that reflect relatedness. Bilingual embedding spaces learned without the translation objective are somewhere between these two extremes.}}{5}{table.1}}
\newlabel{table:perf}{{1}{5}{NMT embeddings (RNNenc and RNNsearch) clearly outperform alternative embedding-learning architectures on tasks that require modelling similarity (below the dashed line), but not on tasks that reflect relatedness. Bilingual embedding spaces learned without the translation objective are somewhere between these two extremes}{table.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Nearest neighbours (excluding plurals) in the embedding spaces of different models. All models were trained for 6 epochs on the translation corpus except CW and FD (as noted previously). NMT embedding spaces are oriented according to similarity, whereas embeddings learned by monolingual models are organized according to relatedness. The other bilingual model BiCVM also exhibits a notable focus on similarity.}}{5}{table.2}}
\newlabel{table:neigh}{{2}{5}{Nearest neighbours (excluding plurals) in the embedding spaces of different models. All models were trained for 6 epochs on the translation corpus except CW and FD (as noted previously). NMT embedding spaces are oriented according to similarity, whereas embeddings learned by monolingual models are organized according to relatedness. The other bilingual model BiCVM also exhibits a notable focus on similarity}{table.2}{}}
\citation{mikolov2013distributed}
\citation{mikolov2013distributed}
\citation{mikolov2013distributed}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Importance of training data quantity}{6}{subsection.3.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces The effect of increasing the amount of training data on the quality of monolingual embeddings, based on similarity-based evaluations (SimLex-999) and two relatedness-based evaluations (MEN and WordSim-353). \emph  {ET} in the legend indicates models trained on the English half of the translation corpus. \emph  {Wiki} indicates models trained on Wikipedia.}}{6}{figure.1}}
\newlabel{fig:size}{{1}{6}{The effect of increasing the amount of training data on the quality of monolingual embeddings, based on similarity-based evaluations (SimLex-999) and two relatedness-based evaluations (MEN and WordSim-353). \emph {ET} in the legend indicates models trained on the English half of the translation corpus. \emph {Wiki} indicates models trained on Wikipedia}{figure.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Analogy Resolution}{6}{subsection.3.3}}
\citation{faruqui2014improving}
\citation{levy2014dependency}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Translation-based embeddings perform best on syntactic analogies (\emph  {run,ran: hide, hid}). Monolingual skipgram/Glove models are better at semantic analogies (\emph  {father, man; mother, woman})}}{7}{figure.2}}
\newlabel{fig:analogy}{{2}{7}{Translation-based embeddings perform best on syntactic analogies (\emph {run,ran: hide, hid}). Monolingual skipgram/Glove models are better at semantic analogies (\emph {father, man; mother, woman})\relax }{figure.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Effect of Target Language}{7}{section.4}}
\citation{morin2005hierarchical}
\citation{mikolov2013distributed,Hermann:2014:ICLR}
\citation{Jean}
\citation{Bengio+Senecal-2003-small}
\citation{luong2014addressing}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Comparison of embeddings learned by RNN Search models translating between English-French (EN-FR) and English-German (EN-DE) on all semantic evaluations (left) and nearest neighbours of selected cue words (right). Bold italics indicate target-language-specific effects. Evaluation items and vocabulary searches were restricted to words common to both models. }}{8}{table.3}}
\newlabel{table:de}{{3}{8}{Comparison of embeddings learned by RNN Search models translating between English-French (EN-FR) and English-German (EN-DE) on all semantic evaluations (left) and nearest neighbours of selected cue words (right). Bold italics indicate target-language-specific effects. Evaluation items and vocabulary searches were restricted to words common to both models}{table.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Overcoming the Vocabulary Size Problem}{8}{section.5}}
\citation{faruqui2014improving}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Comparison of embeddings learned by the original (RNN Search - 30k French words, 50k German words) and extended-vocabulary (RNN Search-LV -500k words) models translating from English to French (EN-FR) and from English to German (EN-DE). For fair comparisons, all evaluations were restricted to the intersection of all model vocabularies.}}{9}{table.4}}
\newlabel{table:ex}{{4}{9}{Comparison of embeddings learned by the original (RNN Search - 30k French words, 50k German words) and extended-vocabulary (RNN Search-LV -500k words) models translating from English to French (EN-FR) and from English to German (EN-DE). For fair comparisons, all evaluations were restricted to the intersection of all model vocabularies}{table.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}How Similarity Emerges}{9}{section.6}}
\newlabel{section:exp}{{6}{9}{How Similarity Emerges\relax }{section.6}{}}
\citation{hill2014simlex}
\citation{faruqui2014improving}
\citation{Hermann:2014:ICLR}
\citation{faruqui2014improving}
\citation{Kocisky:2014}
\citation{dist}
\citation{bergstra+al:2010-scipy,Bastien-Theano-2012}
\bibdata{iclr2015}
\bibcite{Agirre2009}{{1}{2009}{{Agirre et~al.}}{{Agirre, Alfonseca, Hall, Kravalova, Pasca, and Soroa}}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Conclusion}{10}{section.7}}
\bibcite{Bahdanau2014}{{2}{2014}{{Bahdanau et~al.}}{{Bahdanau, Cho, and Bengio}}}
\bibcite{baroni2014don}{{3}{2014}{{Baroni et~al.}}{{Baroni, Dinu, and Kruszewski}}}
\bibcite{Bastien-Theano-2012}{{4}{2012}{{Bastien et~al.}}{{Bastien, Lamblin, Pascanu, Bergstra, Goodfellow, Bergeron, Bouchard, and Bengio}}}
\bibcite{Bengio+Senecal-2003-small}{{5}{2003}{{Bengio \& S\'en\'ecal}}{{Bengio and S\'en\'ecal}}}
\bibcite{Bengio2003lm}{{6}{2003}{{Bengio et~al.}}{{Bengio, Ducharme, Vincent, and Janvin}}}
\bibcite{bergstra+al:2010-scipy}{{7}{2010}{{Bergstra et~al.}}{{Bergstra, Breuleux, Bastien, Lamblin, Pascanu, Desjardins, Turian, Warde-Farley, and Bengio}}}
\bibcite{Bruni2014}{{8}{2014}{{Bruni et~al.}}{{Bruni, Tran, and Baroni}}}
\bibcite{Chandar}{{9}{2014}{{Chandar et~al.}}{{Chandar, Lauly, Larochelle, Khapra, Ravindran, Raykar, and Saha}}}
\bibcite{Cho2014}{{10}{2014}{{Cho et~al.}}{{Cho, van Merrienboer, Gulcehre, Bougares, Schwenk, and Bengio}}}
\bibcite{collobert2008unified}{{11}{2008}{{Collobert \& Weston}}{{Collobert and Weston}}}
\bibcite{devlin2014fast}{{12}{2014}{{Devlin et~al.}}{{Devlin, Zbib, Huang, Lamar, Schwartz, and Makhoul}}}
\bibcite{faruqui2014improving}{{13}{2014}{{Faruqui \& Dyer}}{{Faruqui and Dyer}}}
\bibcite{dist}{{14}{1957}{{Firth}}{{}}}
\bibcite{haghighi2008learning}{{15}{2008}{{Haghighi et~al.}}{{Haghighi, Liang, Berg-Kirkpatrick, and Klein}}}
\bibcite{Hermann:2014:ICLR}{{16}{2014}{{Hermann \& Blunsom}}{{Hermann and Blunsom}}}
\bibcite{Hill2014EMNLP}{{17}{2014}{{Hill \& Korhonen}}{{Hill and Korhonen}}}
\bibcite{hill2014simlex}{{18}{2014}{{Hill et~al.}}{{Hill, Reichart, and Korhonen}}}
\bibcite{Jean}{{19}{2014}{{Jean et~al.}}{{Jean, Cho, Memisevic, and Bengio}}}
\bibcite{kalchbrenner13emnlp}{{20}{2013}{{Kalchbrenner \& Blunsom}}{{Kalchbrenner and Blunsom}}}
\bibcite{Klementiev}{{21}{2012{a}}{{Klementiev et~al.}}{{Klementiev, Titov, and Bhattarai}}}
\bibcite{klementiev2012inducing}{{22}{2012{b}}{{Klementiev et~al.}}{{Klementiev, Titov, and Bhattarai}}}
\bibcite{Kocisky:2014}{{23}{2014}{{Ko\v {c}isk\'{y} et~al.}}{{Ko\v {c}isk\'{y}, Hermann, and Blunsom}}}
\bibcite{landauer1997solution}{{24}{1997}{{Landauer \& Dumais}}{{Landauer and Dumais}}}
\bibcite{levy2014dependency}{{25}{2014}{{Levy \& Goldberg}}{{Levy and Goldberg}}}
\bibcite{luong2014addressing}{{26}{2014}{{Luong et~al.}}{{Luong, Sutskever, Le, Vinyals, and Zaremba}}}
\bibcite{mikolov2013exploiting}{{27}{2013{a}}{{Mikolov et~al.}}{{Mikolov, Le, and Sutskever}}}
\bibcite{mikolov2013distributed}{{28}{2013{b}}{{Mikolov et~al.}}{{Mikolov, Sutskever, Chen, Corrado, and Dean}}}
\bibcite{mnih2009scalable}{{29}{2009}{{Mnih \& Hinton}}{{Mnih and Hinton}}}
\bibcite{morin2005hierarchical}{{30}{2005}{{Morin \& Bengio}}{{Morin and Bengio}}}
\bibcite{nelson2004university}{{31}{2004}{{Nelson et~al.}}{{Nelson, McEvoy, and Schreiber}}}
\bibcite{Pennington2014}{{32}{2014}{{Pennington et~al.}}{{Pennington, Socher, and Manning}}}
\bibcite{Sutskever2014sequence}{{33}{2014}{{Sutskever et~al.}}{{Sutskever, Vinyals, and Le}}}
\bibcite{turney2010frequency}{{34}{2010}{{Turney \& Pantel}}{{Turney and Pantel}}}
\bibcite{vulic2011identifying}{{35}{2011}{{Vuli{\'c} et~al.}}{{Vuli{\'c}, De~Smet, and Moens}}}
\bibstyle{iclr2015}
