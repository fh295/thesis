% This chapter is the conclusion

\section{Contributions of this thesis}

This thesis concerns the problem of learning to represent the meaning of words, phrases and sentences in continuous vector spaces. As I discussed in Chapter~\ref{CH1}, algorithms for learning the meaning of words in continuos space are almost as old as the field of computational linguistics itself. However, the algorithms, possible sources of training data and methods of analysing and evaluating such representations are constantly improving. The first part of this thesis (chapters~\ref{CH2}-\ref{CH3}) focused on understanding and improving distributed word representations via new evaluation paradigms and training settings that better reflect human language acquisition.

The separate problem of acquiring representations of phrases or sentences in discrete or symbolic form (and thus unambiguously interpretable by traditional computer architectures) has also been a core endeavour for computational linguists [REFs]. Phrase and sentence representation is arguably more important than word representation for language applications, since languages generally encode and transmit information in phrases not individual words. In later chapters (\ref{CH4}-\ref{CH6}), I aim to extend the benefits of distributed representations (such as more realistic modelling of the smooth nature of natural language categories, and seamless interface with neural language model applications) from words to phrases and sentences. Unlike previous approaches to this problem, [REFs] I do not do so by building `bottom up' combinations of word representations. Instead, I employ neural networks that compute distributed phrase or sentence representations as an intermediate stage in satisfying some task-agnostic objective on naturally occurring text-based resources. Finally, in Chapter~\ref{CH6} I exemplify a more extrinsic method for analysing representation of textual context, in which the effect of a particular representational form is measured by their downstream effect on a canonical missing-word prediction task.   

The principal contributions of this thesis are as follows:

\paragraph{A new resource for the evaluation of distributed word representations} Without robust ways to evaluate the quality of word representations, it would be difficult to compare various approaches and detect improvements. Existing methods suffered from a range of limitations, such as low word coverage, poorly defined scores or low inter-rater agreement. In chapter~\ref{CH2} I described SimLex-999, a resource designed to mitigate these limitations. SimLex covers a more representative set of word concepts than many alternative evaluation resources. It measures semantic similarity, a relation about which native English speakers seem to have clearer, more consistent intuitions. Since its development, SimLex-999 has been used to evaluate numerous new algorithms and approaches for word representation learning. It has also been translated into German, Italian and Russian. 

\paragraph{Two novel methods for acquiring distributed word representations} Perhaps the most important characteristic of a neural language model is its ability to acquire (and utilise) internal distributed representations, typically of words or word-like entities [REF]. Previous work had shown that the word representations learned by NLMs can perform similarly to, or even surpass, other state-of-the-art approaches for acquiring distributed representations [REF]. In Chapter~\ref{CH3}, I extended the analysis of NLM word representations to cover simple Skipgram models trained on information from different modalities (i.e. not just text but perceptual property norms), and to sequence-to-sequence models trained on bilingual texts. In the case of the Skipgram model, I showed how information relating to the physical properties of concrete concepts propagates in the representation space of the model, leading to richer representational geometry even among abstract words. Using the sequence-to-sequence model, I showed how the objective of translating between sentences in bilingual corpora yields word representation spaces that are more naturally orientated according to semantic similarity than monolingual neural language models. Indeed, such a model produced what was at the time the best reported performance of a distributional model on the SimLex-999 benchmark of similarity modelling.  

\paragraph{Learning phrase representations by training NLMs on dictionaries or encyclopedias} in Chapter~\ref{CH4}, I showed how NLMs could be effectively trained on the textual definitions or descriptions in dictionaries and encyclopedias. In these models, dictionaries provide a bridge between lexical meaning and phrase meaning, allowing the model's interpretation of phrases to be `supervised' by the corresponding lexical representation (which can be  easily acquired by models described in Chapters~\ref{CH2} and~\ref{CH3}). The combination of the representational power of neural language models and the principled semantic information in dictionaries proves to be very powerful. The trained models generalise well beyond the training data. They are capable of beating established dictionary-indexing software at retrieving concepts not defined in the training data, an effect that is magnified when the linguistic style of description of definition differs from that of the training set, and can even answer general-knowledge crossword questions. Moreover, the model performs more consistently than alternative NLM architectures as a general-purpose representation-learning engine across the suite of supervised and unsupervised evaluations applied to all models in Chapter~\ref{CH5}.

\paragraph{Two novel models for learning distributed sentence representations from text} In addition to a systematic comparison of methods for acquiring phrase and sentence representations from unlabelled text-based data, in Chapter~\ref{CH6}, I developed two new algorithms, each with certain specific advantages over existing approaches. The first, the sequential denoising autoencoder, is a modification of the SkipThought model that can be trained on any collection of unordered sentences, and learns representations that are particularly applicable to paraphrasing applications. The second, FastSent, is a modification CBOW, a well-known log-linear model for lexical representation learning, in which word embeddings are optimised to form useful sentence representations under the addition operation. Like other shallow neural language models, FastSent performs best in unsupervised applications involving a linear decoding of its representation space. It outperforms alternatives at direct prediction of sentence relatedness, and qualitative analysis (e.g. via the web demo) suggests a more semantically plausible space of sentence representations than alternatives.       

\paragraph{Representing naturally-occurring language in memory networks} Memory networks had previously been applied to toy tasks involving artificial language, such as question answering. In Chapter~\ref{CH6}, I described one of the first studies in which memory networks are trained to effectively represent naturally-occurring languages (passages of multiple sentences). I also showed how contextual neural language models such as memory networks provide a more extrinsic way to compare representational forms for text, particularly phrases and sentences. I showed that models that effectively focus on small sub-sentential windows convey more useful information (at least with respect to a missing-word completion task) than those whose focus is both broader (entire sentences) or narrower (ordered sequences of words). In addition, I produced and released the Children's Book Test, a benchmark designed to evaluate how well models represent and select information from extra sentential contexts. Together, these contributions can be understood in a general tendency of language processing research away from analysing individual sentences in isolation, towards models that can effectively interpret utterances in particular contexts of documents or dialogues.  

\section{Future work}
3. Say what remains - the future of AI, the future of neuroscience