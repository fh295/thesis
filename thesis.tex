%%%%%
%%
%% Sample document ``thesis.tex''
%%
%% Version: v0.2
%% Authors: Jean Martina, Rok Strnisa, Matej Urbas
%% Date: 30/07/2008
%%
%% Copyright (c) 2008-2011, Rok Strni≈°a, Jean Martina, Matej Urbas
%% License: Simplified BSD License
%% License file: ./License
%% Original License URL: http://www.freebsd.org/copyright/freebsd-license.html
%%%%%

% Available documentclass options:
%
%   <all `report` document class options, e.g.: `a5paper`>
%   withindex   - enables the index. New index entries can be added through `\index{my entry}`
%   glossary    - enables the glossary.
%   techreport  - typesets the thesis in the technical report format.
%   times       - uses the `Times` font.
%
% For more info see `README.md`
\documentclass[withindex,glossary]{cam-thesis}

\usepackage{changepage}

\usepackage{times}

\usepackage{url}

\usepackage{latexsym}

\usepackage{graphicx}

\usepackage{amssymb}

\usepackage{hyperref}
%\usepackage{url}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{xspace}
\usepackage{tabularx}
\usepackage{arydshln}
\usepackage{wrapfig}
\usepackage[utf8]{inputenc}
\usepackage{times}
\usepackage{latexsym}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{siunitx}
\usepackage{url}
\usepackage{array,graphicx}
\usepackage{booktabs}
\usepackage{pifont}
\usepackage{changepage}
\usepackage[font=footnotesize,labelfont=bf]{caption}
\usepackage{times}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{multirow}
\usepackage{dirtytalk}
\usepackage[round]{natbib}
%\usepackage[hyphens]{url}
\DeclareMathOperator*{\argmax}{arg\,max}


\usepackage{arydshln}
\makeatletter
%\newcommand{\@BIBLABEL}{\@emptybiblabel}
\newcommand{\@emptybiblabel}[1]{}
\makeatother
%\usepackage[hidelinks]{hyperref}

\usepackage[normalem]{ulem}
\newcounter{quotecount}
\newcommand{\MyQuote}[1]{\vspace{0.4cm}\addtocounter{quotecount}{1}%
     (\arabic{quotecount})\hspace*{1cm}\parbox{12cm}{\em #1}\\[0.4cm]}

\newcommand{\me}{\mathrm{e}}

\newcommand*\rot{\rotatebox{90}}
\newcommand*\OK{\ding{51}}

\DeclareMathOperator*{\softmax}{softmax}
%\usepackage{todonotes}
\usepackage{color}
\usepackage{enumitem}
\usepackage{placeins}
\newcommand{\todo}[1]{\textcolor{red}{TODO: #1}}

\newcommand{\starspace}{{\sc Embedding Model}\xspace}
%\newcommand{\window}{{\sc sparse conv.}\xspace}
\newcommand{\window}{{\sc window}\xspace}

\newcommand{\bW}{{\bf W}}
\newcommand{\bA}{{\bf A}}
\newcommand{\bB}{{\bf B}}
\newcommand{\bC}{{\bf C}}
\newcommand{\bH}{{\bf H}}
\newcommand{\bU}{{\bf U}}
\newcommand{\bm}{{\bf m}}
\newcommand{\bc}{{\bf c}}
\newcommand{\bq}{{\bf q}}
\newcommand{\ba}{{\bf a}}
\renewcommand{\Re}{\mathbb{R}}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Thesis meta-information
%%
\begin{document}

%% The title of the thesis:
\title{\large Representing Linguistic Meaning in Distributed Memories \hspace{15mm} From Words to Sentences and Beyond}

%% The full name of the author (e.g.: James Smith):
\author{Felix Hill}

%% College affiliation:
\college{St John's College}

%% College shield [optional]:
% \collegeshield{CollegeShields/Clare}
% \collegeshield{CollegeShields/Fitzwilliam}
% \collegeshield{CollegeShields/Queens}
 \collegeshield{CollegeShields/StJohns}
% \collegeshield{CollegeShields/Trinity}


%% Submission date [optional]:
% \submissiondate{November, 2042}

%% You can redefine the submission notice [optional]:
% \submissionnotice{A badass thesis submitted on time for the Degree of PhD}

%% Declaration date:
\date{June 2016}

%% PDF meta-info:
\subjectline{Computer Science}
\keywords{one two three}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Abstract:
%%


\abstract{In recent years, models of language understanding built on distributed semantic representations have advanced the state-of-the-art in various application areas, from text classification to machine translation. However, in many cases, this success was contingent on large-scale task-specific labelled training data. Unlike such models, humans are capable of acquiring linguistic and semantic knowledge from a diverse range of unlabelled language and sensory input for application to multiple tasks. This thesis develops computational methods for acquiring general-purpose semantic representations of linguistic units in distributed memories, to provide a basis for more robust, human-like language understanding that does not require task-specific or labelled data. The proposed techniques include novel methods for encoding the semantics of words, phrases, and sentences (in isolation and in context). In each case, the techniques result in at or near state-of-the-art performance on multiple benchmarks, including comparisons with human data and downstream classification or question-answering tasks. Moreover, resources are developed for better understanding the properties of semantic representations, enabling analysis of how they emerge from raw input as a consequence of model learning objectives. The results highlight the advantage of representing the linguistic signal in distributed memories, and indicate that we ultimately might be able develop a seamless (human-like) interface between such representations and more abstract semantic knowledge encoded in a similar way.}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Acknowledgements:
%%
\acknowledgements{After 3 months I didn't think I would get here, but most of the rest was a constant pleasure, and that was because I learned so much from so many people. In broadly chronological order, these are my advisor, Anna Korhonen, Christian Bentz, Douwe Kiela, Roi Reichart, Yoshua Bengio, Kyunghyun Cho, Antoine Bordes and Jason Weston. I'm also indebted to St John's College, Cambridge, Google and Facebook AI Research for making this research possible. On the personal side, I couldn't have done it without Becky and the Wallers, the Worcester crew and Hannah pushing me over the line. But most of all, this is an achievement for my parents, Zelda and Simon, whose support and encouragement allowed me to do what I dreamed and believed in.  
}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Glossary [optional]:
%%
\newglossaryentry{HOL}{
    name=HOL,
    description={Higher-order logic}
}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Contents:
%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Title page, abstract, declaration etc.:
%% -    the title page (is automatically omitted in the technical report mode).
\frontmatter{}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Thesis body:
%%
\chapter{Introduction}
\input{Chapter_1/Chapter_1.tex}
\chapter{Understanding and evaluating distributed word representations}
\input{Chapter_2/Chapter_2.tex}
\chapter{Embedding word similarity with neural machine translation}
\input{Chapter_3/Chapter_3.tex}
\chapter{Representing phrases with neural language models}
\input{Chapter_4/Chapter_4.tex}
\chapter{Representing sentences with neural language models}
\input{Chapter_5/Chapter_5.tex}
\chapter{Representing and interpreting sentence semantics in context}
\input{Chapter_6/Chapter_6.tex}
\chapter{Conclusion}
\input{Chapter_7/Chapter_7.tex}
\chapter*{List of publications}
\input{Publications.tex}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Bibliography:
%%

\cleardoublepage
\phantomsection
\addcontentsline{toc}{chapter}{Bibliography}
\bibliographystyle{plainnat}
\bibliography{thesis}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Appendix:
%%
\appendix

\chapter{}

\input{AppendixA.tex}

%\chapter{}

%\input{AppendixB.tex}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Index:
%%
\printthesisindex

\end{document}
