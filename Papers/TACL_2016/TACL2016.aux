\relax 
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\citation{mikolov2013distributed}
\citation{zock2004word}
\citation{307754,klementiev2012inducing,hermann2013multilingual,gouws2014bilbowa}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\citation{mikolov2010recurrent}
\citation{kiros2014unifying}
\citation{bahdanau2014neural}
\citation{mikolov2013distributed}
\citation{bengio1994learning}
\citation{hochreiter1997long}
\@writefile{toc}{\contentsline {section}{\numberline {2}Neural Language Model Architectures}{2}{section.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Long Short Term Memory}{2}{subsection.2.1}}
\citation{huang2012improving}
\citation{bordes2015large}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Bag-of-Words NLMs}{3}{subsection.2.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Pre-trained Input Representations}{3}{subsection.2.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Training Objective}{3}{subsection.2.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}Implementation Details}{3}{subsection.2.5}}
\citation{faruqui2014retrofitting}
\citation{cho2014learning}
\citation{bergstra+al:2010-scipy}
\citation{zeiler2012adadelta}
\citation{bilac2003improving,bilac2004dictionary,zock2004word}
\citation{shaw2013building}
\@writefile{toc}{\contentsline {section}{\numberline {3}Reverse Dictionaries}{4}{section.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Data Collection and Training}{4}{subsection.3.1}}
\citation{mitchell2010composition}
\citation{shaw2013building}
\citation{leech1994claws4}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Comparisons}{5}{subsection.3.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Reverse Dictionary Evaluation}{5}{subsection.3.3}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Performance of different reverse dictionary models in different evaluation settings. *Low variance in \emph  {mult} models is due to consistently poor scores, so not highlighted.}}{6}{table.1}}
\newlabel{results}{{1}{6}{Performance of different reverse dictionary models in different evaluation settings. *Low variance in \emph {mult} models is due to consistently poor scores, so not highlighted}{table.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Style difference between \emph  {dictionary definitions} and \emph  {concept descriptions} in the evaluation.}}{6}{table.2}}
\newlabel{tb:tablename}{{2}{6}{Style difference between \emph {dictionary definitions} and \emph {concept descriptions} in the evaluation}{table.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Results}{6}{subsection.3.4}}
\citation{iyyer2015deep}
\citation{hermann2013multilingual,lauly2014autoencoder,gouws2014bilbowa}
\citation{gouws2014bilbowa}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5}Qualitative Analysis}{7}{subsection.3.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6}Cross-Lingual Reverse Dictionaries}{7}{subsection.3.6}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces The top-five candidates for example queries (invented by the authors) from different reverse dictionary models. Both the RNN and BOW models are without Word2Vec input and use the cosine loss.}}{8}{table.3}}
\newlabel{qual}{{3}{8}{The top-five candidates for example queries (invented by the authors) from different reverse dictionary models. Both the RNN and BOW models are without Word2Vec input and use the cosine loss}{table.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Responses from cross-lingual reverse dictionary models to selected queries. Underlined responses are `correct' or potentially useful for a native French speaker.}}{8}{table.4}}
\newlabel{cross}{{4}{8}{Responses from cross-lingual reverse dictionary models to selected queries. Underlined responses are `correct' or potentially useful for a native French speaker}{table.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.7}Discussion}{8}{subsection.3.7}}
\citation{ferrucci2010building}
\citation{molla2007question}
\citation{Iyyer:Boyd-Graber:Claudino:Socher:Daume-2014,weston2015towards}
\citation{berant14paraphrasing,bordes2014question}
\citation{littman2002probabilistic}
\citation{littman2002probabilistic}
\citation{ginsberg2011dr}
\@writefile{toc}{\contentsline {section}{\numberline {4}General Knowledge (crossword) Question Answering}{9}{section.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Evaluation}{9}{subsection.4.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Benchmarks and Comparisons}{9}{subsection.4.2}}
\citation{littman2002probabilistic,ginsberg2011dr}
\citation{mikolov2013distributed}
\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces Performance of different models on crossword questions of different length. The two commercial systems are evaluated via their web interface so only accuracy@10 can be reported in those cases. }}{10}{table.6}}
\newlabel{results2}{{6}{10}{Performance of different models on crossword questions of different length. The two commercial systems are evaluated via their web interface so only accuracy@10 can be reported in those cases}{table.6}{}}
\newlabel{tb:tablename}{{6}{10}{Performance of different models on crossword questions of different length. The two commercial systems are evaluated via their web interface so only accuracy@10 can be reported in those cases}{table.6}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces Examples of the different question types in the crossword question evaluation dataset.}}{10}{table.5}}
\newlabel{tb:tablename}{{5}{10}{Examples of the different question types in the crossword question evaluation dataset}{table.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Results}{10}{subsection.4.3}}
\@writefile{lot}{\contentsline {table}{\numberline {7}{\ignorespaces Responses from different models to example crossword clues. In each case the model output is filtered to exclude any candidates that are not of the same length as the correct answer. BOW and RNN models are trained without Word2Vec input embeddings and cosine loss.}}{11}{table.7}}
\newlabel{egs}{{7}{11}{Responses from different models to example crossword clues. In each case the model output is filtered to exclude any candidates that are not of the same length as the correct answer. BOW and RNN models are trained without Word2Vec input embeddings and cosine loss}{table.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Qualitative Analysis}{11}{subsection.4.4}}
\citation{bordes2014question,graves2014neural,weston2015towards}
\citation{graves2014neural,weston2015towards}
\bibstyle{acl}
\bibdata{TACL2016}
\@writefile{toc}{\contentsline {section}{\numberline {5}Conclusion}{12}{section.5}}
