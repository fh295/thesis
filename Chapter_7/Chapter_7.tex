% This chapter is the conclusion

\section{Contributions of this thesis}

This thesis concerns the problem of learning to represent the meaning of words, phrases and sentences in continuous vector spaces. As I discussed in Chapter~\ref{CH1}, algorithms for learning the meaning of words in continuos space are almost as old as the field of computational linguistics itself. However, the algorithms, possible sources of training data and methods of analysing and evaluating such representations are constantly improving. The first part of this thesis (chapters~\ref{CH2}-\ref{CH3}) focused on understanding and improving distributed word representations via new evaluation paradigms and training settings that better reflect human language acquisition.

The separate problem of acquiring representations of phrases or sentences in discrete or symbolic form (and thus unambiguously interpretable by traditional computer architectures) has also been a core endeavour for computational linguists [REFs]. Phrase and sentence representation is arguably more important than word representation, since languages generally encode and transmit information in phrases not individual words. In later chapters (\ref{CH4}-\ref{CH6}), I aim to extend the benefits of distributed representations (such as more realistic modelling of the smooth nature of natural language categories, and seamless interface with neural language model applications) from words to phrases and sentences, units of language that should ultimately be of wider use to language technology applications. 

The principal contributions of this thesis are summarised below:

\paragraph{A new resource for the evaluation of distributed word representations} Without robust ways to evaluate the quality of word representations, it would be difficult to compare various approaches and detect improvements. Existing methods suffered from a range of limitations, such as low word coverage, poorly defined scores or low inter-rater agreement. In chapter~\ref{CH2} I described SimLex-999, a resource designed to mitigate these limitations. SimLex covers a more representative set of word concepts than many alternative evaluation resources. It measures semantic similarity, a relation about which native English speakers seem to have clearer, more consistent intuitions. Since its development, SimLex-999 has been used to evaluate numerous new algorithms and approaches for word representation learning. It has also been translated into German, Italian and Russian. 

\paragraph{Two novel methods for acquiring distributed word representations} Perhaps the most important characteristic of a neural language model is its ability to acquire (and utilise) internal distributed representations, typically of words or word-like entities [REF]. Previous work had shown that the word representations learned by NLMs can perform similarly to, or even surpass, other state-of-the-art approaches for acquiring distributed representations [REF]. In Chapter~\ref{CH3}, I extended the analysis of NLM word representations to cover simple skipgram models trained on information from different modalities (i.e. not just text but perceptual property norms), and to sequence-to-sequence models trained on bilingual texts. In the case of the skipgram model, I showed how information relating to the physical properties of concrete concepts propagates in the representation space of the model, leading to richer representational geometry even among abstract words. Using the sequence-to-sequence model, I showed how semantic similarity is even more XX

\paragraph{Models for learning distributed phrase representations from dictionaries or encyclopedias}

\paragraph{Two novel models for learning distributed sentence representations from text}

\paragraph{A comparison of forms for text representations in memory networks}

Unlike previous approaches to this problem, [REFs] I do not do so by building `bottom up' combinations of word representations. Instead, I exploit new techniques available for fast training of neural networks to compute distributed phrase or sentence vectors as hidden states of models trained via task-agnostic objectives on naturally occurring text-based resources. Finally, in Chapter~\ref{CH6} I exemplify how such phrasal or sentence representations could ultimately be applied as layers of internal knowledge in deeper neural language models.   



2. Outline where they stand in the history of language understanding, and their impact (e.g. cite press cuttings, google Go and AI in general)

\section{Future work}
3. Say what remains - the future of AI, the future of neuroscience