


This thesis is about computational models that learn language in an unsupervised way. Children. 



Beyond the intuitive parallels with child language learning, are there practical reasons to develop language technology that can be trained in an unsupervised way? Yes. Why?  



Unsupervised learning means X. However, there are numerous ways in which models can use the environment to learn, and the label `unsupervised' tends to mean different things to different people. In this thesis, I use unsupervised to refer to any models that learn from Y. Note that this makes no assumption about the model or the learning objective itself. Indeed, the models I consider, artificial neural networks, can typically be trained in an unambiguously supervised setting. Indeed, in the unsupervised settings presented here, these models learn by backpropagating an error associated (via a cost function) with the correctness of their predictions. Thus, algorithmically at least, they share many characteristics of supervised learning. 


\paragraph{Representation learning} The unsupervised learning considered in this thesis involves learning distributed representations of language (from words, to phrases and sentences to whole passages). In part because of the statistical distributions of natural language types, and in part because of computational (infrastructure) challenges, a single strategy cannot be generally applied to represent each of these different linguistic units. 

There are many reasons to believe that the human brain encodes knowledge in distributed representations. A. B. C. 

science
language
psycho
neuro

engineering
success of deep learning

Historical context of deep learning for language understanding
-- Bengio
-- Speech recognition
-- CW, then Turian
-- Socher (supervised)
-- Kalchbrenner (supervised)
-- Seq2seq
