% This chapter is the conclusion

\section{Contributions of this thesis}


RE-WRITE - HEADLINE CONTRIBUTION: UNSUPERVISED LEARNING OF DISTRIBUTED REPRESENTATIONS FOR PHRASES AND SENTENCES


This thesis concerns the problem of learning to represent the meaning of words, phrases and sentences in continuous vector spaces. As I discussed in Chapter~\ref{CH1}, algorithms for learning the meaning of words in continuos space are almost as old as the field of computational linguistics itself. However, the algorithms, possible sources of training data and methods of analysing and evaluating such representations are constantly improving. The first part of this thesis (chapters~\ref{CH2}-\ref{CH3}) focused on understanding and improving distributed word representations via new evaluation paradigms and training settings that better reflect human language acquisition.

The separate problem of acquiring representations of phrases or sentences in discrete or symbolic form (and thus unambiguously interpretable by traditional computer architectures) has also been a core endeavour for computational linguists [REFs]. Phrase and sentence representation is arguably more important than word representation for language applications, since languages generally encode and transmit information in phrases not individual words. In later chapters (\ref{CH4}-\ref{CH6}), I aim to extend the benefits of distributed representations (such as more realistic modelling of the smooth nature of natural language categories, and seamless interface with neural language model applications) from words to phrases and sentences. Unlike previous approaches to this problem, [REFs] I do not do so by building `bottom up' combinations of word representations. Instead, I employ neural networks that compute distributed phrase or sentence representations as an intermediate stage in satisfying some task-agnostic objective on naturally occurring text-based resources. Finally, in Chapter~\ref{CH6} I exemplify a more extrinsic method for analysing representation of textual context, in which the effect of a particular representational form is measured by their downstream effect on a canonical missing-word prediction task.   

The principal contributions of this thesis are as follows:

\paragraph{A new resource for the evaluation of distributed word representations} Without robust ways to evaluate the quality of word representations, it would be difficult to compare various approaches and detect improvements. Existing methods suffered from a range of limitations, such as low word coverage, poorly defined scores or low inter-rater agreement. In chapter~\ref{CH2} I described SimLex-999, a resource designed to mitigate these limitations. SimLex covers a more representative set of word concepts than many alternative evaluation resources. It measures semantic similarity, a relation about which native English speakers seem to have clearer, more consistent intuitions. Since its development, SimLex-999 has been used to evaluate numerous new algorithms and approaches for word representation learning. It has also been translated into German, Italian and Russian. 

\paragraph{Two novel methods for acquiring distributed word representations} Perhaps the most important characteristic of a neural language model is its ability to acquire (and utilise) internal distributed representations, typically of words or word-like entities [REF]. Previous work had shown that the word representations learned by NLMs can perform similarly to, or even surpass, other state-of-the-art approaches for acquiring distributed representations [REF]. In Chapter~\ref{CH3}, I extended the analysis of NLM word representations to cover simple Skipgram models trained on information from different modalities (i.e. not just text but perceptual property norms), and to sequence-to-sequence models trained on bilingual texts. In the case of the Skipgram model, I showed how information relating to the physical properties of concrete concepts propagates in the representation space of the model, leading to richer representational geometry even among abstract words. Using the sequence-to-sequence model, I showed how the objective of translating between sentences in bilingual corpora yields word representation spaces that are more naturally orientated according to semantic similarity than monolingual neural language models. Indeed, such a model produced what was at the time the best reported performance of a distributional model on the SimLex-999 benchmark of similarity modelling.  

\paragraph{Learning phrase representations by training NLMs on dictionaries or encyclopedias} in Chapter~\ref{CH4}, I showed how NLMs could be effectively trained on the textual definitions or descriptions in dictionaries and encyclopedias. In these models, dictionaries provide a bridge between lexical meaning and phrase meaning, allowing the model's interpretation of phrases to be `supervised' by the corresponding lexical representation (which can be  easily acquired by models described in Chapters~\ref{CH2} and~\ref{CH3}). The combination of the representational power of neural language models and the principled semantic information in dictionaries proves to be very powerful. The trained models generalise well beyond the training data. They are capable of beating established dictionary-indexing software at retrieving concepts not defined in the training data, an effect that is magnified when the linguistic style of description of definition differs from that of the training set, and can even answer general-knowledge crossword questions. Moreover, the model performs more consistently than alternative NLM architectures as a general-purpose representation-learning engine across the suite of supervised and unsupervised evaluations applied to all models in Chapter~\ref{CH5}.

\paragraph{Two novel models for learning distributed sentence representations from text} In addition to a systematic comparison of methods for acquiring phrase and sentence representations from unlabelled text-based data, in Chapter~\ref{CH6}, I developed two new algorithms, each with certain specific advantages over existing approaches. The first, the sequential denoising autoencoder, is a modification of the SkipThought model that can be trained on any collection of unordered sentences, and learns representations that are particularly applicable to paraphrasing applications. The second, FastSent, is a modification CBOW, a well-known log-linear model for lexical representation learning, in which word embeddings are optimised to form useful sentence representations under the addition operation. Like other shallow neural language models, FastSent performs best in unsupervised applications involving a linear decoding of its representation space. It outperforms alternatives at direct prediction of sentence relatedness, and qualitative analysis (e.g. via the web demo) suggests a more semantically plausible space of sentence representations than alternatives.       

\paragraph{Representing naturally-occurring language in memory networks} Memory networks had previously been applied to toy tasks involving artificial language, such as question answering. In Chapter~\ref{CH6}, I described one of the first studies in which memory networks are trained to effectively represent naturally-occurring languages (passages of multiple sentences). I also showed how contextual neural language models such as memory networks provide a more extrinsic way to compare representational forms for text, particularly phrases and sentences. I showed that models that effectively focus on small sub-sentential windows convey more useful information (at least with respect to a missing-word completion task) than those whose focus is both broader (entire sentences) or narrower (ordered sequences of words). In addition, I produced and released the Children's Book Test, a benchmark designed to evaluate how well models represent and select information from extra sentential contexts. Together, these contributions can be understood in a general tendency of language processing research away from analysing individual sentences in isolation, towards models that can effectively interpret utterances in particular contexts of documents or dialogues.  

\section{Future work} The approaches to knowledge representation and language learning described in this thesis all involve training models to make predictions from a language corpora, structured text-based resources, semantic property norms or image representations. The diversity of data sources was designed to mitigate a clear discrepancy between the information available to human language learners and what is available to the majority of language understanding models during training. Nevertheless, there is another important point of difference between these approaches and human language learning that may prove just as important to address if models are to exhibit truly human-like linguistic behaviours. Human language learning is~\emph{interactive}, in the sense that the learner can use language to influence the nature of his or her own linguistic experience (e.g by moving conversations in a particular direction). This is in stark contrast with the regimes applied to train the models in this thesis, where the content of the training data does not depend at any stage on the current predictions of the model. 

The approaches studied in this thesis can be seen to reflect the situation in which a learner is reading or listening passively to language produced by others. This is undoubtedly the way in which babies learn their first words, and even after they can talk it seems likely that a large proportion of language learning relies on this sort of passive acquisition. However, learners also to apply language to satisfy (increasingly complex) communicative goals. In doing so, they influence the nature of the language to which they are exposed, and thus guide their own learning. In order to satisfy more complex goals, their learning must also be robust to sequences of multiple training examples with little or no knowledge of whether their interpretations of these examples are `correct' with respect to the goal in question. Both of these aspects, currently lacking from the approaches studied in this thesis, may be critical for efficiently training models to replicate human linguistic behaviour in a robust way.

The next stage of this research programme is to place the language models described in this thesis into more dynamic, interactive and goal-driven learning environments. In recent work, interactive learning frameworks such as reinforcement learning have proved very effective tools for training agents to resolve games, particularly when deep neural networks are used to represent the situations faced by the agent and thus effectively reduce the search space among state-action pairs [REFs]. Indeed, the same strategy has been tried to some effect when states are described by textual descriptions as part of a game in which the agent must choose between three possible actions at each stage [REF]. Nevertheless, there are many significant challenges in applying such a strategy to language learning in general. Linguistic behaviour cannot easily be reduced to a (small) finite number of possible actions, which makes learning very challenging. Moreover, it is not trivial to model the vast and dynamic array of behavioural goals that are characteristic of human activity and which may combine to make general language understanding viable. 

 